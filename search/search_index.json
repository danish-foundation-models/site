{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Empowering the Danish Language in the Digital Age","text":"<p>Welcome to the Danish Foundation Models (DFM) project, a pioneering initiative in the field of machine learning and natural language processing (NLP) dedicated to the Danish language. Our mission is to develop, maintain, and provide open access to high-quality foundation models tailored for Danish, promoting innovation and inclusivity in language technologies.</p> <p>Read the paper</p> <p>You can read more about the argument for Danish Language models in our publication.</p>"},{"location":"index.html#why-danish-foundation-models","title":"Why Danish Foundation Models?","text":""},{"location":"index.html#bridging-the-digital-language-divide","title":"Bridging the Digital Language Divide","text":"<ul> <li>Global Gap: The rise of large language models has transformed research and technology, but smaller languages like Danish risk falling behind both in development, evaluation and application.</li> <li>Local Focus: We combat this by focusing on the Danish language, ensuring that it is well-represented in the digital landscape.</li> <li>Broad Collaboration: Our project unites public and private institutions, ensuring high data quality and practical applicability of our models.</li> </ul>"},{"location":"index.html#our-objectives","title":"Our Objectives","text":"<ol> <li>To develop and maintain state-of-the-art language models for Danish for applications within both text and speech.</li> <li>To extensively validate foundation models for Danish in a representative set of tasks.</li> <li>To maintain a high standard of documentation of models such as model cards [Mitchell et al., 2019] and datasheets [Gebru et al., 2021].</li> <li>To open-source not only the models but also all components required for reproducibility such as pre-processing, training, and validation code.</li> </ol>"},{"location":"index.html#open-source","title":"Open-source","text":""},{"location":"index.html#open-source-development-with-privacy-focused-data-handling","title":"Open-source Development with Privacy-Focused Data Handling","text":"<p>In our commitment to advancing open-source development, we strongly emphasise the ethical handling of data, particularly when it involves personally sensitive information or material under copyright. This ensures that we share as much as possible while protecting privacy.</p> <p>To achieve this, our project is structured to differentiate between data that can be shared openly and that which cannot.  This demarcation is documented through detailed datasheets and training logs, thereby ensuring transparency in our processes.</p> <p>Additionally, we prioritise the security of the data during its processing and training phases. All data is stored on UCloud, a platform that upholds the recognised highest standards in information security management. This commitment to data security is exemplified by UCloud's adherence to ISO27001, a globally recognised standard, ensuring that our data handling practices meet rigorous international criteria. For more information on our security measures, please visit UCloud's security documentation.</p> <p></p>"},{"location":"index.html#contributions","title":"Contributions","text":"<p>Besides our models DFM has led to a series of positive open-source contributions, the following table include some of these contributions:</p> Project Contribution Packages NLPDedup A deduplication library derived from DFM's deduplication code Code contributions TextDescriptives Added heuristic quality measure for texts dolma Bugfixes and addition of taggers for filtering Benchmarks ScandEval Co-contributors have significant contributions to developing NLU and NLG benchmarks for Scandinavian and Germanic languages Scandinavian Embedding Benchmark The benchmark for evaluating Scandinavian embedding has been created as a part of DFM Datasets m_arc, m_mmlu, m_hellaswag, m_truthfulqa Translated versions of English datasets intended for model evaluation for these domains"},{"location":"index.html#improving-the-danish-language-technology-landscape","title":"Improving the Danish Language Technology Landscape","text":"<p>The Danish Foundations models is a collaboration across Danish universities and research organizations. The project engages with data science communities and initiatives (Danish Data Science Community), to promote the development of Danish language tools. We continually gather information about how to improve the Danish language technologies and how to best support the community. If you want to highlight missing pieces in Danish NLP we invite you to open a thread on the forum stating the problems and potential solutions.</p>"},{"location":"index.html#contributors","title":"Contributors","text":""},{"location":"index.html#the-core-team","title":"The Core Team","text":"<p>Those with data access, who contribute to the project, including data management, model development, project management, and more.</p> <p>From the Center for Humanities Computing at Aarhus University:</p> <ul> <li>Kenneth Enevoldsen (kenneth.enevoldsen@cas.au.dk)</li> <li>Marton Kardos (martonkardos@cas.au.dk)</li> <li>Jan Kostkan (jan.kostkan@cas.au.dk)</li> <li>Peter Vahlstrup (imvpbv@cc.au.dk)</li> <li>Per M\u00f8ldrup-Dalum (per@cas.au.dk)</li> <li>Kristoffer Laigaard Nielbo (kln@cas.au.dk)</li> </ul> <p>From the Alexandra Institute:</p> <ul> <li>Rasmus Larsen (rasmus.larsen@alexandra.dk)</li> <li>Dan Saattrup Nielsen (dan.nielsen@alexandra.dk)</li> <li>Andreas Nugaard Holm (andreas.holm@alexandra.dk)</li> <li>Kristian N\u00f8rgaaard Jensen (kristian.n.jensen@alexandra.dk)</li> <li>Torben Blach (torben.blach@alexandra.dk)</li> <li>Jens Kaas Benner (jens.benner@alexandra.dk)</li> </ul> <p>From the Center for Machine Learning at the University of Southern Denmark:</p> <ul> <li>Peter Schneider-Kamp (petersk@imada.sdu.dk)</li> <li>Lukas Galke (galke@imada.sdu.dk)</li> <li>Andrea Blasi N\u00fa\u00f1ez (abln@mmmi.sdu.dk)</li> <li>Gianluca Barmina (gbarmina@imada.sdu.dk)</li> <li>Jacob Nielsen (jacn@imada.sdu.dk)</li> <li>Mogens Henrik From (from@imada.sdu.dk)</li> <li>Stine Lyngs\u00f8 Beltoft (stinelb@imada.sdu.dk)</li> </ul> <p>From the Department of Computer Science at the University of Copenhagen:</p> <ul> <li>Desmond Elliott (de@di.ku.dk)</li> </ul> <p>From Center for Sprogteknologi at the University of Copenhagen:</p> <ul> <li>Bolette Sandford Pedersen (bspedersen@hum.ku.dk)</li> <li>Ali Basirat (alib@hum.ku.dk)</li> </ul> Project Alumnis   Lasse Hansen, Martin Bernstorff, Tao Tang"},{"location":"index.html#core-contributors","title":"Core Contributors","text":"<p>Those without data access, but who have contributed substantially to the project including code contributions, model development, and experiment planning.</p> <p>From Alvenir:</p> <ul> <li>Martin Carsten Nielsen (martin@alvenir.ai)</li> <li>S\u00f8ren Vejlgaard Holm (swh@alvenir.ai)</li> </ul>"},{"location":"index.html#join-us","title":"Join Us","text":"<p>We invite collaboration and contributions from industry professionals, researchers, and the open-source community. Together, we can advance the field of Danish NLP and create a more inclusive digital future. You can reach out to us using the following channels:</p>  - DDSC Slack Join the discussion in the \"danish-foundation-models-text\"-channel  -  GitHub Discussion Ask questions or start a discussion  - GitHub Issues Noticed a bug in the code? Please create an issue  - Using the model? If you use the model, let us know it makes it easier for us to apply for funding and justify the devopment of the project. <p>Contact us </p>"},{"location":"dcc.html","title":"DCC <sub>v1</sub>","text":"<p>The DCC is a composite corpus consisting of the following subcorpora. For more information about the specific subcorpora, feel free to check out the individual datasheets.</p> Name Description Size Open Access Novel Corpus Text DAGW Danish Gigaword 1B tokens \u2713 \u2717 reddit-da Danish Reddit &lt;.1B tokens \u2713 \u2717 HopeTwitter Danish Tweets 0.48B tokens \u2717 \u2713 DaNews Danish newspapers 0.5B tokens \u2717 \u2713 Netarkivet Text Danish internet &gt;100B tokens \u2717 \u2713 Speech DaRadio Danish talk radio 140,000 hours \u2717 \u2713 DaTV Danish subtitled TV 900 hours \u2717 \u2713"},{"location":"dcc.html#collaborators-and-data-owners","title":"Collaborators and Data Owners","text":"<p>Data are provided in agreement with the data owners and data collaborators. The data is generally accecible by the research collaborators, though  each data agreements has their own access restrictions and might not cover all research collaborators. Access restriction are specified on the server hosting the data in accordance with the data agreements.</p> <ul> <li>Data Owners</li> <li>Aviser / dagblade</li> <li>Danmarks Statistik</li> <li>NetArkivet</li> <li>Data Collaborators</li> <li>Det Kongelige bibliotek</li> <li>Infomedia</li> <li>Research Collaborators</li> <li>Center for humanities Computing, Aarhus Universitet</li> <li>Alexandra Institutet</li> <li>Peter Schneider-Kamp, Syddansk Universitet</li> </ul>"},{"location":"dfm_data.html","title":"DFM Data: A Composite Dataset for Danish LLMs.","text":"<p>This page provides a detailed description of the composite dataset used to train large language models developed by Danish Foundation Models. The dataset is curated to offer a diverse and comprehensive corpus across multiple domains, including legal, financial, and literary texts, with the primary intention of developing language models for Danish.</p>"},{"location":"dfm_data.html#dataset-description","title":"Dataset Description","text":""},{"location":"dfm_data.html#summary","title":"Summary","text":"<p>The DFM Data is a collection of datasets used for Danish Foundation Models. This repository ensure documentation to data along with FAIR data practices.</p>"},{"location":"dfm_data.html#curation-rationale","title":"Curation Rationale","text":"<p>These datasets were collected and curated with the intention of developing language models for Danish.</p>"},{"location":"dfm_data.html#data-collection-and-processing","title":"Data Collection and Processing","text":"<p>The dataset was constructed by collecting and integrating text from a wide variety of public and partner-provided sources. The raw data was subjected to a standardized cleaning pipeline, which included steps such as deduplication, filtering of low-quality content to prepare it for large-scale language model training.</p>"},{"location":"dfm_data.html#dataset-statistics","title":"Dataset Statistics","text":"<ul> <li>Number of samples: 230.07M</li> <li>Number of tokens (Llama 3): 430.24B</li> <li>Average document length in tokens (min, max): 1.87K (1, 51.77M)</li> </ul> <p>The following plot pr. dataset histograms displaying document lengths.</p> <p> </p>"},{"location":"dfm_data.html#languages","title":"Languages","text":"<p>This dataset includes the following languages:</p> <ul> <li>Danish</li> <li>English</li> <li>Swedish</li> <li>Norwegian Bokm\u00e5l</li> <li>Norwegian Nynorsk</li> </ul> <p>Below is a visualisation of the main languages in each of the datasets.</p> <p> </p>"},{"location":"dfm_data.html#domains","title":"Domains","text":"<p>This dataset consist of data from various domains (e.g., legal, books, social media). The following table and figure give an overview of the relative distributions of these domains.</p> Domain Sources N. Tokens Legal retsinformationdk, retspraksis, skat, fm-udgivelser, eur-lex-sum-da, miljoeportalen, cellar, domsdatabasen, caselaw_access_project_filtered, uspto_filtered 162.19B Other dannet, depbank, synne, dsk-cbrain, dsk-hofor, dsk-plesner, dsk-vitec, ncc_parliament, data_provenance_initiative_filtered, public_domain_review_filtered, stackv2_edu_filtered 63.56B Scientific arxiv_abstracts_filtered, arxiv_papers_filtered, peS2o_filtered 46.15B Books adl, gutenberg, jvj, relig, wikibooks, memo, ncc_books, dbc-abstracts, dbc-reviews, danish-pd, grundtvig, biodiversity_heritage_library_filtered, doab_filtered, library_of_congress_filtered, libretexts_filtered, oercommons_filtered, pre_1929_books_filtered, pressbooks_filtered, project_gutenberg_filtered 37.19B Medical health_hovedstaden, pubmed_filtered 35.35B Conversation ep, ft, naat, spont, danske-taler, opensubtitles, github_archive_filtered, stackexchange_filtered, ubuntu_irc_filtered 34.30B Encyclopedic wiki, wikisource, dbc-faktalink, dbc-forfatterweb, wikimedia_filtered, wikiteam_filtered 17.21B Web dsk-alexandra, dsk-atp, dsk-salling, dsk-vejle, ai-aktindsigt, ncc_maalfrid, cccc_filtered 14.20B Governmental plandata, regulations_filtered, uk_hansard_filtered, usgpo_filtered 12.10B Speeches youtube_filtered 4.07B Financial cvr-reports 2.32B News tv2r, dsk-danskerhverv, dsk-dkmedier, dsk-ida, dsk-odense, nordjyllandnews, ncc_newspaper, enevaeldens_nyheder, news_filtered 1.22B Social Media hest 389.32M Readaloud nota 7.30M Technical python_enhancement_proposals_filtered 2.54M Dialect botxt 847.97K Total 430.24B <p> </p>"},{"location":"dfm_data.html#licensing","title":"Licensing","text":"<p>The following gives an overview of the licensing in the DFMv1. To get the exact license of the individual datasets check out the individual datasets by clicking the links in the table. These license is applied to the constituent data, i.e., the text. The collection of datasets (metadata, quality control, etc.) is licensed under CC-0.</p> License Sources N. Tokens Public Domain danish-pd, python_enhancement_proposals_filtered, regulations_filtered, ubuntu_irc_filtered, usgpo_filtered, uspto_filtered 153.74B CC-BY-SA 4.0 depbank, jvj, tv2r, fm-udgivelser, eur-lex-sum-da, memo, cellar, doab_filtered, libretexts_filtered, news_filtered, oercommons_filtered, peS2o_filtered, pressbooks_filtered, public_domain_review_filtered, pubmed_filtered, stackexchange_filtered, wikimedia_filtered, wikiteam_filtered, youtube_filtered 122.21B CC-0 adl, botxt, ep, ft, hest, naat, relig, retspraksis, skat, spont, synne, wiki, wikibooks, wikisource, danske-taler, miljoeportalen, nordjyllandnews, nota, opensubtitles, ncc_books, ncc_newspaper, health_hovedstaden, grundtvig, enevaeldens_nyheder, arxiv_abstracts_filtered, arxiv_papers_filtered, biodiversity_heritage_library_filtered, caselaw_access_project_filtered, cccc_filtered, data_provenance_initiative_filtered, library_of_congress_filtered, pre_1929_books_filtered, project_gutenberg_filtered 74.04B Various - MIT, BSD-3-Clause, Apache-2.0, etc. github_archive_filtered, stackv2_edu_filtered 72.60B Verbal agreement cvr-reports 2.32B Open Parliament License uk_hansard_filtered 2.01B Written agreement (public models, private data) plandata, dbc-abstracts, dbc-faktalink, dbc-forfatterweb, dbc-reviews 1.78B Other (No attribution required) retsinformationdk, domsdatabasen 904.61M Other (Attribution required) dannet, gutenberg, ai-aktindsigt, ncc_maalfrid, ncc_parliament 515.61M DSK-1 dsk-alexandra, dsk-atp, dsk-cbrain, dsk-danskerhverv, dsk-dkmedier, dsk-hofor, dsk-ida, dsk-odense, dsk-plesner, dsk-salling, dsk-vejle, dsk-vitec 113.35M Total 430.24B"},{"location":"dfm_data.html#additional-information","title":"Additional Information","text":""},{"location":"dfm_data.html#citation-information","title":"Citation Information","text":"<p>If you use a model trained on this dataset, please cite the associated DFM project or research paper when it becomes available. A BibTeX entry will be provided here upon the official release of a corresponding paper.</p>"},{"location":"dfm_data.html#disclaimer","title":"Disclaimer","text":"<p>We do not own any of the text from which the data has been extracted. If you believe that we are not allowed to train on any of the datasets noted please do contact us.</p>"},{"location":"dfm_data.html#notice-and-take-down-policy","title":"Notice and take down policy      A\u00a0Danish Foundation Models\u00a0dataset","text":"<p>Notice: Should you consider that our data contains material that is owned by you and should therefore not be included in the training of LLMs here, please:</p> <ul> <li>Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.</li> <li>Clearly identify the copyrighted work claimed to be infringed.</li> <li>Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.</li> </ul> <p>You can contact us by making an issue.</p> <p>Take down: We will comply to legitimate requests by removing the affected sources from the next release of the corpus.</p>"},{"location":"intercoder_reliability.html","title":"Results from corpus tagging","text":"<p>Each user tagged 100 documents unless otherwise specified. Documents were split by newlines into text-blocks, block was rated. Text-blocks longer than 1000 characters were split into multiple blocks of 1000 characters or less.</p> <p>This tagging scheme is similar to (Kreutzer et al., 2022).</p> <p>Each block was put into one of the following categories: Each user tagged 100 documents (unless otherwise specified). Each document were tagged</p> <ul> <li><code>wrong_language</code>: Not Danish</li> <li><code>skipped</code>: Unsure of category</li> <li><code>correct_language</code>: Danish text where at least 80% of the text is reasonable.</li> <li><code>not_language</code>: Text where less than 80% of the text is reasonable. Takes priority over <code>wrong_language</code>.</li> </ul> <p>Additionally, each block was tagged for pornography (yes/no) and offensiveness (yes/no).</p>"},{"location":"intercoder_reliability.html#text-proportions","title":"Text proportions","text":"<p>Kenneth (Session: test)</p> <ul> <li>Date: 2022-09-05</li> <li>Sentences tagged: 102</li> <li>Documents tagged: na</li> </ul> <p>Proportions:</p> <ul> <li>69.16% of characters is <code>correct_language</code></li> <li>25.66% of characters is <code>not_language</code></li> <li>2.74% of characters is <code>skipped</code></li> <li>2.45% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 292</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.03% of characters is <code>correct_language</code></li> <li>29.19% of characters is <code>not_language</code></li> <li>2.10% of characters is <code>skipped</code></li> <li>0.68% of characters is <code>wrong_language</code></li> <li>0.00% of characters is porn</li> <li>1.38% of characters is offensive</li> </ul> <p>Lasse (Session: 1)</p> <ul> <li>Date: 2022-09-07</li> <li>Sentences tagged: 336</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>68.02% of characters is <code>correct_language</code></li> <li>30.97% of characters is <code>not_language</code></li> <li>1.01% of characters is <code>wrong_language</code></li> <li>0.26% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"intercoder_reliability.html#intercoder-reliability","title":"Intercoder Reliability","text":"<p>Kenneth (Session: test) vs Kenneth - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8242 (Overlap in sentences: 98)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.9075 (Overlap in sentences: 98)</p> </li> </ul> <p>Kenneth (Session: test) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.8140 (Overlap in sentences: 95)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.8389 (Overlap in sentences: 95)</p> </li> </ul> <p>Kenneth (Session: 1) vs Lasse - (Session: 1)</p> <ul> <li> <p>Cohen's Kappa (all categories): 0.6767 (Overlap in sentences: 245)</p> </li> <li> <p>Cohen's Kappa (correct_language vs not correct_language): 0.7259 (Overlap in sentences: 245)</p> </li> </ul> <p>Comparison with mC4</p> <p>Note: mC4 did have a high degree of repititious texts. Similarly it did when texts blocks where not language they were often something like:</p> <pre><code>2lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22nowiki%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;]&amp;#x200b;&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&amp;#x200b;&lt;/span&gt;, at en lurifaks som Jimmy page, bruger MIT navn til opfindelsen! SV&lt;span data-rte-instance=\"\"1524-12953202845f3523698f3f1\"\" data-rte-meta=\"\"%7B%22type%22%3A%22ext%22%2C%22wikitext%22%3A%22%3Cref%3ESVIN%3C%5C%2Fref%3E%22%2C%22lineStart%22%3A%22%22%2C%22placeholder%22%3A1%2C%22extName%22%3A%22ref%22%7D\"\" class=\"\"placeholder placeholder-ext\"\" contenteditable=\"\"false\"\"&gt;&lt;sup data-rte-washtml=\"\"1\"\" id=\"\"cite_ref-2\"\" class=\"\"reference\"\" data-rte-attribs=\"\"\n</code></pre> <p>While non-language texts in NAT was often menu bars, contact information, or navigation.</p> <p>Kenneth (Session: 1)</p> <ul> <li>Date: 2022-09-06</li> <li>Sentences tagged: 325</li> <li>Documents tagged: 100</li> </ul> <p>Proportions:</p> <ul> <li>62.47% of characters is <code>correct_language</code></li> <li>34.88% of characters is <code>not_language</code></li> <li>1.27% of characters is <code>skipped</code></li> <li>1.38% of characters is <code>wrong_language</code></li> <li>3.25% of characters is porn</li> <li>0.00% of characters is offensive</li> </ul>"},{"location":"models.html","title":"Models","text":"<p>This section gives an overview of the models available through the DFM project. The models are available through the Huggingface model hub. To avoid duplicating information surrounding the models and the information regarding the models are available at the models model sheet.</p> <p>Model recommendations</p> <p>Danish foundation models maintains a list of state-of-the-art recommendations This list is updated approximately once per year to reflect the best available models for various tasks in Danish language and speech processing.</p>"},{"location":"models.html#text-models","title":"Text Models","text":"Model Model type Size (parameters) munin-7b-alpha Decoder 7.24B dfm-sentence-encoder-large Encoder large (355M) dfm-sentence-encoder-medium Encoder medium (110M) Previously released models <p>Previously the DFM project released the following text models, however these models were taken down due to copyright concerns. Preventative measures have been taken to ensure that future models do not have the same issues.</p> Model Model type Size (parameters) encoder-large-v1 Encoder large (355M) encoder-medium-v1 Encoder medium (110M) encoder-small-v1 Encoder small (22M) <p>Similarly the DFM project previously released the following speech models which were also taken down due to copyright concerns.</p> Model Model type xls-r-300m-danish Pretrained wav2vec2.0 model xls-r-300m-danish-nst-cv9 Automatic speech recognition chcaa/xls-r-300m-nst-cv9-da Automatic speech recognition <p>We refer to our state-of-the-art model recommendations for an best alternatives to these models.</p>"},{"location":"blog/index.html","title":"Posts","text":""},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html","title":"The Imperative of Danish Foundation Models: Bridging the Linguistic AI Divide","text":"<p>In recent years, the field of machine learning has experienced a transformative shift, primarily driven by the advent of foundation models. These models, pre-trained on vast amounts of data, can be finetuned for various downstream tasks, making them invaluable across multiple domains. However, the dominance of the English language in the development of these models poses significant challenges for smaller language communities. The Danish Foundation Models project emerges as a crucial initiative to ensure that the Danish language does not lag behind in this AI revolution.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html#the-case-for-danish-foundation-models","title":"The Case for Danish Foundation Models","text":"<p>The global landscape of foundation models is heavily skewed towards English, with few models catering to other languages. Although multilingual models exist, they often fail to capture the unique linguistic and cultural nuances of smaller languages like Danish. This discrepancy is particularly evident in practical applications where cultural context matters, such as healthcare services or public administration. The Danish Foundation Models project aims to fill this gap by developing high-quality, open-source foundation models specifically for the Danish language.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html#challenges-in-developing-danish-language-models","title":"Challenges in Developing Danish Language Models","text":"<ol> <li> <p>Computational Resources: Danish models have historically been trained with limited computational resources compared to their English counterparts. This disparity in resources leads to less effective models.</p> </li> <li> <p>Data Quality and Quantity: The datasets available for training Danish models are significantly smaller and less diverse. High-quality benchmarks and datasets, crucial for training robust models, are often lacking.</p> </li> <li> <p>Model Documentation: Proper documentation, including model cards and datasheets, is essential for ensuring AI models' ethical and effective use. Danish models frequently suffer from inadequate documentation, impeding their adoption in critical sectors.</p> </li> </ol>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html#the-danish-foundation-models-project","title":"The Danish Foundation Models Project","text":"<p>To address these challenges, the Danish Foundation Models (DFM) project has outlined four primary objectives:</p> <ol> <li> <p>Developing State-of-the-Art Models: Creating and maintaining advanced language models for Danish text and speech applications.</p> </li> <li> <p>Extensive Validation: Rigorous testing of these models across a representative set of tasks to ensure their efficacy and reliability.</p> </li> <li> <p>High-Quality Documentation: Maintaining comprehensive documentation for all models, promoting transparency and trust.</p> </li> <li> <p>Open-Source Collaboration: Ensuring that all models and their training processes are openly available to the community, fostering reproducibility and further innovation.</p> </li> </ol>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html#future-directions","title":"Future Directions","text":"<p>The DFM project plans to develop open-source language models for NLP, NLU, and ASR systems in Danish. Upcoming benchmarks will include data from diverse domains, such as healthcare and legal, ensuring comprehensive evaluation criteria for future models.</p>"},{"location":"blog/2024/07/21/the-imperative-of-danish-foundation-models-bridging-the-linguistic-ai-divide.html#conclusion","title":"Conclusion","text":"<p>The Danish Foundation Models project exemplifies a concerted effort to bridge the linguistic AI divide. By focusing on high-quality, well-documented, and openly accessible models, the DFM initiative not only safeguards the Danish language's digital presence but also sets a precedent for other smaller language communities. As we move forward, the collaboration between academia, industry, and the open-source community will be pivotal in sustaining and advancing this crucial work.</p>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html","title":"Tutorial: Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</code></pre> <pre><code># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#configure-the-model","title":"Configure the Model","text":"<pre><code>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#load-the-model","title":"Load the Model","text":"<pre><code>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#load-and-prepare-data","title":"Load and Prepare Data","text":"<p>Load the dataset from Hugging Face Hub:</p> <pre><code>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</code></pre> <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> <pre><code>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</code></pre> <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> <pre><code>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#finetune","title":"Finetune!","text":"<pre><code>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</code></pre> <pre><code># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</code></pre> <pre><code># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</code></pre> <pre><code># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#try-it-out","title":"Try it Out","text":"<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</code></pre> <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks","text":""},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM","text":"<p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> <pre><code># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> <pre><code># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-finetuning-language-models.html#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion","text":"<p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> <pre><code># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</code></pre> <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html","title":"Tutorial: Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook.  It has descriptions and examples for the different merge methods it supports.</p>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#install-dependencies","title":"Install Dependencies","text":"<pre><code># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</code></pre> <pre><code># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#get-hugging-face-token","title":"Get Hugging Face Token","text":"<p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> <pre><code>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#configure-the-merge","title":"Configure the Merge","text":"<p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> <pre><code>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</code></pre> <pre><code>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#merge","title":"Merge!","text":"<pre><code>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#try-it-out","title":"Try it Out","text":"<p>Time to try out the new merged model. Let's start by loading it from disk.</p> <pre><code>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</code></pre> <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> <pre><code>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</code></pre> <pre><code>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#share-the-model","title":"Share the Model","text":"<p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> <pre><code># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</code></pre>"},{"location":"blog/2024/02/02/tutorial-merging-language-models.html#clean-up","title":"Clean Up","text":"<p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> <pre><code># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</code></pre>"},{"location":"blog/2024/01/11/releasing-munin-7b-alpha---a-danish-llm.html","title":"Releasing Munin 7B Alpha - A Danish LLM","text":"<p>We are excited to announce the release of the first model from the Danish Foundation Models project, nicknamed Munin 7B Alpha. This model represents the beginning of our research into Danish Large Language Models (LLMs), employing continual pre-training based on the already pre-trained Mistral-7b-v0.1 model. It has been pre-trained on the Danish Gigaword dataset, which has been instrumental in training various Danish BERT-style models.</p> <p>The model has been trained for one epoch over the dataset and ends up with a loss 1.27 on the Danish Gigaword. See more the model training logs here.</p> <p>This release underscores our commitment to transparency about our work and the challenges we are facing. We want to clearly note that we expect the model to perform suboptimally for many, if not most, applications. Our evaluations on the limited generative Danish tasks available to us have indicated that our current training approach may negatively impact performance on these downstream tasks, even compared to the upstream Mistral model.</p> Model Name Overall Score Danish Score Norwegian Score Swedish Score gpt-3.5-turbo-0613 58.52 \u00b1 2.42 56.72 \u00b1 2.44 57.31 \u00b1 2.37 61.54 \u00b1 2.46 mistralai/Mistral-7B-v0.1 40.30 \u00b1 2.15 39.60 \u00b1 1.94 35.98 \u00b1 2.54 45.31 \u00b1 1.96 danish-foundation-models/munin-7b-alpha 37.50 \u00b1 2.49 39.56 \u00b1 2.70 30.82 \u00b1 2.69 42.13 \u00b1 2.07 AI-Sweden-Models/gpt-sw3-6.7b-v2 26.67 \u00b1 2.30 23.65 \u00b1 2.02 24.28 \u00b1 2.74 32.08 \u00b1 2.13 mhenrichsen/danskgpt-tiny 16.87 \u00b1 3.05 16.66 \u00b1 2.18 15.16 \u00b1 2.64 18.80 \u00b1 4.35 <p>See the full ScandEval leaderboard for an up-to-date comparison. Despite these challenges, we hope that our open approach encourages the community to collaborate with us in building the best possible Danish LLM. While the current version of the model may not yet be a practical tool for Danish NLP, we believe that sharing our findings is valuable. A critical need has been identified: access to a significantly larger corpus of Danish text data, and a legal framework that reliably allows for training and releasing open models, including for commercial use.</p> <p>At Danish Foundation Models, we are actively pursuing legal access to extensive Danish text data, and are exploring every option for releasing models under the most open license possible. We have already secured agreements that provide us access to several large Danish datasets, and we plan to include these into our training process in the near future.</p> <p>In summary, Munin 7B Alpha is a small step forward. It signifies our commitment to advancing Danish NLP and acknowledges the extensive work ahead. By sharing this model, we aim to foster collaborative efforts within the community. The model is now available for download and experimentation, and we look forward to your insights and discussions on how we can progress.</p> <p>The development of this model, and the Danish Foundation Models project in general, is generously supported by the following:</p> <ul> <li>Danish e-infrastructure Consortium</li> <li>Acquisition and Logistics Organisation at the Danish Ministry of Defence</li> <li>Danish Ministry of Higher Education and Science under the Digital Security, Trust   and Data Ethics performance contract</li> </ul>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html","title":"Datah\u00e5ndtering","text":"<p>For at kunne tr\u00e6ne sprogmodeller (Large Language Models, LLM) skal der store m\u00e6ngder af data til. Fra vi modtager r\u00e5data til at de kan bruges til at tr\u00e6ne sprogmodeller p\u00e5, gennemg\u00e5r de en transformationsprocess.</p> <p>F\u00f8lgende er en overordnet beskrivelse af denne processen. Vi udvikler og forbedre l\u00f8bende processen, for at sikre at vi bruger state-pf-the-art metoder og praksis.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#sikker-handtering","title":"Sikker h\u00e5ndtering","text":"<p>I Danish Foundation Model bruger vi Danish e-infrastructure Consortium DeiC og UCloud til data h\u00e5ndtering. UCloud platformen er ISO27001 godkendt. Det er globalt anerkendt standard, der sikrer at vores datah\u00e5ndteringspraksis opfylder strenge internationale kriterier. For mere information om vores sikkerhedsforanstaltninger, se UClouds sikkerhedsdokumentation.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#dataklargrelse","title":"Dataklarg\u00f8relse","text":"<p>Alt dataklarg\u00f8relse foreg\u00e5r p\u00e5 UCloud. Figuren viser den proces alt data skal igennem f\u00f8r det bruges til tr\u00e6ning af sprogmodellen. Det r\u00e5 data beholdes i sin oprindelige form p\u00e5 UCloud. Derefter annoteres de r\u00e5 data med metadata.</p> <p>Dette datas\u00e6t overf\u00f8res til en GPU-accelereret supercomputer igennem en sikker forbindelse, hvorefter selve tr\u00e6ningen af modellen begyndes. Under tr\u00e6ningen gemmes flere checkpoints med modelv\u00e6gte. De gemte checkpoints med modelv\u00e6gte publiceres sammen med modelkode og anvendes til at k\u00f8re modellen. De tre processer er beskrevet i detalje nedenfor.</p> <p></p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#metadata-og-formatering","title":"Metadata og formatering","text":"<p>Det r\u00e5 data annoteres med to typer af metadata. Den f\u00f8rste type er et datablad (i Markdown, som i HuggingFace dataset cards der opsummerer hele datas\u00e6ttet og beskriver bl.a. proveniens og hvilken licens der er p\u00e5lagt det givne datas\u00e6t. Et udsnit af et databladseksempel er vist nedenfor. Den f\u00f8rste del af databladet er annoteret i et maskinvenligt format, som g\u00f8r det muligt automatisk at udv\u00e6lge datas\u00e6ttet blandt en st\u00f8rre samling. Resten af databladet giver en dybere beskrivelse af datas\u00e6ttet i fritekst.</p> <pre><code>---\npretty_name:  Scrape from Hovedstaden\nlanguage:\n  - da\nlicense: cc0-1.0\nlicense_name: Creative Commons Zero v1.0 Universal\nsize_categories:\n  - 10K&lt;n&lt;100K\ntask_categories:\n  - text-generation\n  - fill-mask\ntask_ids:\n  - language-modeling\n---\n# Dataset Card for scape_hovedstaden\n## Dataset Description\n- **Number of records:** 24752\n- **Languages:** Danish\n</code></pre> <p>Den anden type af metadata er per-dokument metadata, der beskriver hvilket datas\u00e6t dokumentet h\u00f8rer til, hvor det stammer fra, hvorn\u00e5r det er tilf\u00f8jet, samt andre metadata som f.eks. fra hvilken URL dokumentet kommer fra. Per-dokument metadata gemmes sammen med dokumentet i et standardiseret jsonl format. Et eksempel p\u00e5 et enkelt dokument inklusiv metadata fra datas\u00e6ttet \"Scrape from Hovedstaden\" er vist nedenfor. Disse metadata f\u00f8lger dokumentet igennem hele processeringen, s\u00e5 det er muligt at spore dokumenterne tilbage til kilden fra det endelige tr\u00e6ningskorpus. For hvert r\u00e5 datas\u00e6t vedligeholdes et script der kan bruges til konvertering af de r\u00e5 data til det standardiserede format.</p> <pre><code>{\n    'id': 'doc_hovedstaden_Rigshospitalet_Bed\u042bvelse og Intensiv Behandling (NEU)_Transkraniel Doppler - NIA 6021',\n    'text': 'Transkraniel Doppler - NIA 6021\\n\\nM\u00e5lgrupper og anv...',\n    'source': 'scrape_hovedstaden',\n    'added': '2024-05-23',\n    'created': '2023-11-16, 2024-04-04',\n    'metadata': {\n        'subject': 'health',\n        'language': 'danish',\n        'organization': 'The Danish Agency for Digitalisation',\n        'source-pretty': 'University of Southern Denmark (SDU) &amp; Capital Region',\n        'URL': 'https://sprogteknologi.dk/dataset/1076892a-14ee-4f14-a9db-32efb03c40c9'\n    }\n}\n</code></pre> <p>Flere detaljer om formatet er beskrevet her.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#filtrering","title":"Filtrering","text":"<p>Det standardiserede format muligg\u00f8r en ensartet processering af dokumenterne. De enkelte filtreringstrin kan inddeles i f\u00f8lgende kategorier:  - URL-filter (kun for web-data)  - Linje-deduplikering  - Kvalitetsfilter  - Fjernelse af personoplysninger  - Dokument-deduplikering</p> <p>De enkelte trin er beskrevet i nedenst\u00e5ende afsnit. Efter filtreringstrinene bliver vores tekstdata tokenized, dvs. konverteret til et bin\u00e6rt format der kan l\u00e6ses af modellen.</p> <p></p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#url-filtrering","title":"URL-filtrering","text":"<p>Data som kommer fra offentlige hjemmesider og dermed har en URL som metadata, bliver f\u00f8rst processeret af et URL-filter.</p> <p>For alle dom\u00e6ner i datas\u00e6ttet hentes dom\u00e6nets robots.txt og ai.txt periodisk. Hvis disse ikke tillader CommonCrawl eller andre sprogmodel-crawlers tilf\u00f8jes dom\u00e6net til en blokeringsliste og dokumenter der stammer fra disse dom\u00e6ner filtreres v\u00e6k, selv om de p\u00e5g\u00e6ldende sider m\u00e5tte v\u00e6re hentet p\u00e5 et tidspunkt, hvor robots.txt/ai.txt ikke blokerede for denne type for crawling.</p> <p>Derudover anvendes blokeringslister fra forskellige offentligt tilg\u00e6ngelige databaser over skadeligt indhold. Vi bruger datatrove's indbyggede filter samt Dolma's samling af blokeringslister. Disse lister omfatter bl.a. f\u00f8lgende kategorier:</p> <ul> <li>Porno (b\u00e5de via lister og via ord der indg\u00e5r i dom\u00e6ne-navnet)</li> <li>Phishing</li> <li>Reklamer</li> <li>Kriminelle sider</li> <li>Abuse</li> <li>Fraud</li> <li>Malware</li> <li>Pirat</li> <li>Ransomware</li> <li>Scam</li> <li>Redirect</li> <li>Crypto</li> <li>Drugs</li> <li>Gambling</li> <li>Vaping</li> <li>Social Networks</li> </ul>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#deduplikering","title":"Deduplikering","text":"<p>Deduplikering anvendes til at fjerne gentagelser. Gentagelser i tr\u00e6ningsdata kan p\u00e5virke modellen i en u\u00f8nsket retning. Der anvendes to typer af deduplikering linje-deduplikering og dokument-deduplikering</p> <p>Linje-deduplikering er en proces hvor gentagne linjer fjernes p\u00e5 tv\u00e6rs af dokumenter. Dette er is\u00e6r anvendeligt p\u00e5 web-data, hvor f.eks. cookie notifikationer og menuer gentages p\u00e5 tv\u00e6rs af mange sider. Denne type af deduplikering implementeres effektivt vha. et s\u00e5kaldt Bloom filter. Visse typer af datas\u00e6t kan med fordel fritages for linje-deduplikering. F.eks. vil der i juridiske dokumenter ofte indg\u00e5 en r\u00e6kke standard formuleringer og deduplikering af disse kan \u00f8del\u00e6gge dokumenternes betydning.</p> <p>I dokument-deduplikering sammenlignes alle dokumenter p\u00e5 tv\u00e6rs af det rensede dokumentkorpus og dokumenter der indholdsm\u00e6ssigt er tilpas t\u00e6t p\u00e5 hinanden grupperes i en klynge. Fra hver klynge udtr\u00e6kkes \u00e9t enkelt dokument. P\u00e5 den m\u00e5de undg\u00e5s at visse dokumenter bliver overrepr\u00e6senteret i det endelige datas\u00e6t. </p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#kvalitetsfilter","title":"Kvalitetsfilter","text":"<p>Web-data kan indeholde meget st\u00f8j i form af stumper af HTML eller andet kode og ufuldst\u00e6ndige s\u00e6tninger. Der anvendes forskellige heuristikker, som er baseret p\u00e5 statistik for almindelig tekst, der fanger disse dokumenter af d\u00e5rlig kvalitet. Vi bruger p.t. samme filtre som Gopher og C4, men der unders\u00f8ges ogs\u00e5 mulighed for filtrering baseret p\u00e5 perpleksitet og andre metrikker.</p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#personhenfrbar-information","title":"Personhenf\u00f8rbar Information","text":"<p>N\u00e5r en model tr\u00e6nes p\u00e5 data, som indeholder personhenf\u00f8rbar information, medf\u00f8rer det en risiko for at modellen reproducerer denne information under k\u00f8rsler. S\u00e5 vidt det er muligt detekteres disse kategorier og erstattes med generiske erstatninger af samme type. Eksempler p\u00e5 personhenf\u00f8rbar information er: Navne, e-mails, telefonnumre, CPR-numre.</p> <p>En udfordring er at hverken menneskelig eller maskinel fjernelse af personhenf\u00f8rbar information er 100% n\u00f8jagtigt, s\u00e5 datas\u00e6t uden disse er at foretr\u00e6kke. </p>"},{"location":"blog/2024/07/09/datah%C3%A5ndtering.html#dialog-om-data","title":"Dialog om data","text":"<p>Vi har den st\u00f8rste respekt for dem, der ejer data. Vi forst\u00e5r, hvor vigtigt det er at beskytte og respektere dataejeres \u00f8nsker om hvad og hvordan deres data m\u00e5 bruges til. </p> <p>Hvis du har nogen sp\u00f8rgsm\u00e5l vedr\u00f8rende de data vi bruger, er du altid velkommen til at kontakte os. Vi er meget \u00e5bne for dialog og s\u00e6tter pris p\u00e5 input, da det hj\u00e6lper os med at forbedre vores praksis og sikre, at vi lever op til dataejeres \u00f8nsker. </p> <p>Din feedback er vigtig for os, og vi ser frem til at h\u00f8re fra dig.</p>"},{"location":"blog/2024/07/04/datakilder.html","title":"Datakilder","text":"<p>De data som sprogmodeller tr\u00e6nes p\u00e5 er afg\u00f8rende for hvad de kan bruges til. I Danish Foundation Models (DFM) er tilgangen at vi skal have sikkerhed for at vi m\u00e5 benytte de data vi tr\u00e6ner p\u00e5 fra data ejere, samt at vi har fokus p\u00e5 v\u00e6rdiskabende use-cases. Dette g\u00f8r vi blandt andet gennem samarbejdet med Dansk Sprogmodel Konsortium.</p>"},{"location":"blog/2024/07/04/datakilder.html#nuvrende-datakilder","title":"Nuv\u00e6rende datakilder","text":"<p>Vi arbejder kontinuerligt p\u00e5 at indsamle data fra flere kilder. Nedenst\u00e5ende tabel indeholder kilder som lige nu, efter bedste overbevisning, kan anvendes til tr\u00e6ning af en dansk sprogmodel. M\u00e6ngden af data vi har nu, er ikke tilstr\u00e6kkelig til at tr\u00e6ne en dansk sprogmodel fra grunden. St\u00f8rrelsen er angivet i antal tegn.</p> Datas\u00e6t Dato Dom\u00e6ne Licens St\u00f8rrelse AI aktindsigt nutidig Kommunale hjemmesider CC0-1.0 408M Domsdatabasen 1855-nu Domme CC0-1.0 91.2M Eur-lex-sum-da 1993-nu Jura (EU) CC-BY-SA 4.0 87.8M FTSpeech 2017-nu Folketingets taler Ikke standard 244M Scrape Hovedstaden nutidig Sundhed CC0-1.0 79.9M MeMo 1870-1899 Sk\u00f8nlitteratur Offentligt Dom\u00e6ne 319M Wikipedia nutidig Encyklop\u00e6di CC-BY-SA 4.0 498M Retsinformation.dk (*) nutidig Lovtekster Ikke standard (*) 1.42G Skat.dk (*) nutidig Skatteinformation CC0-1.0 354M H-S\u00f8 (*) nutidig Retssager CC0-1.0 204 Hestenettet (*) nutidig Forum CC0-1.0 1.19G Folketinget (*) 2009-2019 Debat Ikke standard 351M Europarl (*) 2004-2008 Debat CC0-1.0 312M Spontaneous Speech (*) 2019 Samtaler CC0-1.0 4.0M NAAT (*) 1930-nu Taler CC0-1.0 881k Dansk Litteratur (*) 1700-nu Litteratur CC0-1.0 162M Gutenberg (*) 1700-nu Litteratur Ikke Standard 19.2M WikiBooks (*) 2019-2020 Manualer CC0-1.0 17.5M WikiSource (*) 1700-nu Litteratur CC0-1.0 15.5M Johannes V. Jensen (*) - JVJ\u2019s v\u00e6rker CC-BY-SA 4.0 10.7M Religi\u00f8se Tekster (*) - Religi\u00f8se CC0-1.0 3.56M TV2R (*) 2015-2019 Nyheder CC-BY 4.0 64.04M Dasem Data (*) nutidig Andet Ikke standard 4.45M Botxt (*) nutidig Bornholmsk CC0-1.0 2.01M DDT (*) nutidig Andet CC-BY-SA 4.0 546k S\u00f8nderjysk (*) nutidig S\u00f8nderjysk CC0-1.0 140k <p>Listen vil l\u00f8bende blive opdateret med flere datakilder. Data kommer bl.a. til at v\u00e6re fra samarbejdet med Dansk Sprogmodel Konsortium. Det skal bem\u00e6rkes at nogle af datas\u00e6ttene kommer fra Danish Gigaword, angivet i tabellen med (*).</p>"},{"location":"blog/2024/07/04/datakilder.html#respekt-for-dataejere","title":"Respekt for dataejere","text":"<p>Vi har den st\u00f8rste respekt for dem, der ejer data. Vi forst\u00e5r, hvor vigtigt det er at beskytte og respektere dataejeres \u00f8nsker om hvad deres data m\u00e5 bruges til. Hvis du har nogen sp\u00f8rgsm\u00e5l vedr\u00f8rende de data vi bruger, er du altid velkommen til at kontakte os. Vi er meget \u00e5bne for dialog og s\u00e6tter pris p\u00e5 input, da det hj\u00e6lper os med at forbedre vores praksis og sikre, at vi lever op til dataejeres \u00f8nsker. Din feedback er vigtig for os, og vi ser frem til at h\u00f8re fra dig.</p>"},{"location":"data/adl/adl.html","title":"Dataset Card for Archive for Danish Literature","text":""},{"location":"data/adl/adl.html#dataset-description","title":"Dataset Description","text":"<p>Danish literature from 1700-2023 from the Archive for Danish Literature (ADL).</p> <p>Archive for Danish Literature (ADL) is a literary-historical collection of selected parts of older Danish literature, from the Middle Ages up to the mid-20th century. It provides access to both the texts and introductory material on most of the authors. ADL is a resource for research, teaching, and broad dissemination of older Danish literature. Currently, ADL contains works by 78 authors. The texts are reproduced from standard printed editions. All texts are searchable, and many can also be viewed as facsimiles (photographs of the original edition)  on the Danish Royal Library's website. </p> <p>See also dataset entry on sprogteknologi.dk and an API.</p> <ul> <li>Number of samples: 498</li> <li>Number of tokens (Llama 3): 58.49M</li> <li>Average document length in tokens (min, max): 117.46K (53, 662.14K)</li> </ul>"},{"location":"data/adl/adl.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"adl_aakjaer06val\",\n  \"text\": \"SAMLEDE V\u00c6RKER\\n\\nJEPPE AAKJ\u00c6R GYLDENDALSKE BOGHANDEL - NORDISK FORLAG KJ\u00d8BENHAVN OG\\nKRISTIANIA 1919 0[...]\",\n  \"source\": \"adl\",\n  \"added\": \"2020-09-14\",\n  \"created\": \"1700-01-01, 2022-01-01\",\n  \"token_count\": 439908\n}\n</code></pre>"},{"location":"data/adl/adl.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/adl/adl.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/adl/adl.html#additional-information","title":"Additional Information","text":""},{"location":"data/adl/adl.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html","title":"Dataset Card for AI Aktindsigt","text":"<p>Multiple web scrapes from municipality websites collected as a part of the AI-aktindsigt project.</p> <p>The dataset consists of multiple scrapes of municipal websites compiled in connection with the work on the AI-aktindsigt project. The scrape is made across different domains from several different municipalities.</p>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 200.91K</li> <li>Number of tokens (Llama 3): 139.23M</li> <li>Average document length in tokens (min, max): 693.0064405666105 (9, 152.60K)</li> </ul>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"ai-aktindsigt_0\",\n  \"text\": \"Vallensb\u00e6k Stationstorv 100 2665 Vallensb\u00e6k Strand Telefon: +45 4797 4000\",\n  \"source\": \"ai-aktindsigt\",\n  \"added\": \"2025-03-24\",\n  \"created\": \"2010-01-01, 2024-03-18\",\n  \"token_count\": 29\n}\n</code></pre>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#additional-information","title":"Additional Information","text":""},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#sourced-data","title":"Sourced data","text":"<p>This dataset is derived from <code>AI-aktindsigt/Skrabet_kommunale_hjemmesider</code></p>"},{"location":"data/ai-aktindsigt/ai-aktindsigt.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work. We recommend citing the huggingface repository.</p>"},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html","title":"Dataset Card for ArXiv Abstracts","text":"<p>A set of public domain arxiv paper abstracts.</p> <p>Each paper uploaded to ArXiv includes structured metadata fields, including an abstract summarizing the paper\u2019s findings and contributions. According to ArXiv\u2019s licensing policy, the metadata for any paper submitted to ArXiv is distributed under the CC0 license, regardless of the license of the paper itself. Thus, this dataset contains the abstract for every paper submitted to ArXiv through late 2024. We source the abstracts from ArXiv\u2019s API via the Open Archives Initiative Protocol for Metadata Harvesting endpoint and reproduce them as-is. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 2.50M</li> <li>Number of tokens (Llama 3): 524.45M</li> <li>Average document length in tokens (min, max): 209.42911292167878 (4, 1.31K)</li> </ul>"},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/arxiv_abstracts_filtered/arxiv_abstracts_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html","title":"Dataset Card for ArXiv Papers","text":"<p>Set of public domain papers, published on ArXiv.</p> <p>ArXiv is an online open-access repository of over 2.4 million scholarly papers covering fields such as computer science, mathematics, physics, quantitative biology, economics, and more. When uploading papers, authors can choose from a variety of licenses. This dataset includes text from all papers uploaded under CC BY, CC BY-SA, and CC0 licenses through a three-step pipeline: first, the latex source files for openly licensed papers were downloaded from ArXiv\u2019s bulk-access S3 bucket; next, the LATEXML conversion tool was used to convert these source files into a single HTML document; finally, the HTML was converted to plaintext using the Trafilatura HTML-processing library. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 295.41K</li> <li>Number of tokens (Llama 3): 6.11B</li> <li>Average document length in tokens (min, max): 20.69K (3, 994.99K)</li> </ul>"},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/arxiv_papers_filtered/arxiv_papers_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html","title":"Dataset Card for Biodiversity Heritage Library","text":"<p>A set of ~15 million public domain books and documents from the BHL collection.</p> <p>The Biodiversity Heritage Library (BHL) is an open-access digital library for biodiversity literature and archives. This dataset contains over 15 million public domain books and documents from the BHL collection. These works were collected using the bulk data download interface provided by the BHL and were filtered based on their associated license metadata. We use the optical character recognition (OCR)-generated text distributed by BHL. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 14.61M</li> <li>Number of tokens (Llama 3): 8.62B</li> <li>Average document length in tokens (min, max): 589.7122386742171 (6, 16.89K)</li> </ul>"},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/biodiversity_heritage_library_filtered/biodiversity_heritage_library_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/botxt/botxt.html","title":"Dataset Card for Bornholmsk","text":""},{"location":"data/botxt/botxt.html#dataset-description","title":"Dataset Description","text":"<p>The Bornholmsk Ordbog Dictionary Project</p> <p>Fictional texts of various kinds written in Bornholmsk, the dialect spoken on the Danish island of Bornholm (The language code for Bornholmsk under IETF BCP-47 is da-bornholm), have been digitized (OCR\u2019ed and proofread) by volunteers working within the recently resumed Bornholmsk Ordbog dictionary project (Kjeldsen, 2019). Most of the material included is written by Otto J. Lund in the period 1930-48 (novels, short stories, and poems). The Bornholmsk subcorpus, which in its present state amounts to circa 400 K words, also includes folk stories published by J. P. Kuhre in 1938, and by K. M. Kofoed in 1935, fictional letters by various authors published in the 1930s, as well as poems by Alfred Jensen published in 1948 and various other texts from the same period. The non-standardized orthography varies considerably from source to source. The Bornholmsk part of the Danish Gigaword is a significantly extended dataset, well beyond that studied in earlier NLP work on the dialect (Derczynski and Kjeldsen, 2019).</p> <ul> <li>Number of samples: 106</li> <li>Number of tokens (Llama 3): 847.97K</li> <li>Average document length in tokens (min, max): 8.00K (407, 83.79K)</li> </ul>"},{"location":"data/botxt/botxt.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"botxt_0000040\",\n  \"text\": \"R\u00e6ua-L\u00e2rs\\n\\nR\u00e6ua-L\u00e2rs \u00e5 hans Konna, Stina, bode uda i Torpabakkana. Hanj hed nok \u00e6jla L\u00e2rs\\nNielsen, m[...]\",\n  \"source\": \"botxt\",\n  \"added\": \"2024-05-16\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 7229\n}\n</code></pre>"},{"location":"data/botxt/botxt.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/botxt/botxt.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/botxt/botxt.html#additional-information","title":"Additional Information","text":""},{"location":"data/botxt/botxt.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html","title":"Dataset Card for Caselaw Access Project","text":"<p>The Caselaw Access Project consists of nearly 40 million pages of U.S. federal and state court decisions and judges\u2019 opinions from the last 365 years.</p> <p>This dataset contains 6.7 million cases from the Caselaw Access Project and Court Listener. The Caselaw Access Project consists of nearly 40 million pages of U.S. federal and state court decisions and judges\u2019 opinions from the last 365 years. In addition, Court Listener adds over 900 thousand cases scraped from 479 courts. The Caselaw Access Project and Court Listener source legal data from a wide variety of resources such as the Harvard Law Library, the Law Library of Congress, and the Supreme Court Database. From these sources, we only included documents that were in the public domain. Erroneous OCR errors were further corrected after digitization, and additional post-processing was done to fix formatting and parsing. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 6.72M</li> <li>Number of tokens (Llama 3): 17.36B</li> <li>Average document length in tokens (min, max): 2.58K (7, 769.94K)</li> </ul>"},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/caselaw_access_project_filtered/caselaw_access_project_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/cccc_filtered/cccc_filtered.html","title":"Dataset Card for Creative Commons Common Crawl","text":"<p>A dataset consisting of permissively licensed web pages processed from common crawl.</p> <p>This dataset contains text from 52 Common Crawl snapshots, covering about half of Common Crawl snapshots available to date and covering all years of operations of Common Crawl up to 2024. We found a higher level of duplication across this collection, suggesting that including more snapshots would lead to a modest increase in total token yield. From these snapshots, we extract HTML content using FastWarc. Then, using a regular expression adapted from the C4Corpus project. To ensure license accuracy, we manually verified the top 1000 domains by content volume, retaining only the 537 domains with confirmed licenses where the Creative Commons designation applied to the all text content rather than embedded media or a subset of the text on the domain. As an additional check, we did a second round of annotations with the assistance of OpenAI's o3 model. Specifically, we instructed the model to examine each web domain and identify the ones that were openly licensed. We then had a second team manually annotate the cases where the AI does not approve of the domain but the original human auditor did. This resulted in todo domains being removed.</p> <p>We extract the main content of these documents and remove boilerplate using Resiliparse. We perform URL-level exact deduplication and use Bloom filters to remove near-duplicates with 80% ngram overlap. We also employ rule-based filters matching Dolma; namely, we use C4-derived heuristics to filter pages containing Javascript, Lorem Ipsum, and curly braces {}. We also apply all Gopher rules to remove low-quality pages. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/cccc_filtered/cccc_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 6.85M</li> <li>Number of tokens (Llama 3): 13.99B</li> <li>Average document length in tokens (min, max): 2.04K (3, 154.80K)</li> </ul>"},{"location":"data/cccc_filtered/cccc_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/cccc_filtered/cccc_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/cccc_filtered/cccc_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/cccc_filtered/cccc_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/cccc_filtered/cccc_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository. This dataset has been updated to remove instances of incorrect licensing. If you require the exact version that Comma v0.1 was trained on for non-commercial research purposes, please start a discussion on this repository.</p>"},{"location":"data/cccc_filtered/cccc_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/cellar/cellar.html","title":"Dataset Card for Cellar","text":"<p>The official digital repository for European Union legal documents and open data.</p> <p>The EU Dataset Cellar serves as the central access point for all official EU publications, legislation, and open data resources. Maintained by the Publications Office of the European Union, this comprehensive digital archive contains millions of documents in multiple languages, including regulations, directives, decisions, treaties, case law, and preparatory acts dating back decades. The repository employs standardized metadata and unique identifiers to organize its vast collection, making it an essential resource for researchers, legal professionals, policymakers, and citizens seeking authoritative information on EU law and policy. The Cellar's linked data architecture also enables sophisticated search capabilities and integration with other information systems across the European Union's digital landscape.</p>"},{"location":"data/cellar/cellar.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 63.40K</li> <li>Number of tokens (Llama 3): 1.15B</li> <li>Average document length in tokens (min, max): 18.17K (7, 2.60M)</li> </ul>"},{"location":"data/cellar/cellar.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/cellar/cellar.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/cellar/cellar.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/cellar/cellar.html#additional-information","title":"Additional Information","text":""},{"location":"data/cellar/cellar.html#license-information","title":"License Information","text":""},{"location":"data/cellar/cellar.html#citation-information","title":"Citation Information","text":"<p>No citation applicable.</p>"},{"location":"data/cvr-reports/cvr-reports.html","title":"Dataset Card for Annual Reports (CVR)","text":"<p>Annual reports from danish companies in the period 2010-2025. </p> <p>This dataset contains text extracted from annual reports (\u00e5rsrapporter) filed by companies in Denmark. Below is a brief explanation of how these reports are created, what they typically include, and the legal framework that governs them. Read more about the system here.</p>"},{"location":"data/cvr-reports/cvr-reports.html#legal-framework","title":"Legal Framework","text":"<p>Danish annual reports are regulated primarily by the Danish Financial Statements Act (\u00c5rsregnskabsloven). This law defines:</p> <ul> <li>Which companies are required to prepare and file annual reports</li> <li>What information must be included, depending on company size and type</li> <li>The applicable accounting standards</li> <li>Deadlines and enforcement mechanisms</li> <li>Oversight is carried out by the Danish Business Authority (Erhvervsstyrelsen), which operates the public platform where reports are submitted and published.</li> </ul>"},{"location":"data/cvr-reports/cvr-reports.html#purpose","title":"Purpose","text":"<p>Annual reports are created to:</p> <ul> <li>Provide transparency to stakeholders (owners, investors, creditors, the public)</li> <li>Comply with legal and tax obligations</li> <li>Enable public access to key financial and structural information about companies operating in Denmark</li> </ul>"},{"location":"data/cvr-reports/cvr-reports.html#reporting-requirements","title":"Reporting Requirements","text":"<p>All Danish companies (except cetain types, such as sole proprietorships: see more) must submit an annual report, usually within 6 months after the end of their financial year. Reports vary in complexity depending on the company\u2019s accounting class:</p>"},{"location":"data/cvr-reports/cvr-reports.html#typical-contents-of-an-annual-report","title":"Typical Contents of an Annual Report","text":"<p>While the exact structure varies, Danish annual reports commonly include the following sections:</p> <ul> <li>Management\u2019s Statement: A signed declaration of responsibility</li> <li>Independent Auditor\u2019s Report (if applicable)</li> <li>Management Commentary: Overview of financial performance, risks, and strategic direction</li> <li>Income Statement</li> <li>Balance Sheet</li> <li>Statement of Changes in Equity</li> <li>Notes: Detailed explanations of accounting policies, risks, employees, related parties, etc.</li> </ul> <p>Reports are typically submitted in XBRL (machine-readable) and/or PDF (human-readable) formats. The primary language is Danish, but some large or international companies may file English versions.</p>"},{"location":"data/cvr-reports/cvr-reports.html#public-availability","title":"Public Availability","text":"<p>Once filed, all annual reports are made publicly accessible through the Danish Business Authority\u2019s platform https://datacvr.virk.dk. This supports Denmark\u2019s broader commitment to transparency and open government data.</p>"},{"location":"data/cvr-reports/cvr-reports.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 288.35K</li> <li>Number of tokens (Llama 3): 2.32B</li> <li>Average document length in tokens (min, max): 8.05K (2, 1.83M)</li> </ul>"},{"location":"data/cvr-reports/cvr-reports.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/cvr-reports/cvr-reports.html#additional-processing","title":"Additional Processing","text":"<p>Text Extraction: The texts are gathered in PDFs, so for extracting the tests we have used the marker-pdf python library.</p> <p>Deduplication: Since the reports\u2014particularly those from smaller companies\u2014often contain large amounts of standardized text (notably in the Independent Auditor\u2019s Report section), we applied semantic deduplication with a similarity threshold of 0.95 to reduce redundancy in the dataset.</p>"},{"location":"data/cvr-reports/cvr-reports.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/cvr-reports/cvr-reports.html#additional-information","title":"Additional Information","text":""},{"location":"data/cvr-reports/cvr-reports.html#license-information","title":"License Information","text":"<p>Indg\u00e5et mundtlig aftale mellem DFM og repr\u00e6sentant for Erhversstyrelsen. Ved sp\u00f8rgsm\u00e5l kontakt os p\u00e5 mail: kristian.n.jensen@alexandra.dk </p>"},{"location":"data/cvr-reports/cvr-reports.html#citation-information","title":"Citation Information","text":""},{"location":"data/danish-pd/danish-pd.html","title":"Dataset Card for PleIAs - Danish Public Domain","text":"<p>PleIAs - Danish Public Domain is a large collection aiming to aggregate all Danish monographies and periodicals in the public domain.</p> <p>The collection contains 3113 individual titles making up 322,141,347 words recovered from multiple sources, including Internet Archive and various European national libraries and cultural heritage institutions. Each parquet file has the full text of 2,000 books selected at random.</p>"},{"location":"data/danish-pd/danish-pd.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 33.28K</li> <li>Number of tokens (Llama 3): 532.43M</li> <li>Average document length in tokens (min, max): 16.00K (82, 27.00K)</li> </ul>"},{"location":"data/danish-pd/danish-pd.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/danish-pd/danish-pd.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/danish-pd/danish-pd.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/danish-pd/danish-pd.html#additional-information","title":"Additional Information","text":""},{"location":"data/danish-pd/danish-pd.html#license-information","title":"License Information","text":"<p>The entire collection is in the public domain in all regions. This means that the patrimonial rights of each individual or collective right holders have expired.</p> <p>There has been a debate for years in Europe over the definition of public domain and the possibility to restrict its use. Since 2019, the EU Copyright Directive states that \"Member States shall provide that, when the term of protection of a work of visual art has expired, any material resulting from an act of reproduction of that work is not subject to copyright or related rights, unless the material resulting from that act of reproduction is original in the sense that it is the author's own intellectual creation.\" (art. 14)</p>"},{"location":"data/danish-pd/danish-pd.html#citation-information","title":"Citation Information","text":"<p>The corpus was stored and processed with the generous support of Scaleway. It was built up with the support and concerted efforts of the state start-up LANGU:IA (start-up d\u2019Etat), supported by the French Ministry of Culture and DINUM, as part of the prefiguration of the service offering of the Alliance for Language technologies EDIC (ALT-EDIC).</p> <p>Corpus collection has been largely facilitated thanks to the open science LLM community insights, cooperation and support (Occiglot, Eleuther AI, OpenLLM France, Allen AI).</p>"},{"location":"data/dannet/dannet.html","title":"Dataset Card for DanNet","text":"<p>DanNet is a Danish WordNet.</p> <p>A WordNet is a lexico-semantic network which show the meaning and the relation between words through named connections. It can be considered a machine-readable dictionary.</p>"},{"location":"data/dannet/dannet.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 47.60K</li> <li>Number of tokens (Llama 3): 1.48M</li> <li>Average document length in tokens (min, max): 31.079364745919374 (2, 106)</li> </ul>"},{"location":"data/dannet/dannet.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"dannet_46506\",\n  \"text\": \"N\u00e5r fodboldholdet fra 1. division i Ikast spiller hjemmekampe, lyder r\u00e5bet ud over Ikast Stadion: We[...]\",\n  \"source\": \"dannet\",\n  \"added\": \"2020-09-24\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 50\n}\n</code></pre>"},{"location":"data/dannet/dannet.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dannet/dannet.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dannet/dannet.html#license-information","title":"License Information","text":"DanNet 1.0 License <p> Commercial Use of DanNet  DanNet may be used in commercial applications in accordance with the following license agreement. An attorney representing the commercial interest should review this DanNet license with respect to the intended use.  DanNet 1.0 License  DanNet Release 2.1  This software and database is being provided to you, the LICENSEE, by University of Copenhagen and Society for Danish Language and Literature under the following license. By obtaining, using and/or copying this software and database, you agree that you have read, understood, and will comply with these terms and conditions.  Permission to use, copy, modify and distribute this software and database and its documentation for any purpose and without fee or royalty is hereby granted, provided that you agree to comply with the following copyright notice and statements, including the disclaimer, and that the same appear on ALL copies of the software, database and documentation, including modifications that you make for internal use or for distribution.  THIS SOFTWARE AND DATABASE IS PROVIDED \"AS IS\" AND UNIVERSITY OF COPENHAGEN and SOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, UNIVERSITY OF COPENHAGEN AND SOCIETY FOR DANISH LANGUAGE AND LITERATURE MAKE NO REPRESENTATIONS OR WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS.  The names of University of Copenhagen and Society for Danish Language and Literature may not be used in advertising or publicity pertaining to distribution of the software and/or database. Title to copyright in this software, database and any associated documentation shall at all times remain with University of Copenhagen and Society for Danish Language and Literature and LICENSEE agrees to preserve same.  DanNet 2.1 Copyright 2009-12 by University of Copenhagen and Society for Danish </p>"},{"location":"data/dannet/dannet.html#additional-information","title":"Additional Information","text":""},{"location":"data/dannet/dannet.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/danske-taler/danske-taler.html","title":"Dataset Card for Danske Taler","text":"<p>Danish Speeches from dansketaler.dk.</p> <p>The database dansketaler.dk is managed by Danske Taler, an independent institution that in addition to managing the database and carries out cultural and democratic projects based on speeches.  Danske Taler state as their goals that they seek to preserve our cultural heritage and promotes active citizenship and democratic confidence through its work.  Additionally, Danske Taler provides data to a number of online resources, including: lex.dk, sprogteknologi.dk, and ordnet.dk.</p> <p>The goal of the dataset is to collect historical and timely speeches and make them available for the public.</p> <p>Learn more about danske taler by reading their about us page.</p> <p>NOTE: Danske-Taler is also collecting sermons, but these are not included in this dataset. </p>"},{"location":"data/danske-taler/danske-taler.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 2.91K</li> <li>Number of tokens (Llama 3): 8.72M</li> <li>Average document length in tokens (min, max): 3.00K (129, 53.40K)</li> </ul>"},{"location":"data/danske-taler/danske-taler.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/danske-taler/danske-taler.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/danske-taler/danske-taler.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/danske-taler/danske-taler.html#additional-information","title":"Additional Information","text":""},{"location":"data/danske-taler/danske-taler.html#dataset-collection-process","title":"Dataset Collection Process","text":"<p>This dataset was collected using the publicly available API. </p>"},{"location":"data/danske-taler/danske-taler.html#quality-assurance","title":"Quality Assurance","text":"<p>We check for and remove exact duplicates, empty texts, duplicate ids after the initial download. We additionally check if the articles contain any HTML.</p>"},{"location":"data/danske-taler/danske-taler.html#opportunities-for-improvement","title":"Opportunities for Improvement","text":"<p>While this dataset can be updated to include the latest availabe speeches. </p> <p>We consider the quality of the current collection high with a low chance of  incorrect formatting,  spelling errors, empty documents or  misformatted segments.  This stems both from the quality assurance, source of documents and subjective inspection.</p>"},{"location":"data/danske-taler/danske-taler.html#license-information","title":"License Information","text":"<p>Since the license information isn't avaiable through the API we collect this data directly from the webpage of each article under the header  \"Ophavsret\".</p> <p>For speeches where it is noted that \"Materialet er fri af ophavsret\" (The material is in the public domain) or similarly we assign it a <code>cc0</code> license.</p> <p>Such an example can be seen here:</p> <p>Ophavsret</p> <p>Materialet er fri af ophavsret. Taler, som er holdt i offentligheden, er ikke omfattet af ophavsret (Jf. ophavsretslovens \u00a7 26 og 32).  Det betyder, at n\u00e5r en tale er indg\u00e5et i Danske Talers database, kan den bruges af tredjeparter, fx til undervisning eller forskning.</p> <p>source: Ursula von der Leyens tale om europ\u00e6isk forsvar og sikkerhed p\u00e5 H\u00e6rens Officersskole</p> <p>Speeches without this mention is removed. Such an example include:</p> <p>Ophavsret</p> <p>Materialet er beskyttet af ophavsret</p> <p>Source: Christina Egelunds tale ved Aarhus Universitets \u00e5rsfest</p> <p>We manually checked the unique set of license descriptions to see if any were open licenses that weren't included in the current criteria.</p> <p>For specific filtering criteria see the <code>create.py</code> script.</p>"},{"location":"data/danske-taler/danske-taler.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work. We recommend citing the huggingface repository.</p>"},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html","title":"Dataset Card for Data Provenance Initiative","text":"<p>The Data Provenance Initiative is a digital library of supervised datasets that have been manually annotated with their source and license information.</p> <p>The Data Provenance Initiative is a digital library of supervised datasets that have been manually annotated with their source and license information [ 104, 107 ]. We leverage their tooling to filter HuggingFace datasets, based on a range of criteria, including their licenses. Specifically, we filter the data according to these criteria: contains English language or code data, the text is not model-generated, the dataset\u2019s audit yielded an open license, and the original sources of the data are only from recognized public domain sources. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 3.51M</li> <li>Number of tokens (Llama 3): 817.36M</li> <li>Average document length in tokens (min, max): 232.97177537274933 (2, 275.30K)</li> </ul>"},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/data_provenance_initiative_filtered/data_provenance_initiative_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/dbc-abstracts/dbc-abstracts.html","title":"Dataset Card for DBC D1G1TAL - Abstracts","text":"<p>dbc-abstracts consists of more than 11.6 million abstracts of books and other materials collected and created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>The dataset contains millions of abstracts in the Danish lanugage, which are supplemented by English, Norwegian, Swedish, German, and other language abstracts. The dataset was collected and created by DBC D1G1TAL A/S as one of the backbones for their catalogue of books and other materials. The dataset includes abstracts created between 1991 and 2024.</p>"},{"location":"data/dbc-abstracts/dbc-abstracts.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 9.89M</li> <li>Number of tokens (Llama 3): 694.42M</li> <li>Average document length in tokens (min, max): 70.2343489339804 (2, 10.41K)</li> </ul>"},{"location":"data/dbc-abstracts/dbc-abstracts.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dbc-abstracts/dbc-abstracts.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dbc-abstracts/dbc-abstracts.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dbc-abstracts/dbc-abstracts.html#additional-information","title":"Additional Information","text":""},{"location":"data/dbc-abstracts/dbc-abstracts.html#license-information","title":"License Information","text":"<p>Danish Foundation Models have a written agreement with DBC D1G1TAL regarding the use of the data for training and releasing models.  Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"data/dbc-abstracts/dbc-abstracts.html#citation-information","title":"Citation Information","text":"<p>No citation at the moment.</p>"},{"location":"data/dbc-faktalink/dbc-faktalink.html","title":"Dataset Card for DBC D1G1TAL - Faktalink","text":"<p>dbc-faktalink consists of more than 5 hundred articles created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>All articles are written in Danish language. Instances that comprise this dataset represent articles on a variety of topics on aspects relevant to Danish society. The dataset includes articles created between 1991 and 2024.</p> <p>Descrition taken from dbcdigital.dk</p> <p>Faktalink is a tool used by libraries and schools to provide factual knowledge on current, socially relevant, and historical topics. Through serious, trustworthy, and thoroughly researched articles, students and library users can gain a solid overview of various current and important subjects, as well as historical events.</p> <p>On Faktalink, you'll find articles on everything from politics and history to sports, culture, and much more. We prioritize topics that are relevant for school use and those with broader interest.</p> <p>There are around 500 articles and themes on Faktalink, and they are continuously updated with the latest information. Additionally, the site is regularly expanded with new, relevant articles and themes.</p>"},{"location":"data/dbc-faktalink/dbc-faktalink.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 522</li> <li>Number of tokens (Llama 3): 1.99M</li> <li>Average document length in tokens (min, max): 3.81K (2, 18.77K)</li> </ul>"},{"location":"data/dbc-faktalink/dbc-faktalink.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dbc-faktalink/dbc-faktalink.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dbc-faktalink/dbc-faktalink.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dbc-faktalink/dbc-faktalink.html#additional-information","title":"Additional Information","text":""},{"location":"data/dbc-faktalink/dbc-faktalink.html#license-information","title":"License Information","text":"<p>Danish Foundation Models have a written agreement with DBC D1G1TAL regarding the use of the data for training and releasing models.  Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"data/dbc-faktalink/dbc-faktalink.html#citation-information","title":"Citation Information","text":"<p>No citation at the moment.</p>"},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html","title":"Dataset Card for DBC D1G1TAL - Forfatterweb","text":"<p>dbc-forfatterweb consists of more than 1 thousand articles created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>Forfatter web is a tool created by DBC D1G1TAL. Description from dbcdigital.dk</p> <p>Forfatterweb is a tool for libraries and schools that provides factual knowledge about literature. With a wide array of thoroughly researched author portraits and themes, as well as introductions to literary genres and periods, Forfatterweb is your gateway to understanding books, authorships, and literary movements throughout history.</p> <p>Forfatterweb is a subscription service and can be used with either a Uni-Login or library login, as long as your school or library subscribes to Forfatterweb.</p> <p>All articles are written in Danish language. Instances that comprise this dataset represent articles on Danish writers.  The dataset includes articles created between 1991 and 2024.</p>"},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.19K</li> <li>Number of tokens (Llama 3): 1.42M</li> <li>Average document length in tokens (min, max): 1.20K (3, 14.44K)</li> </ul>"},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#additional-information","title":"Additional Information","text":""},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#license-information","title":"License Information","text":"<p>Danish Foundation Models have a written agreement with DBC D1G1TAL regarding the use of the data for training and releasing models.  Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"data/dbc-forfatterweb/dbc-forfatterweb.html#citation-information","title":"Citation Information","text":"<p>No citation at the moment.</p>"},{"location":"data/dbc-reviews/dbc-reviews.html","title":"Dataset Card for DBC D1G1TAL - Reviews","text":"<p>dbc-reviews consists of more than 214 thousand reviews of books and other materials collected and created by DBC D1G1TAL (former Dansk Bibliotekscenter).</p> <p>The dataset contains thousands of reviews in the Danish language, which are supplemented by English, Norwegian, Swedish, German, and other language reviews. Instances that comprise this dataset represent reviews of books or other materials. The dataset includes reviews created between 1991 and 2024.</p>"},{"location":"data/dbc-reviews/dbc-reviews.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 213.93K</li> <li>Number of tokens (Llama 3): 53.96M</li> <li>Average document length in tokens (min, max): 252.22140730249103 (3, 1.11K)</li> </ul>"},{"location":"data/dbc-reviews/dbc-reviews.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dbc-reviews/dbc-reviews.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dbc-reviews/dbc-reviews.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dbc-reviews/dbc-reviews.html#additional-information","title":"Additional Information","text":""},{"location":"data/dbc-reviews/dbc-reviews.html#license-information","title":"License Information","text":"<p>Danish Foundation Models have a written agreement with DBC D1G1TAL regarding the use of the data for training and releasing models.  Data will only be available at the entity during the project. Requests regarding access to the dataset should be directed to the data owner DBC D1G1TAL.</p>"},{"location":"data/dbc-reviews/dbc-reviews.html#citation-information","title":"Citation Information","text":"<p>No citation at the moment.</p>"},{"location":"data/depbank/depbank.html","title":"Dataset Card for Danish Dependency Treebank","text":"<p>The Danish subsection of the Universal Dependencies Treebank.</p> <p>The Danish UD treebank has been converted from the Danish Dependency Treebank (Buch-Kromman, 2003) into Universal Dependencies (UD). It consists of 5,512 sentences (100k words). The Danish source texts and the Danish part-of-speech tags were created by the PAROLE-DK project (Keson 1998) by the Danish Society for Language and Literature.</p> <p>While the dataset was initially intended as a rich annotation, this corpora only uses the raw text.</p>"},{"location":"data/depbank/depbank.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 536</li> <li>Number of tokens (Llama 3): 185.45K</li> <li>Average document length in tokens (min, max): 345.99626865671644 (261, 517)</li> </ul>"},{"location":"data/depbank/depbank.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"depbank_0375\",\n  \"text\": \"\\nH.L. Hansen var en us\u00e6dvanmlig og frodig personlighed. Han skabte \\ngl\u00e6de og munterhed omkring sig o[...]\",\n  \"source\": \"depbank\",\n  \"added\": \"2024-05-16\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 389\n}\n</code></pre>"},{"location":"data/depbank/depbank.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/depbank/depbank.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/depbank/depbank.html#additional-information","title":"Additional Information","text":""},{"location":"data/depbank/depbank.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/doab_filtered/doab_filtered.html","title":"Dataset Card for Directory of Open Access Books","text":"<p>The Directory of Open Access Books (DOAB) is an online index of over 94,000 peer-reviewed books curated from trusted open-access publishers.</p> <p>The Directory of Open Access Books (DOAB) is an online index of over 94,000 peer-reviewed books curated from trusted open-access publishers. To collect the openly licensed content from DOAB, we retrieve metadata using their official metadata feed. We then filter the collection to include only English-language books released under CC BY and CC BY-SA licenses. The filtered books are downloaded in PDF format and converted to plaintext using the Marker PDF-to-text converter. As an additional validation step, we manually create a whitelist of open license statements and retain only texts explicitly containing one of these statements in their front- or back-matter. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/doab_filtered/doab_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 403.92K</li> <li>Number of tokens (Llama 3): 2.80B</li> <li>Average document length in tokens (min, max): 6.93K (5, 1.11M)</li> </ul>"},{"location":"data/doab_filtered/doab_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/doab_filtered/doab_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/doab_filtered/doab_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/doab_filtered/doab_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/doab_filtered/doab_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/doab_filtered/doab_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/domsdatabasen/domsdatabasen.html","title":"Dataset Card for Domsdatabasen.dk","text":"<p>Domsdatabasen.dk is a public database containing selected judgments from the Danish courts.</p> <p>Launched in early 2022, the platform aims to increase transparency and public insight into the workings of the judiciary in Denmark. It is accessible to everyone \u2013 legal professionals, citizens, companies, and public authorities interested in Danish case law.</p>"},{"location":"data/domsdatabasen/domsdatabasen.html#dataset-description","title":"Dataset Description","text":""},{"location":"data/domsdatabasen/domsdatabasen.html#purpose-and-scope","title":"Purpose and Scope","text":"<p>The main goal of the database is to support the principle of openness in the administration of justice. It offers users access to selected civil and criminal decisions, with an initial focus on rulings from the higher courts, such as:</p> <ul> <li>The Supreme Court (H\u00f8jesteret)</li> <li>The High Courts (Landsretterne)</li> <li>The Maritime and Commercial Court (S\u00f8- og Handelsretten)</li> </ul> <p>Some rulings from the district courts (byretterne) are also included, particularly when they are part of a case string that has been appealed. Over time, the database will expand in coverage and volume, especially as the court system transitions to new digital case management systems.</p>"},{"location":"data/domsdatabasen/domsdatabasen.html#pseudonymization-and-data-protection","title":"Pseudonymization and Data Protection","text":"<p>All published rulings are pseudonymized to protect the privacy of individuals involved, in accordance with the EU General Data Protection Regulation (GDPR), the Danish Data Protection Act, and rules from the Danish Data Protection Agency.</p> <p>Pseudonymization involves replacing personally identifiable information (e.g., names, CPR numbers) with general terms such as \u201cthe accused\u201d, \u201cwitness 1\u201d, etc. Additional data such as addresses or health-related details may be redacted or pseudonymized based on a case-specific evaluation.</p> <p>Some roles and names are not pseudonymized, including:</p> <ul> <li>Judges from higher courts</li> <li>Legal representatives (lawyers)</li> <li>Author names in cited legal literature (unless directly involved in the case)</li> <li>Names in EU court decisions</li> </ul> <p>Businesses involved in cases are typically not pseudonymized unless their name reveals personal information or constitutes a trade secret.</p>"},{"location":"data/domsdatabasen/domsdatabasen.html#access-and-development","title":"Access and Development","text":"<p>Domsdatabasen is continuously being developed. As digitization progresses and technical workflows improve, the number of published decisions is expected to grow. The judgments are published as full case strings, including decisions at multiple judicial levels, providing context and legal reasoning throughout the appeal process.</p> <ul> <li>Number of samples: 8.47K</li> <li>Number of tokens (Llama 3): 86.35M</li> <li>Average document length in tokens (min, max): 10.20K (15, 1.01M)</li> </ul>"},{"location":"data/domsdatabasen/domsdatabasen.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"11389\",\n  \"text\": \"## **Ikke grundlag for varet\u00e6gtsf\u00e6ngsling af hensyn til retsh\u00e5ndh\u00e6velsen**\\n\\nDer var ikke s\u00e6rligt bes[...]\",\n  \"source\": \"Domsdatabasen\",\n  \"added\": \"2025-07-04\",\n  \"created\": \"2025-07-04, 2025-07-04\",\n  \"token_count\": 796\n}\n</code></pre>"},{"location":"data/domsdatabasen/domsdatabasen.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/domsdatabasen/domsdatabasen.html#license-information","title":"License Information","text":"Danish Copyright Law <p> Danish Copyright law at https://www.retsinformation.dk/forms/r0710.aspx?id=164796 states    \u00a7 9. Love, administrative forskrifter, retsafg\u00f8relser og lignende offentlige aktstykker er ikke genstand for ophavsret.  Stk. 2. Bestemmelsen i stk. 1 g\u00e6lder ikke for v\u00e6rker, der fremtr\u00e6der som selvst\u00e6ndige bidrag i de i stk. 1 n\u00e6vnte aktstykker. S\u00e5danne v\u00e6rker m\u00e5 dog gengives i forbindelse med aktstykket. Retten til videre udnyttelse afh\u00e6nger af de i \u00f8vrigt g\u00e6ldende regler.  </p>"},{"location":"data/domsdatabasen/domsdatabasen.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/domsdatabasen/domsdatabasen.html#additional-information","title":"Additional Information","text":"<p>Extraction of text: The documents being downloaded from domsdatabasen.dk is PDFs. To extract the texts from those, the <code>create.py</code> script uses the marker-pdf package.</p>"},{"location":"data/dsk-alexandra/dsk-alexandra.html","title":"Dataset Card for DSK - Alexandra Institutet","text":"<p>A collection of crawled webpages that is managed by Alexandra Institutet.</p> <p>This data has been contributed by Alexandra Institutet through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-alexandra/dsk-alexandra.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 386</li> <li>Number of tokens (Llama 3): 584.35K</li> <li>Average document length in tokens (min, max): 1.51K (10, 56.50K)</li> </ul>"},{"location":"data/dsk-alexandra/dsk-alexandra.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-alexandra/dsk-alexandra.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-alexandra/dsk-alexandra.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-alexandra/dsk-alexandra.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-alexandra/dsk-alexandra.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-alexandra/dsk-alexandra.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-atp/dsk-atp.html","title":"Dataset Card for DSK - ATP","text":"<p>A collection of crawled webpages that is managed by ATP.</p> <p>The crawled pages comes from two full domains, and one partial: - atp.dk (Full) - aes.dk (Full) - borger.dk (Partial)</p> <p>This data has been contributed by ATP through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-atp/dsk-atp.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.90K</li> <li>Number of tokens (Llama 3): 2.86M</li> <li>Average document length in tokens (min, max): 1.51K (12, 34.13K)</li> </ul>"},{"location":"data/dsk-atp/dsk-atp.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-atp/dsk-atp.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-atp/dsk-atp.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-atp/dsk-atp.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-atp/dsk-atp.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-atp/dsk-atp.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-cbrain/dsk-cbrain.html","title":"Dataset Card for DSK - cBrain","text":"<p>A collection of Marketing material, product guides, and datasheets produced by cBrain for their products.</p> <p>This data has been contributed by cBrain through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-cbrain/dsk-cbrain.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 435</li> <li>Number of tokens (Llama 3): 4.19M</li> <li>Average document length in tokens (min, max): 9.64K (121, 79.55K)</li> </ul>"},{"location":"data/dsk-cbrain/dsk-cbrain.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-cbrain/dsk-cbrain.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-cbrain/dsk-cbrain.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-cbrain/dsk-cbrain.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-cbrain/dsk-cbrain.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-cbrain/dsk-cbrain.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html","title":"Dataset Card for DSK - Dansk Erhverv","text":"<p>A set of newsletters written by Dansk Erhverv, primarily focusing on financials and companies world wide.  </p> <p>This data has been contributed by Dansk Erhverv through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.19K</li> <li>Number of tokens (Llama 3): 1.12M</li> <li>Average document length in tokens (min, max): 937.143216080402 (123, 2.04K)</li> </ul>"},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-danskerhverv/dsk-danskerhverv.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-dkmedier/dsk-dkmedier.html","title":"Dataset Card for DSK - DK Medier","text":"<p>A collection of ~100K news articles from DK Medier, written in the period 2000-2024. </p> <p>This data has been contributed by DK Medier through the Dansk Sprogmodel Konsortium. </p>"},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 103.64K</li> <li>Number of tokens (Llama 3): 63.64M</li> <li>Average document length in tokens (min, max): 614.0232537944211 (44, 28.66K)</li> </ul>"},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-dkmedier/dsk-dkmedier.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-hofor/dsk-hofor.html","title":"Dataset Card for DSK - HOFOR","text":"<p>A collection of articles, guides and newsletters written by HOFOR for their customers.</p> <p>This data has been contributed by HOFOR through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-hofor/dsk-hofor.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 11</li> <li>Number of tokens (Llama 3): 143.49K</li> <li>Average document length in tokens (min, max): 13.04K (2.15K, 31.12K)</li> </ul>"},{"location":"data/dsk-hofor/dsk-hofor.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-hofor/dsk-hofor.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-hofor/dsk-hofor.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-hofor/dsk-hofor.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-hofor/dsk-hofor.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-hofor/dsk-hofor.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-ida/dsk-ida.html","title":"Dataset Card for DSK - IDA","text":"<p>A collection of newsletters, articles and other texts produced by IDA.</p> <p>This data has been contributed by IDA through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-ida/dsk-ida.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 40</li> <li>Number of tokens (Llama 3): 417.32K</li> <li>Average document length in tokens (min, max): 10.43K (341, 72.69K)</li> </ul>"},{"location":"data/dsk-ida/dsk-ida.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-ida/dsk-ida.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-ida/dsk-ida.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-ida/dsk-ida.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-ida/dsk-ida.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-ida/dsk-ida.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-odense/dsk-odense.html","title":"Dataset Card for DSK - Odense Kommune","text":"<p>A set of newsletters stories, covering events in Odense Municipality. Have been published on their website.</p> <p>This data has been contributed by Odense Kommune through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-odense/dsk-odense.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.13K</li> <li>Number of tokens (Llama 3): 1.18M</li> <li>Average document length in tokens (min, max): 1.04K (102, 3.55K)</li> </ul>"},{"location":"data/dsk-odense/dsk-odense.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-odense/dsk-odense.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-odense/dsk-odense.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-odense/dsk-odense.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-odense/dsk-odense.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-odense/dsk-odense.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-plesner/dsk-plesner.html","title":"Dataset Card for DSK - Plesner","text":"<p>A combination of crawled webpages from Plesners own website, and a series of internal documents outlining procedures.</p> <p>This data has been contributed by Plesner through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-plesner/dsk-plesner.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.69K</li> <li>Number of tokens (Llama 3): 896.33K</li> <li>Average document length in tokens (min, max): 531.312981624185 (21, 59.15K)</li> </ul>"},{"location":"data/dsk-plesner/dsk-plesner.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-plesner/dsk-plesner.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-plesner/dsk-plesner.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-plesner/dsk-plesner.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-plesner/dsk-plesner.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-plesner/dsk-plesner.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-salling/dsk-salling.html","title":"Dataset Card for DSK - Salling Group","text":"<p>A collection of crawled webpages that is managed by Salling Group. The dataset consists mainly of product pages from online stores such as bilka.dk, br.dk and such. The data consists of ~24K webpages.</p> <p>The data have been crawled from 11 different domains: - sallingfondene.dk - sallinggroup.com - bilka.dk - bilkatogo.dk - bilkamadudafhuset.dk - foetex.dk - foetexudafhuset.dk - netto.dk - br.dk - salling.dk - flowr.dk</p> <p>This data has been contributed by Salling Group through the Dansk Sprogmodel Konsortium. </p>"},{"location":"data/dsk-salling/dsk-salling.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 23.48K</li> <li>Number of tokens (Llama 3): 9.79M</li> <li>Average document length in tokens (min, max): 417.0077935351987 (2, 72.88K)</li> </ul>"},{"location":"data/dsk-salling/dsk-salling.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-salling/dsk-salling.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-salling/dsk-salling.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-salling/dsk-salling.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-salling/dsk-salling.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-salling/dsk-salling.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-vejle/dsk-vejle.html","title":"Dataset Card for DSK - Vejle Kommune","text":"<p>A collection of crawled webpages that is managed by Vejle Kommune. Contains various information, covering everything from tourists to garbage collection to historical knowledge of the area. </p> <p>The data have been crawled from 8 different domains: - kyst-kyststien.dk  - voresressourcer.dk  - vejlesfolkemoede.dk - vejle.dk - vejlewiki.dk - vejlestadsarkiv.dk - vejlebrandvaesen.dk - nyrosborg.dk</p> <p>This data has been contributed by Vejle Kommune through the Dansk Sprogmodel Konsortium. </p>"},{"location":"data/dsk-vejle/dsk-vejle.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 16.77K</li> <li>Number of tokens (Llama 3): 27.99M</li> <li>Average document length in tokens (min, max): 1.67K (11, 651.44K)</li> </ul>"},{"location":"data/dsk-vejle/dsk-vejle.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-vejle/dsk-vejle.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-vejle/dsk-vejle.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-vejle/dsk-vejle.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-vejle/dsk-vejle.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-vejle/dsk-vejle.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/dsk-vitec/dsk-vitec.html","title":"Dataset Card for DSK - Vitec","text":"<p>A collection of documents covering product descriptions, to newsletters, to internal documentation.</p> <p>This data has been contributed by Vitec through the Dansk Sprogmodel Konsortium.</p>"},{"location":"data/dsk-vitec/dsk-vitec.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 175</li> <li>Number of tokens (Llama 3): 537.07K</li> <li>Average document length in tokens (min, max): 3.07K (72, 58.19K)</li> </ul>"},{"location":"data/dsk-vitec/dsk-vitec.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/dsk-vitec/dsk-vitec.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/dsk-vitec/dsk-vitec.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/dsk-vitec/dsk-vitec.html#additional-information","title":"Additional Information","text":""},{"location":"data/dsk-vitec/dsk-vitec.html#license-information","title":"License Information","text":"<p>This data is licensed under the data sharing agreement made between the contributor and the Dansk Sprogmodel Konsortium (DSK).  It allows DFM to use the data for training and releasing models, but prohibits DFM from releasing any of the data, except metadata describing the data. </p>"},{"location":"data/dsk-vitec/dsk-vitec.html#citation-information","title":"Citation Information","text":"<p>There is currently no citation for this dataset.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html","title":"Dataset Card for Enev\u00e6ldens Nyheder Online","text":"<p>High quality OCR'd texts from Danish and Norwegian newspapers during the period of constitutional absolutism in Denmark (1660\u20131849).</p> <p>During the eighteenth century, newspapers became a ubiquitous medium. They informed a relatively large reading public about everything from high politics to the mundanities of local markets.  The dataset was created by re-processing over 550.000 digital images scanned from microfilm and held in the Danish Royal Library's collection. They had initially been OCR-processed, but the results were generally unreadable. ENO reprocessed the images using tailored pylaia models in Transkribus. The OCR-quality is generally high, despite the difficult state of the original images. The newspaper editions have been segmented into individual texts using a model designed by the project team. Such texts are the base entity of the dataset. They include mainly two genres: news items and advertisements.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 4.59M</li> <li>Number of tokens (Llama 3): 1.03B</li> <li>Average document length in tokens (min, max): 225.1811458085686 (3, 37.29K)</li> </ul> <ul> <li>Curated by: Johan Heinsen and Camilla B\u00f8geskov, Historisk Datalaboratorium, Aalborg University. With assistance from Sofus Landor Dam, Anders Birkemose, Kamilla Matthiassen and Louise Karoline Sort. </li> <li>Funded by: MASSHINE, Aalborg University.</li> </ul> <p>The dataset contains a wide range of newspapers. The total distribution can be studied here. They cover most of Denmark as well as the three oldest newspapers of Norway, running until the separation of the Danish-Norwegian conglomerate in 1814. This dataset represents version 0.9 (updated 5th of August 2025).</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#dataset-sources","title":"Dataset Sources","text":"<p>The sources of the dataset can be studied in more detail at the project website. Most of the original image material is available in LOAR \u2013 a data repository of the Danish Royal Library. The Norwegian material was downloaded via the API of Nettbiblioteket. The scans of Nyeste Skilderie af Kj\u00f8benhavn were taken from the Internet Archive repository of Niels Jensen. The scans for Politivennen stem from K\u00f8benhavns Biblioteker. Some early newspapers come from recent scans made available to the project by the Danish Royal Library. These are not yet available online.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#uses","title":"Uses","text":"<p>This dataset represents an effort to enable analysis of Denmark-Norway in the seventeenth, eighteenth, and nineteenth centuries. The data can be used to study and model sentiments, political and cultural currents, and the minutiae of urban life.</p> <p>In addition, this dataset is part of Danish Dynaword, a collection of datasets intended for training language models, thereby integrating Danish cultural heritage into the next generation of digital technologies.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"enevaeldens_nyheder_aalborg1767_1767-01-02_1000001\",\n  \"text\": \"Et Menneske er skabt ey for sig selv allene: Hvert Lem paa Legemet det heele tiene maae, En Stolpes [...]\",\n  \"source\": \"enevaeldens_nyheder\",\n  \"added\": \"2025-08-05\",\n  \"created\": \"1767-01-02, 1767-01-02\",\n  \"token_count\": 2377\n}\n</code></pre>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#dataset-creation","title":"Dataset Creation","text":""},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#curation-rationale","title":"Curation Rationale","text":"<p>The newspapers in the dataset generally represent the longest-running newspaper series in the Danish and Norwegian libraries. We prioritised long-running newspapers to enable historical analysis of changes over time. As historians, this was our initial ambition: to allow us to get quality serial text data.  We also prioritised geographical diversity, representing different regions of Denmark-Norway. Of course, this varies over time, as newspapers were most common in Copenhagen until the late eighteenth century. Since the newspapers of Denmark's Caribbean colonies were primarily in English, they are not included. The text recognition model designed for the project struggles with English text. Besides long-running series, we also included a few smaller newspaper series, mainly with an eye towards diversity of subject matter. These include Politivennen, which was concerned with very local news from Copenhagen and carried a lot of reader contributions, offering a unique insight into urban sentiments at the time. A similar inclusion was made with Jyllandsposten (of 1838), which was defined by a somewhat radical rural horizon.</p> <p>As a rule of thumb, publications have been digitised in total, as they exist in their respective collections.  This means that they sometimes include appendices and sometimes do not, depending on whether these exist. Holes in the dataset mirror holes in the archival collections.  The one exception to this rule is the newspaper K\u00f8benhavns Adresseavis. This advertisement paper has survived continuously from its inception in 1759, but from 1804 onwards, it is only included here with samples every fifth year.  The reason for sampling is a combination of the massive extent of this advertisement paper and the poor condition of the digital images available for this specific period. Combined this meant that the results of the text recognition process were not entirely satisfying relative to the resources necessary for the effort. Therefore, we decided to prioritize other publications that would yield better quality text.</p> <p>Most publications contain title page marginalia (date, title, etc.). Because these were set with large ornamental types, they are typically recognised with much less accuracy than the regular text. We are currently working on implementing a step in the workflow to identify and filter out these elements.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#data-collection-and-processing","title":"Data Collection and Processing","text":"<p>The text recognition model used to create the dataset is available via Transkribus. A description of the text segmentation process can be found here. Besides segmentation into separate news items / advertisements, no further processing of the text has taken place. We are currently experimenting with automated error correction using decoder-models.</p> <p>For Danish Dynaword we apply additional filtering including:</p> <ul> <li>1) Removing 1 word documents (using a whitespace split) </li> <li>2) Removing document with a PWA &lt; 0.7</li> </ul> <p>PWA is defined as:</p> <p>A predicted word accuracy [PWA] based on a dictionary consisting of words from literary works, personal names and place names from the census of 1787, and a manually curated list of common words that are present in the material, but not represented in canonical literature. This is an estimate. In general we advise that you filter the dataset on this variable in case of using the material for language modelling. This will also filter out texts in other languages than Danish.</p> <p>source: JohanHeinsen/ENO</p> <p>Below you see 10 examples of documents (truncated to 200 characters) filtered out due to the PWA filtering:</p> <p><code>['Under Staders Segl. nespil.',  'Frisk Selter=, Permonter=, Bitter, og FachingerVand bekommes paa L\u00f8veapotheket.',  'S\u00f8esyglinsk, Christoph. Auf Anordning der Liquidations=Commission, den ten August 1834. (Ges.) Mitglied der Commission, Regierungsrath: Pestof. Stellvertretender Secretair. Gabriel Ostrowski.',  'J de Reussiske Koge: Bordelummer Seil.',  'Scriptores historiae Byzantinae vird bei uns un entgeltlich ansgegeben. Anch sind bei und fortige Bogen dieses Werkes in den verschiedenen Ansgeden auf Druck., Schreibe und Velinpapier niedergelegt, z',  'Gammel Conjac. Potten.',  'NOTIFICATION. Von der 5ten Classe, der 7ten K\u00f6nigl. allen privilegitten Kopenhagner Lotteren, deren Ziehung den 17ten Passati geendiget worden, werden die Gewinne den 8ten hujus und f\u00f8lgende Werkeltag',  'Jm Verlag des Unterzeichneten har die Presse verlassen: Uever dis religi\u00f8se Bestimmung der Jugend, in einigen Predigten von K. C. von Gehren. Jn dieser Samlung sind f\u00f8lgende Gegenstande behandelt: 1) ',  \"ditoyens fortund, ) vous qui, loin des combats, Pouves jouir en pair dans vos heureur ClimatsDes trefors annuel d'unne moisson fertileDont il plait aux saisons de couronner votre ile, Vous, diseje, a \",  'AVERTISSEMENTS. Ausser der am Seelandischen Langericht geschehene Proclamation, wird auch hiedurch zu dreien mahlen kund gethan, das die Theilungs Berichtigung nach dem menland Johann Georg Kanneworff']</code></p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#dataset-statistics","title":"Dataset Statistics","text":"<p>The coverage of the newspapers included can be seen here:</p> <p></p> <p>The distribution of texts pr. year is as follows:</p> <p></p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#personal-and-sensitive-information","title":"Personal and Sensitive Information","text":"<p>Due to the historical nature of the data, ENO contains no personal or sensitive information.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#bias-risks-and-limitations","title":"Bias, Risks, and Limitations","text":"<p>The data reflects the time of its initial creation. This means that it mirrors and describes a deeply hierarchical society that was structured by deep-seated biases and forms of discrimination that are alien even to some of the worst among the living today. For example, the material contains racist language in describing contemporary phenomena such as the Transatlantic slave trade and the persecution of Jewish diasporas. Use cases which might relay or perpetuate such sentiments should be aware of these risks. It is a historical text corpora, warts and all.</p> <p>Please also note that, although the newspapers are all in Danish, they do contain intermittent passages in German and Latin.</p> <p>Some advertisements were reprinted verbatim. The dataset, therefore, includes occasional duplicate texts.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#license-information","title":"License Information","text":"<p>The dataset is licensed under CC BY-SA 4.0. Please note that this license only pertains to the digitised text and dataset curation, not the original images. The original images of all material stemming from The Danish Royal Library, Nettbiblioteket, K\u00f8benhavns Biblioteker as well as the scans of Nyeste Skilderie af Ki\u00f8benhavn made available by Niels Jensen are all in the public domain.</p>"},{"location":"data/enevaeldens_nyheder/enevaeldens_nyheder.html#more-information","title":"More Information","text":"<p>For questions related to the dataset, curation, and annotation we please contact Johan Heinsen, Aalborg University heinsen@dps.aau.dk</p>"},{"location":"data/ep/ep.html","title":"Dataset Card for European Parliament","text":"<p>The Danish subsection of Europarl.</p> <p>The europarl is a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. It was initially intended as training data for statistical machine translation.</p>"},{"location":"data/ep/ep.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 3.93K</li> <li>Number of tokens (Llama 3): 100.84M</li> <li>Average document length in tokens (min, max): 25.66K (8, 222.73K)</li> </ul>"},{"location":"data/ep/ep.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"ep_07-02-01-008\",\n  \"text\": \"TALER 6703: Jeg har stemt for henstillingen om godkendelse af opdelingsanordninger til beskyttelse a[...]\",\n  \"source\": \"ep\",\n  \"added\": \"2019-11-20\",\n  \"created\": \"2004-01-01, 2009-01-01\",\n  \"token_count\": 16237\n}\n</code></pre>"},{"location":"data/ep/ep.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ep/ep.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ep/ep.html#additional-information","title":"Additional Information","text":""},{"location":"data/ep/ep.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html","title":"Dataset Card for EUR-Lex SUM","text":"<p>The Danish subsection of EUR-lex SUM consisting of EU legislation paired with professionally written summaries.</p> <p>EUR-Lex SUM is a dataset containing summaries of EU legislation from the EUR-Lex database. It consists of pairs of full legal texts and their corresponding professionally written summaries, covering European Union legal documents. The dataset is designed for training and evaluating automatic text summarization systems, particularly for legal documents. It's valuable for natural language processing (NLP) research since it provides high-quality, human-written summaries of complex legal texts in a specialized domain.</p>"},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.00K</li> <li>Number of tokens (Llama 3): 31.37M</li> <li>Average document length in tokens (min, max): 31.31K (2.14K, 1.72M)</li> </ul>"},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#additional-information","title":"Additional Information","text":""},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#license-information","title":"License Information","text":""},{"location":"data/eur-lex-sum-da/eur-lex-sum-da.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work.</p>"},{"location":"data/fm-udgivelser/fm-udgivelser.html","title":"Dataset Card for Finansministeriets Udgivelser","text":"<p>The official publication series of the Danish Ministry of Finance containing economic analyses, budget proposals, and fiscal policy documents.</p> <p>Finansministeriets Udgivelser (translated as \"Publications of the Ministry of Finance\") is the publishing arm or publication series of the Danish Ministry of Finance. It includes official reports, economic analyses, budget proposals, fiscal policy documents, and various other publications related to Denmark's public finances, economic policy, and financial governance.</p> <p>These publications typically provide insights into Denmark's economic outlook, public spending plans, tax policies, and financial reforms. They serve as important reference materials for economists, policy makers, researchers, and citizens interested in understanding Denmark's financial policies and economic direction.</p> <p>The publications are authoritative sources of information on Danish fiscal policy and are often used by various stakeholders to track and analyze the country's economic performance and public finance management.</p>"},{"location":"data/fm-udgivelser/fm-udgivelser.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 443</li> <li>Number of tokens (Llama 3): 50.34M</li> <li>Average document length in tokens (min, max): 113.62K (209, 595.33K)</li> </ul>"},{"location":"data/fm-udgivelser/fm-udgivelser.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/fm-udgivelser/fm-udgivelser.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/fm-udgivelser/fm-udgivelser.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/fm-udgivelser/fm-udgivelser.html#additional-information","title":"Additional Information","text":""},{"location":"data/fm-udgivelser/fm-udgivelser.html#license-information","title":"License Information","text":""},{"location":"data/fm-udgivelser/fm-udgivelser.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work.</p>"},{"location":"data/ft/ft.html","title":"Dataset Card for Folketinget","text":""},{"location":"data/ft/ft.html#dataset-description","title":"Dataset Description","text":"<p>Records from all meetings of The Danish parliament (Folketinget) in the parliament hall. </p> <p>All records have a transcript produced by commercial Automatic Speech Recognition (ASR) followed by postediting by linguists employed by Folketinget for intelligibility, i.e., edit out dysfluencies, restarts, repairs, and mistakes. The transcript is, therefore, not a representation of spoken Danish but rather information content.</p> <p>In the parliament hall, one speaker at a time addresses members of the parliament. Monologues may include rebuttals or other comments to statements in previous monologues. While speakers can read aloud from a prepared statement or speak extemporaneously, we expect no difference to be apparent in the data because of the post-editing. The Folketinget section covers parliament hall sessions between 2009 and 2019. It contains discussions on a wide range of topics, issues, and named entities relevant to Danish society.</p> <ul> <li>Number of samples: 1.31K</li> <li>Number of tokens (Llama 3): 114.09M</li> <li>Average document length in tokens (min, max): 86.76K (49, 342.32K)</li> </ul>"},{"location":"data/ft/ft.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"ft_20121M100\",\n  \"text\": \"TALER 50: M\u00f8det er \u00e5bnet. I dag er der f\u00f8lgende anmeldelser: Ministeren for by, bolig og landdistrik[...]\",\n  \"source\": \"ft\",\n  \"added\": \"2021-03-28\",\n  \"created\": \"2009-01-01, 2019-01-01\",\n  \"token_count\": 84355\n}\n</code></pre>"},{"location":"data/ft/ft.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ft/ft.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ft/ft.html#additional-information","title":"Additional Information","text":""},{"location":"data/ft/ft.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/github_archive_filtered/github_archive_filtered.html","title":"Dataset Card for GitHub Archive","text":"<p>A large set of issues and pull request descriptions along with their comments.</p> <p>According to GitHub\u2019s terms of service, issues and pull request descriptions\u2014along with their comments\u2014inherit the license of their associated repository. To collect this data, we used the GitHub Archive\u2019s public BigQuery table of events to extract all issue, pull request, and comment events since 2011 and aggregated them into threads. The table appeared to be missing \u201cedit\u201d events so the text from each comment is the original from when it was first posted. We filtered out comments from bots. This resulted in approximately 177 million threads across 19 million repositories. We then removed threads whose repositories did not have a Blue Oak Council-approved license. License information for each repository comes from either 1) the \u201cpublic-data:github_repos\u201d BigQuery Table, 2) metadata from the StackV2, or 3) the GitHub API. License filtering left 10 million repositories. PyMarkdown was used to convert from GitHub-flavored markdown to plain text. When parsing failed, the raw markdown was kept. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/github_archive_filtered/github_archive_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 23.35M</li> <li>Number of tokens (Llama 3): 10.21B</li> <li>Average document length in tokens (min, max): 437.26702271848893 (2, 657.71K)</li> </ul>"},{"location":"data/github_archive_filtered/github_archive_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/github_archive_filtered/github_archive_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/github_archive_filtered/github_archive_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/github_archive_filtered/github_archive_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/github_archive_filtered/github_archive_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/github_archive_filtered/github_archive_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/grundtvig/grundtvig.html","title":"Dataset Card for Grundtvig's Works","text":"<p>The complete collection of Grundtvig (1783-1872) one of Denmark\u2019s most influential figures. </p> <p>Grundtvig's Works is a comprehensive digital humanities dataset containing the complete collected writings of Nicolai Frederik Severin Grundtvig (1783-1872) was one of Denmark\u2019s most influential cultural and intellectual figures.  As a critical edition, it includes editorial commentary by philologists and is continually updated.  The project is scheduled for completion in 2030 and will comprise 1,000 individual works spanning 35,000 pages. The complete edition is freely available online.</p>"},{"location":"data/grundtvig/grundtvig.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 632</li> <li>Number of tokens (Llama 3): 10.53M</li> <li>Average document length in tokens (min, max): 16.65K (100, 453.72K)</li> </ul> <p>This dataset represents version 1.25 (updated May 2025) of the digital scholarly edition of Grundtvig\u2019s Works, comprising 632 texts by N.F.S. Grundtvig.  All texts have been OCR-scanned, and each is processed through three separate textual collations. We compare three different first editions, identify variants between them,  and incorporate their differences into the digitized version.</p> <p>Following collation, we enrich the texts with several layers of annotation, marked up in XML according to TEI P5 guidelines. </p> <p>These include: </p> <ul> <li>Explanatory commentaries \u2013 clarifying older words or shifts in meaning</li> <li>Named entities \u2013 identifying people, places, titles, and mythological figures</li> <li>Emendations \u2013 documenting any corrections (no silent changes are made)</li> <li>Bible references - allusions, quotations, and explicit references are identified and categorized according to type and source</li> <li>We also provide introductory texts and textual essays to offer historical and interpretive context. Before publication, each text undergoes a triple review process.</li> </ul>"},{"location":"data/grundtvig/grundtvig.html#dataset-sources","title":"Dataset Sources","text":"<ul> <li>Dataset Website: www.grundtvigsv\u00e6rker.dk</li> </ul>"},{"location":"data/grundtvig/grundtvig.html#uses","title":"Uses","text":"<p>This dataset represents a major digital preservation effort of Denmark's literary and intellectual heritage, providing structured access to works that shaped Danish theology, education, democracy, and cultural identity. It's valuable for research in digital humanities, Scandinavian studies, religious history, and 19th-century European thought.</p> <p>In addition, this dataset is also a part of Danish Dynaword, a collection of dataset intended for training language models, thus integrating Danish Cultural Heritage into the next generation of digital technologies.</p>"},{"location":"data/grundtvig/grundtvig.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"grundtvig_1824_392_txt\",\n  \"text\": \"---\\ntitle: S\u00f8gubrot med Efterklang\\nauthor: Nicolai Frederik Severin Grundtvig\\ndate: 2019-12-03\\npubli[...]\",\n  \"source\": \"grundtvig\",\n  \"added\": \"2025-07-21\",\n  \"created\": \"1824-01-01, 1824-12-31\",\n  \"token_count\": 4106\n}\n</code></pre>"},{"location":"data/grundtvig/grundtvig.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/grundtvig/grundtvig.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/grundtvig/grundtvig.html#dataset-creation","title":"Dataset Creation","text":""},{"location":"data/grundtvig/grundtvig.html#curation-rationale","title":"Curation Rationale","text":"<p>The digital scholarly edition of Grundtvig\u2019s Works was created to provide open, reliable, and annotated access to the entire published oeuvre of N.F.S. Grundtvig (1783\u20131872), one of Denmark\u2019s most influential thinkers. The rationale behind this effort is twofold: public accessibility and scholarly accuracy.</p> <p>On the one hand, the edition enables the general public to read Grundtvig\u2019s works on their own terms, supported by textual commentary that helps decode complex 19th-century language and theological or philosophical concepts. On the other, the edition serves as a scholarly tool, offering a searchable, critically edited, and TEI-encoded corpus that facilitates in-depth research across disciplines.</p> <p>Grundtvig\u2019s writings have had a lasting influence on Danish culture, education, and national identity. They are frequently referenced in contemporary political and cultural debates. However, many of his texts have until now only existed in fragile first editions or scattered, outdated collections. By digitizing, editing, and annotating his complete published works \u2013 including posthumous publications central to his public image \u2013 the project ensures both preservation and access.</p> <p>The primary motivation behind creating this dataset was to bridge the gap between Grundtvig\u2019s historical significance and the limited access to his writings. By offering a freely accessible digital edition, the project not only preserves a vital part of Danish cultural heritage but also democratizes access to foundational texts in Danish intellectual history. This aligns with both public interest and scholarly needs: to make Grundtvig\u2019s complex legacy understandable, searchable, and usable in modern contexts.</p> <p>The edition was launched in 2010 by the Center for Grundtvig Studies and is scheduled for completion in 2030. It is funded by the Danish Finance Act, ensuring its continued development as a national cultural and scholarly resource.</p>"},{"location":"data/grundtvig/grundtvig.html#data-collection-and-processing","title":"Data Collection and Processing","text":"<p>All texts in the Grundtvig\u2019s Works dataset originate from printed first editions of N.F.S. Grundtvig\u2019s published writings. The digitization process begins with OCR (optical character recognition) scanning of the original editions. Following OCR, each text undergoes three separate textual collations: we compare three different first editions of the same work to identify textual variants. Differences are systematically incorporated into the digitized version to ensure accuracy and representational fidelity.</p> <p>After collation, the editorial team performs further corrections based on internal editorial review. The final result is a fully digitized, TEI P5 XML\u2013encoded version of each text. On top of this, all works are accompanied by facsimiles \u2013 high-resolution images of the first printed editions \u2013 allowing users to view the original sources alongside the transcribed and annotated texts.</p> <p>Before upload the dataset was provided as XML and .txt files. The XML files were converted to .md and uploaded to Huggingface. The scripts for conversion and upload can be found here and a .lock file specifying the version can be found here.</p>"},{"location":"data/grundtvig/grundtvig.html#who-are-the-source-data-producers","title":"Who are the source data producers?","text":"<p>N.F.S. Grundtvig is the author of the source material. The Center for Grundtvig Studies at Aarhus University curates, digitizes, and maintains the dataset.</p>"},{"location":"data/grundtvig/grundtvig.html#annotations","title":"Annotations","text":"<p>We annotate explanatory commentaries, named entities (people, places, titles, and mythological figures), biblical references and emendations (documenting corrections). Only emendations are part of this dataset. The annotation process began in 2010 and will continue until 2030. </p>"},{"location":"data/grundtvig/grundtvig.html#who-are-the-annotators","title":"Who are the annotators?","text":"<p>The editorial team consists of 12 philologists and 5 student assistants and one editor in chief.</p> <p>As of 2025, the team includes 13 female and 5 male staff members.</p>"},{"location":"data/grundtvig/grundtvig.html#formatting","title":"Formatting","text":"<p>The samples are currently formatted as markdown using a frontmatter which contain information about the author, year of digitization etc.</p>"},{"location":"data/grundtvig/grundtvig.html#personal-and-sensitive-information","title":"Personal and Sensitive Information","text":"<p>This dataset contains no personal or sensitive information.</p>"},{"location":"data/grundtvig/grundtvig.html#bias-risks-and-limitations","title":"Bias, Risks, and Limitations","text":"<p>The Grundtvig\u2019s Works dataset contains texts written in 19th-century Danish and reflects the linguistic, cultural, and ideological norms of its time. As such, it includes perspectives, assumptions, and biases characteristic of the period. Readers should be aware that the author, N.F.S. Grundtvig, expressed strong personal and political opinions,  including nationalistic views and critical stances toward specific groups \u2013 such as Germans \u2013 which may be considered offensive or exclusionary by contemporary standards.</p>"},{"location":"data/grundtvig/grundtvig.html#license-information","title":"License Information","text":"<p>N.F.S. Grundtvig's works fall under Public Domain (CC0)</p>"},{"location":"data/grundtvig/grundtvig.html#citation-information","title":"Citation Information","text":"<p>Studies where the dataset from Grundtvig\u2019s Works has been used:</p> <ul> <li>Baunvig, K. F., &amp; Nielbo, K. L. (2022). Mermaids are Birds: Embedding N.F.S. Grundtvig\u2019s Bestiary. I K. Berglund, M. La Mela, &amp; I. Zwart (red.), Proceedings of the 6th Digital Humanities in the Nordic and Baltic Countries Conference (DHNB 2022) (Bind 3232, s. 23-32). CEUR-WS.org. http://ceur-ws.org/Vol-3232/paper02.pdf</li> <li>Baunvig, K. F., Jarvis, O., &amp; Nielbo, K. L. (2021). Emotional Imprints: Exclamation Marks in N.F.S. Grundtvig's Writings. I S. Reinsone, I. Skadi\u0146a, J. Daugavietis, &amp; A. Bakl\u0101ne (red.), Post-Proceedings of the 5th Conference Digital Humanities in the Nordic Countries (DHN 2020) (s. 156-169) http://ceur-ws.org/Vol-2865/short7.pdf</li> <li>Nielbo, K.L., Baunvig, K. F., Liu, B., &amp; Gao, J. (2019). A Curious Case of Entropic Decay: Persistent Complexity in Textual Cultural Heritage. Digital Scholarship in the Humanities, Volume 34, Issue 3, September 2019, Pages 542\u2013557, https://doi.org/10.1093/llc/fqy054.</li> </ul>"},{"location":"data/grundtvig/grundtvig.html#more-information","title":"More Information","text":"<p>For questions related to the dataset, curation, and annotation we please contact Center for Grundtvig Studies</p> <p>The edition is funded by the Danish Finance Act</p>"},{"location":"data/gutenberg/gutenberg.html","title":"Dataset Card for Gutenberg","text":""},{"location":"data/gutenberg/gutenberg.html#dataset-description","title":"Dataset Description","text":"<p>The Danish subsection from Project Gutenberg. </p> <p>Project Gutenberg is an online library of free eBooks. Project Gutenberg was the first provider of free electronic books, or eBooks.</p> <ul> <li>Number of samples: 66</li> <li>Number of tokens (Llama 3): 6.76M</li> <li>Average document length in tokens (min, max): 102.47K (7.92K, 250.51K)</li> </ul>"},{"location":"data/gutenberg/gutenberg.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"gutenberg_43899\",\n  \"text\": \"Afskriverens bem\u00e6rkninger: \u00c5benlyse trykfejl er rettet\\ni denne e-bog, men forfatterens stavning er f[...]\",\n  \"source\": \"gutenberg\",\n  \"added\": \"2020-09-12\",\n  \"created\": \"1700-01-01, 2022-01-01\",\n  \"token_count\": 128423\n}\n</code></pre>"},{"location":"data/gutenberg/gutenberg.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/gutenberg/gutenberg.html#license-information","title":"License Information","text":"Gutenberg License <p> <pre><code>*** START: FULL LICENSE ***\n\nTHE FULL PROJECT GUTENBERG LICENSE\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n\nTo protect the Project Gutenberg-tm mission of promoting the free\ndistribution of electronic works, by using or distributing this work\n(or any other work associated in any way with the phrase \"Project\nGutenberg\"), you agree to comply with all the terms of the Full Project\nGutenberg-tm License available with this file or online at\n  www.gutenberg.org/license.\n\n\nSection 1.  General Terms of Use and Redistributing Project Gutenberg-tm\nelectronic works\n\n1.A.  By reading or using any part of this Project Gutenberg-tm\nelectronic work, you indicate that you have read, understand, agree to\nand accept all the terms of this license and intellectual property\n(trademark/copyright) agreement.  If you do not agree to abide by all\nthe terms of this agreement, you must cease using and return or destroy\nall copies of Project Gutenberg-tm electronic works in your possession.\nIf you paid a fee for obtaining a copy of or access to a Project\nGutenberg-tm electronic work and you do not agree to be bound by the\nterms of this agreement, you may obtain a refund from the person or\nentity to whom you paid the fee as set forth in paragraph 1.E.8.\n\n1.B.  \"Project Gutenberg\" is a registered trademark.  It may only be\nused on or associated in any way with an electronic work by people who\nagree to be bound by the terms of this agreement.  There are a few\nthings that you can do with most Project Gutenberg-tm electronic works\neven without complying with the full terms of this agreement.  See\nparagraph 1.C below.  There are a lot of things you can do with Project\nGutenberg-tm electronic works if you follow the terms of this agreement\nand help preserve free future access to Project Gutenberg-tm electronic\nworks.  See paragraph 1.E below.\n\n1.C.  The Project Gutenberg Literary Archive Foundation (\"the Foundation\"\nor PGLAF), owns a compilation copyright in the collection of Project\nGutenberg-tm electronic works.  Nearly all the individual works in the\ncollection are in the public domain in the United States.  If an\nindividual work is in the public domain in the United States and you are\nlocated in the United States, we do not claim a right to prevent you from\ncopying, distributing, performing, displaying or creating derivative\nworks based on the work as long as all references to Project Gutenberg\nare removed.  Of course, we hope that you will support the Project\nGutenberg-tm mission of promoting free access to electronic works by\nfreely sharing Project Gutenberg-tm works in compliance with the terms of\nthis agreement for keeping the Project Gutenberg-tm name associated with\nthe work.  You can easily comply with the terms of this agreement by\nkeeping this work in the same format with its attached full Project\nGutenberg-tm License when you share it without charge with others.\n\n1.D.  The copyright laws of the place where you are located also govern\nwhat you can do with this work.  Copyright laws in most countries are in\na constant state of change.  If you are outside the United States, check\nthe laws of your country in addition to the terms of this agreement\nbefore downloading, copying, displaying, performing, distributing or\ncreating derivative works based on this work or any other Project\nGutenberg-tm work.  The Foundation makes no representations concerning\nthe copyright status of any work in any country outside the United\nStates.\n\n1.E.  Unless you have removed all references to Project Gutenberg:\n\n1.E.1.  The following sentence, with active links to, or other immediate\naccess to, the full Project Gutenberg-tm License must appear prominently\nwhenever any copy of a Project Gutenberg-tm work (any work on which the\nphrase \"Project Gutenberg\" appears, or with which the phrase \"Project\nGutenberg\" is associated) is accessed, displayed, performed, viewed,\ncopied or distributed:\n\nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.org\n\n1.E.2.  If an individual Project Gutenberg-tm electronic work is derived\nfrom the public domain (does not contain a notice indicating that it is\nposted with permission of the copyright holder), the work can be copied\nand distributed to anyone in the United States without paying any fees\nor charges.  If you are redistributing or providing access to a work\nwith the phrase \"Project Gutenberg\" associated with or appearing on the\nwork, you must comply either with the requirements of paragraphs 1.E.1\nthrough 1.E.7 or obtain permission for the use of the work and the\nProject Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or\n1.E.9.\n\n1.E.3.  If an individual Project Gutenberg-tm electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any additional\nterms imposed by the copyright holder.  Additional terms will be linked\nto the Project Gutenberg-tm License for all works posted with the\npermission of the copyright holder found at the beginning of this work.\n\n1.E.4.  Do not unlink or detach or remove the full Project Gutenberg-tm\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg-tm.\n\n1.E.5.  Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg-tm License.\n\n1.E.6.  You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including any\nword processing or hypertext form.  However, if you provide access to or\ndistribute copies of a Project Gutenberg-tm work in a format other than\n\"Plain Vanilla ASCII\" or other format used in the official version\nposted on the official Project Gutenberg-tm web site (www.gutenberg.org),\nyou must, at no additional cost, fee or expense to the user, provide a\ncopy, a means of exporting a copy, or a means of obtaining a copy upon\nrequest, of the work in its original \"Plain Vanilla ASCII\" or other\nform.  Any alternate format must include the full Project Gutenberg-tm\nLicense as specified in paragraph 1.E.1.\n\n1.E.7.  Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg-tm works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8.  You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg-tm electronic works provided\nthat\n\n- You pay a royalty fee of 20% of the gross profits you derive from\n     the use of Project Gutenberg-tm works calculated using the method\n     you already use to calculate your applicable taxes.  The fee is\n     owed to the owner of the Project Gutenberg-tm trademark, but he\n     has agreed to donate royalties under this paragraph to the\n     Project Gutenberg Literary Archive Foundation.  Royalty payments\n     must be paid within 60 days following each date on which you\n     prepare (or are legally required to prepare) your periodic tax\n     returns.  Royalty payments should be clearly marked as such and\n     sent to the Project Gutenberg Literary Archive Foundation at the\n     address specified in Section 4, \"Information about donations to\n     the Project Gutenberg Literary Archive Foundation.\"\n\n- You provide a full refund of any money paid by a user who notifies\n     you in writing (or by e-mail) within 30 days of receipt that s/he\n     does not agree to the terms of the full Project Gutenberg-tm\n     License.  You must require such a user to return or\n     destroy all copies of the works possessed in a physical medium\n     and discontinue all use of and all access to other copies of\n     Project Gutenberg-tm works.\n\n- You provide, in accordance with paragraph 1.F.3, a full refund of any\n     money paid for a work or a replacement copy, if a defect in the\n     electronic work is discovered and reported to you within 90 days\n     of receipt of the work.\n\n- You comply with all other terms of this agreement for free\n     distribution of Project Gutenberg-tm works.\n\n1.E.9.  If you wish to charge a fee or distribute a Project Gutenberg-tm\nelectronic work or group of works on different terms than are set\nforth in this agreement, you must obtain permission in writing from\nboth the Project Gutenberg Literary Archive Foundation and Michael\nHart, the owner of the Project Gutenberg-tm trademark.  Contact the\nFoundation as set forth in Section 3 below.\n\n1.F.\n\n1.F.1.  Project Gutenberg volunteers and employees expend considerable\neffort to identify, do copyright research on, transcribe and proofread\npublic domain works in creating the Project Gutenberg-tm\ncollection.  Despite these efforts, Project Gutenberg-tm electronic\nworks, and the medium on which they may be stored, may contain\n\"Defects,\" such as, but not limited to, incomplete, inaccurate or\ncorrupt data, transcription errors, a copyright or other intellectual\nproperty infringement, a defective or damaged disk or other medium, a\ncomputer virus, or computer codes that damage or cannot be read by\nyour equipment.\n\n1.F.2.  LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right\nof Replacement or Refund\" described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg-tm trademark, and any other party distributing a Project\nGutenberg-tm electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees.  YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3.  YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3.  LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from.  If you\nreceived the work on a physical medium, you must return the medium with\nyour written explanation.  The person or entity that provided you with\nthe defective work may elect to provide a replacement copy in lieu of a\nrefund.  If you received the work electronically, the person or entity\nproviding it to you may choose to give you a second opportunity to\nreceive the work electronically in lieu of a refund.  If the second copy\nis also defective, you may demand a refund in writing without further\nopportunities to fix the problem.\n\n1.F.4.  Except for the limited right of replacement or refund set forth\nin paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO OTHER\nWARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\nWARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n\n1.F.5.  Some states do not allow disclaimers of certain implied\nwarranties or the exclusion or limitation of certain types of damages.\nIf any disclaimer or limitation set forth in this agreement violates the\nlaw of the state applicable to this agreement, the agreement shall be\ninterpreted to make the maximum disclaimer or limitation permitted by\nthe applicable state law.  The invalidity or unenforceability of any\nprovision of this agreement shall not void the remaining provisions.\n\n1.F.6.  INDEMNITY - You agree to indemnify and hold the Foundation, the\ntrademark owner, any agent or employee of the Foundation, anyone\nproviding copies of Project Gutenberg-tm electronic works in accordance\nwith this agreement, and any volunteers associated with the production,\npromotion and distribution of Project Gutenberg-tm electronic works,\nharmless from all liability, costs and expenses, including legal fees,\nthat arise directly or indirectly from any of the following which you do\nor cause to occur: (a) distribution of this or any Project Gutenberg-tm\nwork, (b) alteration, modification, or additions or deletions to any\nProject Gutenberg-tm work, and (c) any Defect you cause.\n\n\nSection  2.  Information about the Mission of Project Gutenberg-tm\n\nProject Gutenberg-tm is synonymous with the free distribution of\nelectronic works in formats readable by the widest variety of computers\nincluding obsolete, old, middle-aged and new computers.  It exists\nbecause of the efforts of hundreds of volunteers and donations from\npeople in all walks of life.\n\nVolunteers and financial support to provide volunteers with the\nassistance they need are critical to reaching Project Gutenberg-tm's\ngoals and ensuring that the Project Gutenberg-tm collection will\nremain freely available for generations to come.  In 2001, the Project\nGutenberg Literary Archive Foundation was created to provide a secure\nand permanent future for Project Gutenberg-tm and future generations.\nTo learn more about the Project Gutenberg Literary Archive Foundation\nand how your efforts and donations can help, see Sections 3 and 4\nand the Foundation information page at www.gutenberg.org\n\n\nSection 3.  Information about the Project Gutenberg Literary Archive\nFoundation\n\nThe Project Gutenberg Literary Archive Foundation is a non profit\n501(c)(3) educational corporation organized under the laws of the\nstate of Mississippi and granted tax exempt status by the Internal\nRevenue Service.  The Foundation's EIN or federal tax identification\nnumber is 64-6221541.  Contributions to the Project Gutenberg\nLiterary Archive Foundation are tax deductible to the full extent\npermitted by U.S. federal laws and your state's laws.\n\nThe Foundation's principal office is located at 4557 Melan Dr. S.\nFairbanks, AK, 99712., but its volunteers and employees are scattered\nthroughout numerous locations.  Its business office is located at 809\nNorth 1500 West, Salt Lake City, UT 84116, (801) 596-1887.  Email\ncontact links and up to date contact information can be found at the\nFoundation's web site and official page at www.gutenberg.org/contact\n\nFor additional contact information:\n     Dr. Gregory B. Newby\n     Chief Executive and Director\n     gbnewby@pglaf.org\n\nSection 4.  Information about Donations to the Project Gutenberg\nLiterary Archive Foundation\n\nProject Gutenberg-tm depends upon and cannot survive without wide\nspread public support and donations to carry out its mission of\nincreasing the number of public domain and licensed works that can be\nfreely distributed in machine readable form accessible by the widest\narray of equipment including outdated equipment.  Many small donations\n($1 to $5,000) are particularly important to maintaining tax exempt\nstatus with the IRS.\n\nThe Foundation is committed to complying with the laws regulating\ncharities and charitable donations in all 50 states of the United\nStates.  Compliance requirements are not uniform and it takes a\nconsiderable effort, much paperwork and many fees to meet and keep up\nwith these requirements.  We do not solicit donations in locations\nwhere we have not received written confirmation of compliance.  To\nSEND DONATIONS or determine the status of compliance for any\nparticular state visit www.gutenberg.org/donate\n\nWhile we cannot and do not solicit contributions from states where we\nhave not met the solicitation requirements, we know of no prohibition\nagainst accepting unsolicited donations from donors in such states who\napproach us with offers to donate.\n\nInternational donations are gratefully accepted, but we cannot make\nany statements concerning tax treatment of donations received from\noutside the United States.  U.S. laws alone swamp our small staff.\n\nPlease check the Project Gutenberg Web pages for current donation\nmethods and addresses.  Donations are accepted in a number of other\nways including checks, online payments and credit card donations.\nTo donate, please visit:  www.gutenberg.org/donate\n\n\nSection 5.  General Information About Project Gutenberg-tm electronic\nworks.\n\nProfessor Michael S. Hart was the originator of the Project Gutenberg-tm\nconcept of a library of electronic works that could be freely shared\nwith anyone.  For forty years, he produced and distributed Project\nGutenberg-tm eBooks with only a loose network of volunteer support.\n\nProject Gutenberg-tm eBooks are often created from several printed\neditions, all of which are confirmed as Public Domain in the U.S.\nunless a copyright notice is included.  Thus, we do not necessarily\nkeep eBooks in compliance with any particular paper edition.\n\nMost people start at our Web site which has the main PG search facility:\n\n     www.gutenberg.org\n\nThis Web site includes information about Project Gutenberg-tm,\nincluding how to make donations to the Project Gutenberg Literary\nArchive Foundation, how to help produce our new eBooks, and how to\nsubscribe to our email newsletter to hear about new eBooks.\n</code></pre> </p>"},{"location":"data/gutenberg/gutenberg.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/gutenberg/gutenberg.html#additional-information","title":"Additional Information","text":""},{"location":"data/gutenberg/gutenberg.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/gutenberg/gutenberg.html#additional-information_1","title":"Additional Information","text":""},{"location":"data/gutenberg/gutenberg.html#citation-information_1","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/health_hovedstaden/health_hovedstaden.html","title":"Dataset Card for Health Hovedstaden","text":"<p>Guidelines and informational documents for healthcare professionals from the Capital Region</p> <p>The document collection consists of guidelines and informational documents for healthcare professionals in the Capital Region of Denmark. The documents therefore contain a number of specialized terms and concepts that are frequently used within the healthcare sector.</p> <p>The corpus was created based on the texts in the document collection and has been post-processed so that the texts can be used for the development of language technology.</p> <p>Martin Sundahl Laursen and Thiusius R. Savarimuthu from the University of Southern Denmark have assisted the Danish Agency for Digital Government with the post-processing of the data. Read their joint paper on \"Automatic Annotation of Training Data for Deep Learning Based De-identification of Narrative Clinical Text.\"</p>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 24.00K</li> <li>Number of tokens (Llama 3): 27.07M</li> <li>Average document length in tokens (min, max): 1.13K (4, 51.03K)</li> </ul>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"health_hovedstaden_0\",\n  \"text\": \"Acetylsalicylsyre - Aspirin, Akutl\u00e6gebil\\n\\nM\u00e5lgrupper og anvendelsesomr\u00e5de\\nDefinitioner\\nFremgangsm\u00e5de[...]\",\n  \"source\": \"health_hovedstaden\",\n  \"added\": \"2025-07-07\",\n  \"created\": \"2015-01-01, 2020-12-31\",\n  \"token_count\": 766\n}\n</code></pre>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/health_hovedstaden/health_hovedstaden.html#unintended-uses","title":"Unintended Uses","text":"<p>Please note that the corpus has been developed for the purpose of language technology development and should not be used as a source of healthcare information. The documents were scraped at a specific time and will therefore not be updated with changes. In this regard, please refer to the Capital Region of Denmark's document collection.</p>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/health_hovedstaden/health_hovedstaden.html#additional-information","title":"Additional Information","text":""},{"location":"data/health_hovedstaden/health_hovedstaden.html#license-information","title":"License Information","text":"<p>The dataset have been released under a CC-0 license. </p>"},{"location":"data/health_hovedstaden/health_hovedstaden.html#citation-information","title":"Citation Information","text":"<p>If you are using the data please reference the following paper Automatic Annotation of Training Data for Deep Learning Based De-identification of Narrative Clinical Text</p>"},{"location":"data/hest/hest.html","title":"Dataset Card for Hestenettet","text":"<p>Samples from the Danish debate forum www.heste-nettet.dk.</p> <p>The forum have been in use since 1997 and it is used as a debate forum covering a wide range of everyday topics. </p> <p>Its inclusion as training data for large language models have multiple times reached national news.</p>"},{"location":"data/hest/hest.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 14.34K</li> <li>Number of tokens (Llama 3): 389.32M</li> <li>Average document length in tokens (min, max): 27.15K (3, 9.81M)</li> </ul>"},{"location":"data/hest/hest.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"hest_forum112802271280227_0\",\n  \"text\": \"Er den ikke k\u00e6r? \\nJeg kan ikke forst\u00e5 at der altid er nogle der \u00e5benbart ser alle indl\u00e6g her p\u00e5 HN ,[...]\",\n  \"source\": \"hest\",\n  \"added\": \"2020-10-05\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 311\n}\n</code></pre>"},{"location":"data/hest/hest.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/hest/hest.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/hest/hest.html#additional-information","title":"Additional Information","text":""},{"location":"data/hest/hest.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/jvj/jvj.html","title":"Dataset Card for Johannes V. Jensen","text":"<p>The works of the Danish author and poet, Johannes V. Jensen.</p>"},{"location":"data/jvj/jvj.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 42</li> <li>Number of tokens (Llama 3): 3.55M</li> <li>Average document length in tokens (min, max): 84.50K (15.47K, 271.79K)</li> </ul>"},{"location":"data/jvj/jvj.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"jvj_J\u00f8rgine\",\n  \"text\": \"J\u00d8RGINE J\u00d8RGINE K\u00d8BENHAVN HAGE &amp; CLAUSENS FORLAG (J. FR. CLAUSEN) 1926 JOHANNES V. JENSEN COPYRIGHT [...]\",\n  \"source\": \"jvj\",\n  \"added\": \"2020-06-26\",\n  \"created\": \"1873-01-01, 1951-01-01\",\n  \"token_count\": 29393\n}\n</code></pre>"},{"location":"data/jvj/jvj.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/jvj/jvj.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/jvj/jvj.html#additional-information","title":"Additional Information","text":""},{"location":"data/jvj/jvj.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html","title":"Dataset Card for Library of Congress","text":"<p>A large set of public domain books from the \"Selected Digitized Books\" collection.</p> <p>The Library of Congress (LoC) curates a collection of public domain books called \"Selected Digitized Books\". We have downloaded over 130,000 English-language books from this public domain collection as OCR plain text files using the LoC APIs.</p>"},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 128.69K</li> <li>Number of tokens (Llama 3): 8.06B</li> <li>Average document length in tokens (min, max): 62.66K (18, 4.35M)</li> </ul>"},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/library_of_congress_filtered/library_of_congress_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/libretexts_filtered/libretexts_filtered.html","title":"Dataset Card for LibreTexts","text":"<p>A catalog of open-access text books.</p> <p>LibreTexts is an online platform that provides a catalog of over 3,000 open-access textbooks. To collect openly licensed content from LibreTexts we gather links to all textbooks in the catalog and check each textbook section for a license statement indicating that it is in the public domain or under a CC BY, CC BY-SA, or the GNU Free Documentation License. We extract plaintext from these textbook sections directly from the HTML pages hosted on the LibreTexts website. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/libretexts_filtered/libretexts_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 40.04K</li> <li>Number of tokens (Llama 3): 84.19M</li> <li>Average document length in tokens (min, max): 2.10K (21, 319.08K)</li> </ul>"},{"location":"data/libretexts_filtered/libretexts_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/libretexts_filtered/libretexts_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/libretexts_filtered/libretexts_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/libretexts_filtered/libretexts_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/libretexts_filtered/libretexts_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/libretexts_filtered/libretexts_filtered.html#citation-information","title":"Citation Information","text":"<pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre>"},{"location":"data/memo/memo.html","title":"Dataset Card for MeMo Canonical Novels","text":"<p>The MeMo corpus comprising almost all Danish novels from the period 1870-1899, known as the Modern Breakthrough.</p> <p>The MeMo corpus is established to investigate literary and cultural change in a seminal epoch of Scandinavian cultural and social history (known as 'the modern breakthrough') using natural language processing and other computational methods. The corpus consists of original novels by Norwegian and Danish authors printed in Denmark in the period 1870-99. It includes 858 volumes, totaling 4.5 million sentences and 65 million words.</p> <p>Additional information about this dataset can be found on their project page or on their huggingface dataset. The dataset can be inspected online using the Korp platform.</p>"},{"location":"data/memo/memo.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 858</li> <li>Number of tokens (Llama 3): 113.74M</li> <li>Average document length in tokens (min, max): 132.57K (6.67K, 720.17K)</li> </ul>"},{"location":"data/memo/memo.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"1887_Paulsen_EnFremtidskvinde\",\n  \"text\": \"En fremtidskvinde?\\n\\nSkrevet af John Paulsen\\nPubliceret 1887 af Schubothe\\n ------- \\n\\nDen skandinavisk[...]\",\n  \"source\": \"memo\",\n  \"added\": \"2025-06-23\",\n  \"created\": \"1887-01-01, 1887-12-31\",\n  \"token_count\": 98454\n}\n</code></pre>"},{"location":"data/memo/memo.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/memo/memo.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/memo/memo.html#processing","title":"Processing","text":"<p>In addition to the text itself we prefix the document with the title, year, author name, pseudonym and publisher. This is to allow the model to learn the relation between the document and relevant metadata.</p>"},{"location":"data/memo/memo.html#updated-and-corrections","title":"Updated and Corrections","text":"<p>This version fixed a previous issues in MeMo where the documents where incorrectly truncated and normalized. Removing this truncation led to a &gt;10x increase in number of tokens. </p>"},{"location":"data/memo/memo.html#additional-information","title":"Additional Information","text":""},{"location":"data/memo/memo.html#contact","title":"Contact","text":"<p>For questions related to the processing and conversion feel free to open a discussion thread for question related to the initial collect of the data please contact the project PI, Jens Bjerring-Hansen, associate professor at Copenhagen University.</p>"},{"location":"data/memo/memo.html#citation-information","title":"Citation Information","text":"<p>This dataset is derived from the publicly availabe dataset MiMe-MeMo/Corpus-v1.1 and was release as a part of paper Bjerring-Hansen, Jens, et al. \"Mending Fractured Texts. A heuristic procedure for correcting OCR data.\" (2022). https://ceur-ws.org/Vol-3232/paper14.pdf. Which has the follwing citation:</p> <pre><code>@inproceedings{bjerring2022mending,\n  title={Mending Fractured Texts. A heuristic procedure for correcting OCR data},\n  author={Bjerring-Hansen, Jens and Kristensen-McLachlan, Ross Deans and Diderichsen, Philip and Hansen, Dorte Haltrup},\n  booktitle={CEUR Workshop Proceedings},\n  volume={3232},\n  pages={177--186},\n  year={2022},\n  organization={ceur workshop proceedings}\n}\n</code></pre>"},{"location":"data/memo/memo.html#other-uses-of-this-dataset","title":"Other uses of this dataset","text":"<p>This study have additionally  <pre><code>@inproceedings{feldkamp_canonical_2024,\n    address = {Miami, Florida, USA},\n    title = {Canonical {Status} and {Literary} {Influence}: {A} {Comparative} {Study} of {Danish} {Novels} from the {Modern} {Breakthrough} (1870--1900)},\n    booktitle = {Proceedings of the {Joint} 4th {International} {Conference} on {Natural} {Language} {Processing} for {Digital} {Humanities}},\n    publisher = {Association for Computational Linguistics, Forthcoming},\n    author = {Feldkamp, Pascale and Lassche, Alie and Kostkan, Jan and Kardos, M\u00e1rton and Baunvig, Katrine F. and Nielbo, Kristoffer L.},\n    year = {2024},\n}\n</code></pre></p>"},{"location":"data/miljoeportalen/miljoeportalen.html","title":"Dataset Card for Milj\u00f8portalen","text":"<p>Data from Danmarks Milj\u00f8portalen (Denmark's Environment Portal)</p> <p>Denmark's Environment Portal (Danmarks Milj\u00f8portal) is a joint public partnership owned by the state, municipalities, and regions, which aims to support digital environmental management in Denmark.</p> <p>Danmarks Milj\u00f8portal's goal is for environmental data to be included early in all decisions that have an environmental impact. They do this by creating easy and open access to environmental data, making it possible for authorities and businesses to integrate the environment into their decisions.</p> <p>This can be decisions specifically targeted at the environment such as water plans, Green Tripartite Agreement, biodiversity and nature restoration, but also decisions about, for example, renewable energy, climate adaptation, new roads, residential areas, and industrial enterprises, where environmental aspects need to be considered.</p>"},{"location":"data/miljoeportalen/miljoeportalen.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 2.12K</li> <li>Number of tokens (Llama 3): 127.38M</li> <li>Average document length in tokens (min, max): 60.08K (54, 1.44M)</li> </ul>"},{"location":"data/miljoeportalen/miljoeportalen.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"miljoeportalen_0\",\n  \"text\": \"Bila110  g   1 101 10 - miljTIL R  lj  TIL RTIL R\u00f8ra\u00c6TSHUSKO  pp  \u00c6TSHUS KO\u00c6TSHUS Kort\\n\\nLOKALPLAN NR[...]\",\n  \"source\": \"miljoeportalen\",\n  \"added\": \"2025-03-24\",\n  \"created\": \"2024-01-01, 2025-01-01\",\n  \"token_count\": 9054\n}\n</code></pre>"},{"location":"data/miljoeportalen/miljoeportalen.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/miljoeportalen/miljoeportalen.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/miljoeportalen/miljoeportalen.html#additional-information","title":"Additional Information","text":""},{"location":"data/miljoeportalen/miljoeportalen.html#license-information","title":"License information","text":"<p>This dataset is licensed under CCO this license was clarified by support@miljoeportal.dk: </p> <p>Data er underlagt Creative Common CC0, se: https://creativecommons.org/publicdomain/zero/1.0/deed.da.</p> <p>Lad mig vide hvis du har yderligere sp\u00f8rgsm\u00e5l. Har du sp\u00f8rgsm\u00e5l til din sag eller yderligere kommentarer, bedes du besvare denne mail.</p>"},{"location":"data/miljoeportalen/miljoeportalen.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work.</p>"},{"location":"data/naat/naat.html","title":"Dataset Card for NAAT","text":"<p>Danish speeches from 1930-2022.</p>"},{"location":"data/naat/naat.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 129</li> <li>Number of tokens (Llama 3): 286.68K</li> <li>Average document length in tokens (min, max): 2.22K (228, 3.95K)</li> </ul>"},{"location":"data/naat/naat.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"naat_1958kongfrederikix\",\n  \"text\": \"Naar jeg i aften sender min nytaarshilsen til det danske folk og t\u00e6nker tilbage paa det aar, der sva[...]\",\n  \"source\": \"naat\",\n  \"added\": \"2020-02-11\",\n  \"created\": \"1930-01-01, 2022-01-01\",\n  \"token_count\": 1059\n}\n</code></pre>"},{"location":"data/naat/naat.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/naat/naat.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/naat/naat.html#additional-information","title":"Additional Information","text":""},{"location":"data/naat/naat.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/ncc_books/ncc_books.html","title":"Dataset Card for Norwegian Colossal Corpus (books)","text":"<p>Danish books extracted from the Norwegian Colossal Corpus derived from OCR. </p> <p>The Norwegian Colossal Corpus is a collection of multiple smaller Norwegian corpuses suitable for training large language models.  </p>"},{"location":"data/ncc_books/ncc_books.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 4.90K</li> <li>Number of tokens (Llama 3): 531.97M</li> <li>Average document length in tokens (min, max): 108.52K (58, 383.51K)</li> </ul>"},{"location":"data/ncc_books/ncc_books.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"digibok_2009033103031\",\n  \"text\": \"P. FR. RIST. OLAF RYES SAGA. OPTEGNELSER, DAGB\u00d8GER OG BREVE. DET NORDISKE FORLAG. Denne Bog s\u00f8ger at[...]\",\n  \"source\": \"ncc_books\",\n  \"added\": \"2025-05-08\",\n  \"created\": \"1899-01-01, 1899-12-31\",\n  \"token_count\": 192301\n}\n</code></pre>"},{"location":"data/ncc_books/ncc_books.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ncc_books/ncc_books.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ncc_books/ncc_books.html#additional-information","title":"Additional Information","text":""},{"location":"data/ncc_books/ncc_books.html#license-information","title":"License Information","text":"<p>This dataset is licensed under CC0 1.0.  This license is derived from the original publication, which is published by the  National Library of Norway.</p>"},{"location":"data/ncc_books/ncc_books.html#filtering","title":"Filtering","text":"<p>This subset is the result of the following filtering from all available data splits on the NCC:</p> <ul> <li>is_books: Documents, which are tagged as books</li> <li>language_filter: Document is classified as Danish with a threshold of 0.75</li> <li>min_length: Document has at least 10 words (whitespace separated strings + punctuation)</li> <li>alpha_ratio: The ratio of all words / words with only alphabetical characters is at least 0.7</li> <li>min_stop_words: The document contains at least 2 Danish stop words</li> <li>duplicate: Duplicate documents were removed</li> </ul> <p>The effect of each of these steps is outlined in the table below:</p> Filtering step Number of document is_books 20 939 language_filter 5 125 min_length 5 125 alpha_ratio 4 902 min_stop_words 4 902 duplicate 4 902"},{"location":"data/ncc_books/ncc_books.html#quality","title":"Quality","text":"<p>It is important to note, that recurring OCR errors and historic expressions in older  texts hinder the legibility of some of the documents and make differentiating between Norwegian and Danish difficult.</p>"},{"location":"data/ncc_books/ncc_books.html#citation-information","title":"Citation Information","text":"<p>If you use this source please cite the following articles:</p> <pre><code>@inproceedings{kummervold-etal-2022-norwegian-colossal,\n  title     = {The {N}orwegian colossal corpus: A text corpus for training large {N}orwegian language models},\n  author    = {Kummervold, Per E  and\n               Wetjen, Freddy    and\n               De la Rosa, Javier},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)},\n  year      = {2022},\n  address   = {Marseille, France},\n  publisher = {European Language Resources Association},\n  url       = {https://aclanthology.org/2022.lrec-1.410},\n  pages     = {3852--3860},\n  abstract  = {Norwegian has been one of many languages lacking sufficient available text to train quality language models. In an attempt to bridge this gap, we introduce the Norwegian Colossal Corpus (NCC), which comprises 49GB of clean Norwegian textual data containing over 7B words. The NCC is composed of different and varied sources, ranging from books and newspapers to government documents and public reports, showcasing the various uses of the Norwegian language in society. The corpus contains mainly Norwegian Bokm\u00e5l and Norwegian Nynorsk. Each document in the corpus is tagged with metadata that enables the creation of sub-corpora for specific needs. Its structure makes it easy to combine with large web archives that for licensing reasons could not be distributed together with the NCC. By releasing this corpus openly to the public, we hope to foster the creation of both better Norwegian language models and multilingual language models with support for Norwegian.},\n}\n\n@inproceedings{kummervold-etal-2021-operationalizing,\n  title     = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  author    = {Kummervold, Per E  and\n               De la Rosa, Javier  and\n               Wetjen, Freddy  and\n               Brygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  year      = {2021},\n  address   = {Reykjavik, Iceland (Online)},\n  publisher = {Link\u00f6ping University Electronic Press, Sweden},\n  url       = {https://aclanthology.org/2021.nodalida-main.3},\n  pages     = {20--29},\n  abstract  = {In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library.\n  The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models\n  in several token and sequence classification tasks for both Norwegian Bokm\u00e5l and Norwegian Nynorsk. Our model also improves the mBERT performance for other\n  languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore,\n  we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.},\n}\n</code></pre>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html","title":"Dataset Card for Norwegian Colossal Corpus (maalfrid)","text":"<p>Danish content from Norwegian institutions websites.</p> <p>Documents are derived from the M\u00e5lfrid collection as a subsection of the Norwegian Colossal Corpus, which is a collection of multiple smaller Norwegian corpuses suitable for training large language models.</p>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 33.34K</li> <li>Number of tokens (Llama 3): 29.26M</li> <li>Average document length in tokens (min, max): 877.7404907607391 (12, 5.11K)</li> </ul>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"maalfrid_56267641f4d6de44ab69875a31634e31e68db1a8_166\",\n  \"text\": \"Anno 1815, Torsdagen den 5te Octbr. blev i Sagen Fuldm\u00e6gtig Engebrethsen contra Snedkermester Hansen[...]\",\n  \"source\": \"ncc_maalfrid\",\n  \"added\": \"2025-05-08\",\n  \"created\": \"2021-01-01, 2021-12-31\",\n  \"token_count\": 742\n}\n</code></pre>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#additional-information","title":"Additional Information","text":""},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#license-information","title":"License Information","text":"<p>This dataset is licensed under NLOD 2.0.  This license is derived from the original publication, which is published by the  National Library of Norway.</p>"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#filtering","title":"Filtering","text":"<p>This subset is the result of the following filtering from all available data splits on the NCC:</p> <ul> <li>is_maalfrid: Documents, which are tagged as a part of the M\u00e5lfrid corpus</li> <li>language_filter: Document is classified as Danish with a threshold of 0.75</li> <li>min_length: Document has at least 10 words (whitespace separated strings + punctuation)</li> <li>alpha_ratio: The ratio of all words / words with only alphabetical characters is at least 0.7</li> <li>min_stop_words: The document contains at least 2 Danish stop words</li> <li>duplicate: Duplicate documents were removed</li> </ul> Filtering step Number of document is_maalfrid 4 719 569 language_filter 51523 min_length 49 948 alpha_ratio 33 390 min_stop_words 33 340 duplicate 33 336"},{"location":"data/ncc_maalfrid/ncc_maalfrid.html#citation-information","title":"Citation Information","text":"<p>If you use this source please cite the following articles:</p> <pre><code>@inproceedings{kummervold-etal-2022-norwegian-colossal,\n  title     = {The {N}orwegian colossal corpus: A text corpus for training large {N}orwegian language models},\n  author    = {Kummervold, Per E  and\n               Wetjen, Freddy    and\n               De la Rosa, Javier},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)},\n  year      = {2022},\n  address   = {Marseille, France},\n  publisher = {European Language Resources Association},\n  url       = {https://aclanthology.org/2022.lrec-1.410},\n  pages     = {3852--3860},\n  abstract  = {Norwegian has been one of many languages lacking sufficient available text to train quality language models. In an attempt to bridge this gap, we introduce the Norwegian Colossal Corpus (NCC), which comprises 49GB of clean Norwegian textual data containing over 7B words. The NCC is composed of different and varied sources, ranging from books and newspapers to government documents and public reports, showcasing the various uses of the Norwegian language in society. The corpus contains mainly Norwegian Bokm\u00e5l and Norwegian Nynorsk. Each document in the corpus is tagged with metadata that enables the creation of sub-corpora for specific needs. Its structure makes it easy to combine with large web archives that for licensing reasons could not be distributed together with the NCC. By releasing this corpus openly to the public, we hope to foster the creation of both better Norwegian language models and multilingual language models with support for Norwegian.},\n}\n\n@inproceedings{kummervold-etal-2021-operationalizing,\n  title     = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  author    = {Kummervold, Per E  and\n               De la Rosa, Javier  and\n               Wetjen, Freddy  and\n               Brygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  year      = {2021},\n  address   = {Reykjavik, Iceland (Online)},\n  publisher = {Link\u00f6ping University Electronic Press, Sweden},\n  url       = {https://aclanthology.org/2021.nodalida-main.3},\n  pages     = {20--29},\n  abstract  = {In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library.\n  The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models\n  in several token and sequence classification tasks for both Norwegian Bokm\u00e5l and Norwegian Nynorsk. Our model also improves the mBERT performance for other\n  languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore,\n  we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.},\n}\n</code></pre>"},{"location":"data/ncc_newspaper/ncc_newspaper.html","title":"Dataset Card for Norwegian Colossal Corpus (newspaper)","text":"<p>OCR'd Newspapers derived from NCC</p> <p>The Norwegian Colossal Corpus is a collection of multiple smaller Norwegian corpuses suitable for training large language models.</p>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 5.37K</li> <li>Number of tokens (Llama 3): 1.05M</li> <li>Average document length in tokens (min, max): 195.95942676344686 (12, 3.85K)</li> </ul>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"fylkestidendeforsognogfjordane_null_null_19410723_69_54_1_MODSMD_ARTICLE5\",\n  \"text\": \"STOCKHOLM: Det er kommet melding Ul den svenske turlst forenlng om at de to svenske ljellklatrerne s[...]\",\n  \"source\": \"ncc_newspaper\",\n  \"added\": \"2025-05-01\",\n  \"created\": \"1941-01-01, 1941-12-31\",\n  \"token_count\": 137\n}\n</code></pre>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ncc_newspaper/ncc_newspaper.html#additional-information","title":"Additional Information","text":""},{"location":"data/ncc_newspaper/ncc_newspaper.html#license-information","title":"License Information","text":"<p>This dataset is licensed under CC0 1.0. This license is derived from the original publication, which is published by the  National Library of Norway.</p>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#filtering","title":"Filtering","text":"<p>This subset is the result of the following filtering from all available data splits on the NCC:</p> <ul> <li>is_books: Documents, which are tagged as newspapers</li> <li>language_filter: Document is classified as Danish with a threshold of 0.75</li> <li>min_length: Document has at least 10 words (whitespace separated strings + punctuation)</li> <li>alpha_ratio: The ratio of all words / words with only alphabetical characters is at least 0.7</li> <li>min_stop_words: The document contains at least 2 Danish stop words</li> <li>duplicate: Duplicate documents were removed</li> </ul> <p>The effect of each of these steps is outlined in the table below:</p> Filtering step Number of document is_books 504 602 language_filter 7 632 min_length 6 401 alpha_ratio 5 439 min_stop_words 5 374 duplicate 5 373"},{"location":"data/ncc_newspaper/ncc_newspaper.html#quality","title":"Quality","text":"<p>It is important to note, that recurring OCR errors and historic expressions in older  texts hinder the legibility of some of the documents and make differentiating between Norwegian and Danish difficult.</p>"},{"location":"data/ncc_newspaper/ncc_newspaper.html#citation-information","title":"Citation Information","text":"<p>If you use this source please cite the following articles:</p> <pre><code>@inproceedings{kummervold-etal-2022-norwegian-colossal,\n  title     = {The {N}orwegian colossal corpus: A text corpus for training large {N}orwegian language models},\n  author    = {Kummervold, Per E  and\n               Wetjen, Freddy    and\n               De la Rosa, Javier},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)},\n  year      = {2022},\n  address   = {Marseille, France},\n  publisher = {European Language Resources Association},\n  url       = {https://aclanthology.org/2022.lrec-1.410},\n  pages     = {3852--3860},\n  abstract  = {Norwegian has been one of many languages lacking sufficient available text to train quality language models. In an attempt to bridge this gap, we introduce the Norwegian Colossal Corpus (NCC), which comprises 49GB of clean Norwegian textual data containing over 7B words. The NCC is composed of different and varied sources, ranging from books and newspapers to government documents and public reports, showcasing the various uses of the Norwegian language in society. The corpus contains mainly Norwegian Bokm\u00e5l and Norwegian Nynorsk. Each document in the corpus is tagged with metadata that enables the creation of sub-corpora for specific needs. Its structure makes it easy to combine with large web archives that for licensing reasons could not be distributed together with the NCC. By releasing this corpus openly to the public, we hope to foster the creation of both better Norwegian language models and multilingual language models with support for Norwegian.},\n}\n\n@inproceedings{kummervold-etal-2021-operationalizing,\n  title     = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  author    = {Kummervold, Per E  and\n               De la Rosa, Javier  and\n               Wetjen, Freddy  and\n               Brygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  year      = {2021},\n  address   = {Reykjavik, Iceland (Online)},\n  publisher = {Link\u00f6ping University Electronic Press, Sweden},\n  url       = {https://aclanthology.org/2021.nodalida-main.3},\n  pages     = {20--29},\n  abstract  = {In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library.\n  The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models\n  in several token and sequence classification tasks for both Norwegian Bokm\u00e5l and Norwegian Nynorsk. Our model also improves the mBERT performance for other\n  languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore,\n  we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.},\n}\n</code></pre>"},{"location":"data/ncc_parliament/ncc_parliament.html","title":"Dataset Card for Norwegian Colossal Corpus (parliament)","text":"<p>Collections from the Norwegian parliament in Danish. Extracted from the Norwegian Colossal Corpus derived from ocr.</p> <p>The Norwegian Colossal Corpus is a collection of multiple smaller Norwegian corpuses suitable for training large language models.</p>"},{"location":"data/ncc_parliament/ncc_parliament.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.08K</li> <li>Number of tokens (Llama 3): 338.87M</li> <li>Average document length in tokens (min, max): 314.64K (129, 373.59K)</li> </ul>"},{"location":"data/ncc_parliament/ncc_parliament.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"digistorting_1906-07_part6_vol-a_part2\",\n  \"text\": \"Liknes \u2014Aaroot i foranstaaende oversigt over omkostningerne er beregnet til kr. 37 500,00 under foru[...]\",\n  \"source\": \"ncc_parliament\",\n  \"added\": \"2025-05-08\",\n  \"created\": \"2021-01-01, 2021-12-31\",\n  \"token_count\": 360308\n}\n</code></pre>"},{"location":"data/ncc_parliament/ncc_parliament.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ncc_parliament/ncc_parliament.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ncc_parliament/ncc_parliament.html#additional-information","title":"Additional Information","text":""},{"location":"data/ncc_parliament/ncc_parliament.html#license-information","title":"License Information","text":"<p>This dataset is licensed under NLOD 2.0.  This license is derived from the original publication, which is published by the  National Library of Norway.</p>"},{"location":"data/ncc_parliament/ncc_parliament.html#filtering","title":"Filtering","text":"<p>This subset is the result of the following filtering from all available data splits on the NCC:</p> <ul> <li>is_parliament: Documents, which are tagged as parliament data</li> <li>language_filter: Document is classified as Danish with a threshold of 0.75</li> <li>min_length: Document has at least 10 words (whitespace separated strings + punctuation)</li> <li>alpha_ratio: The ratio of all words / words with only alphabetical characters is at least 0.7</li> <li>min_stop_words: The document contains at least 2 Danish stop words</li> <li>duplicate: Duplicate documents were removed</li> </ul> <p>The effect of each of these steps is outlined in the table below:</p> Filtering step Number of document is_parliament 9 528 language_filter 1 275 min_length 1 275 alpha_ratio 1 077 min_stop_words 1 077 duplicate 1 077 <p>Note that a total of 976 long texts (&gt;~1e5 tokens) were found.</p>"},{"location":"data/ncc_parliament/ncc_parliament.html#quality","title":"Quality","text":"<p>It is important to note, that recurring OCR errors and historic expressions in older  texts hinder the legibility of some of the documents and make differentiating between Norwegian and Danish difficult.</p>"},{"location":"data/ncc_parliament/ncc_parliament.html#citation-information","title":"Citation Information","text":"<p>If you use this source please cite the following articles:</p> <pre><code>@inproceedings{kummervold-etal-2022-norwegian-colossal,\n  title     = {The {N}orwegian colossal corpus: A text corpus for training large {N}orwegian language models},\n  author    = {Kummervold, Per E  and\n               Wetjen, Freddy    and\n               De la Rosa, Javier},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)},\n  year      = {2022},\n  address   = {Marseille, France},\n  publisher = {European Language Resources Association},\n  url       = {https://aclanthology.org/2022.lrec-1.410},\n  pages     = {3852--3860},\n  abstract  = {Norwegian has been one of many languages lacking sufficient available text to train quality language models. In an attempt to bridge this gap, we introduce the Norwegian Colossal Corpus (NCC), which comprises 49GB of clean Norwegian textual data containing over 7B words. The NCC is composed of different and varied sources, ranging from books and newspapers to government documents and public reports, showcasing the various uses of the Norwegian language in society. The corpus contains mainly Norwegian Bokm\u00e5l and Norwegian Nynorsk. Each document in the corpus is tagged with metadata that enables the creation of sub-corpora for specific needs. Its structure makes it easy to combine with large web archives that for licensing reasons could not be distributed together with the NCC. By releasing this corpus openly to the public, we hope to foster the creation of both better Norwegian language models and multilingual language models with support for Norwegian.},\n}\n\n@inproceedings{kummervold-etal-2021-operationalizing,\n  title     = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  author    = {Kummervold, Per E  and\n               De la Rosa, Javier  and\n               Wetjen, Freddy  and\n               Brygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  year      = {2021},\n  address   = {Reykjavik, Iceland (Online)},\n  publisher = {Link\u00f6ping University Electronic Press, Sweden},\n  url       = {https://aclanthology.org/2021.nodalida-main.3},\n  pages     = {20--29},\n  abstract  = {In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library.\n  The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models\n  in several token and sequence classification tasks for both Norwegian Bokm\u00e5l and Norwegian Nynorsk. Our model also improves the mBERT performance for other\n  languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore,\n  we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.},\n}\n</code></pre>"},{"location":"data/news_filtered/news_filtered.html","title":"Dataset Card for News","text":"<p>A set of news stories, scraped from opennewswire.</p> <p>We scrape the news sites that publish content under CC BY or CC BY-SA according to opennewswire. These include 360info, Africa is a Country, Alt News, Balkan Diskurs, Factly, Freedom of the Press Foundation, Agenzia Fides, Global Voices, Meduza, Mekong Eye, Milwaukee Neighborhood News Service, Minority Africa, New Canadian Media, SciDev.Net, The Solutions Journalism Exchange, Tasnim News Agency, ZimFact, Oxpeckers, Propastop, and The Public Record. Plain text was extracted from the HTML using a custom pipeline, including extraction of the title and byline to include at the beginning of each article. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/news_filtered/news_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 126.67K</li> <li>Number of tokens (Llama 3): 53.77M</li> <li>Average document length in tokens (min, max): 424.46852131077657 (20, 14.88K)</li> </ul>"},{"location":"data/news_filtered/news_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/news_filtered/news_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/news_filtered/news_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/news_filtered/news_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/news_filtered/news_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/news_filtered/news_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/nordjyllandnews/nordjyllandnews.html","title":"Dataset Card for Nordjylland News","text":"<p>Articles from the Danish Newspaper TV2 Nord.</p> <p>The data is derived from the Huggingface dataset alexandrainst/nordjylland-news-summarization originally intended for text summarization.</p>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 75.22K</li> <li>Number of tokens (Llama 3): 37.90M</li> <li>Average document length in tokens (min, max): 503.9497440670079 (29, 12.26K)</li> </ul>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"nordjyllandnews_0\",\n  \"text\": \"Lav et referat af nedenst\u00e5ende tekst:\\n\\nTekst:\\nOpdatering: Manden er nu fundet af Nordjyllands Politi[...]\",\n  \"source\": \"nordjyllandnews\",\n  \"added\": \"2024-12-16\",\n  \"created\": \"2000-01-01, 2024-01-01\",\n  \"token_count\": 628\n}\n</code></pre>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/nordjyllandnews/nordjyllandnews.html#additional-information","title":"Additional Information","text":""},{"location":"data/nordjyllandnews/nordjyllandnews.html#opportunities-for-improvement","title":"Opportunities for Improvement","text":"<p>An updated version of the this data could be fetched from their API.</p>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#sourced-data","title":"Sourced data","text":"<p>This dataset is derived from <code>alexandrainst/nordjylland-news-summarization</code></p>"},{"location":"data/nordjyllandnews/nordjyllandnews.html#citation-information","title":"Citation Information","text":"<p>No citation is applicable for this work. We recommend citing the huggingface repository.</p>"},{"location":"data/nota/nota.html","title":"Dataset Card for Nota lyd- og tekstdata (Tekst only)","text":"<p>The text only part of the Nota lyd- og tekstdata dataset. </p> <p>Nota lyd- og tekstdata (Tekst only) is a readaloud dataset consisting of few very long texts.</p>"},{"location":"data/nota/nota.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 446</li> <li>Number of tokens (Llama 3): 7.30M</li> <li>Average document length in tokens (min, max): 16.37K (4.48K, 107.26K)</li> </ul>"},{"location":"data/nota/nota.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"INSL20160004\",\n  \"text\": \"Inspiration nr. 4, 2016\\nBiblioteksbetjening \\nTelefon: 39 13 46 00\\nEmail: biblioteket@nota.dk\\nInspira[...]\",\n  \"source\": \"nota\",\n  \"added\": \"2025-02-03\",\n  \"created\": \"2016-01-01, 2016-12-31\",\n  \"token_count\": 69977\n}\n</code></pre>"},{"location":"data/nota/nota.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/nota/nota.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/nota/nota.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/nota/nota.html#additional-information","title":"Additional Information","text":""},{"location":"data/nota/nota.html#citation-information","title":"Citation Information","text":""},{"location":"data/oercommons_filtered/oercommons_filtered.html","title":"Dataset Card for OERCommons","text":"<p>OERCommons is an online platform where educators share open-access instructional materials\u2014such as textbooks, lesson plans, problem sets, course syllabi, and worksheets\u2014with the goal of expanding access to affordable education. </p> <p>OERCommons is an online platform where educators share open-access instructional materials\u2014such as textbooks, lesson plans, problem sets, course syllabi, and worksheets\u2014with the goal of expanding access to affordable education. To collect the openly licensed content available on OERCommons, we construct a search query to retrieve English-language content released into the public domain or under CC BY or CC BY-SA licenses. The resulting documents are converted to plain text directly from the HTML pages hosted on the OERCommons website. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/oercommons_filtered/oercommons_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 5.24K</li> <li>Number of tokens (Llama 3): 10.82M</li> <li>Average document length in tokens (min, max): 2.06K (48, 293.85K)</li> </ul>"},{"location":"data/oercommons_filtered/oercommons_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/oercommons_filtered/oercommons_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/oercommons_filtered/oercommons_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/oercommons_filtered/oercommons_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/oercommons_filtered/oercommons_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/oercommons_filtered/oercommons_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/opensubtitles/opensubtitles.html","title":"Dataset Card for OpenSubtitles","text":"<p>Danish subsection of OpenSubtitles.</p>"},{"location":"data/opensubtitles/opensubtitles.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 29.82K</li> <li>Number of tokens (Llama 3): 271.60M</li> <li>Average document length in tokens (min, max): 9.11K (45, 70.14K)</li> </ul>"},{"location":"data/opensubtitles/opensubtitles.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"opensub_6822913\",\n  \"text\": \"Tidligere i vikingerne...\\nJeg skal g\u00e5 tilbage til England.\\nBurde v\u00e6re g\u00e5et tilbage for lang tid side[...]\",\n  \"source\": \"opensubtitles\",\n  \"added\": \"2025-01-02\",\n  \"created\": \"1920-01-01, 2018-01-01\",\n  \"token_count\": 3559\n}\n</code></pre>"},{"location":"data/opensubtitles/opensubtitles.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/opensubtitles/opensubtitles.html#additional-processing","title":"Additional Processing","text":"<p>Due to copyright concern additional documents have been removed due to copyright concerns. These include:</p> <pre><code>{\n    # Der kommer en dag\n    \"opensub_6726481\",\n    \"opensub_6732371\",\n    # Kollektivet\n    \"opensub_6645818\",\n    # Flaskepost fra P\n    \"opensub_6666922\",\n    \"opensub_6720216\",\n    \"opensub_6958711\",\n    # Fasandr\u00e6berne\n    \"opensub_6036947\",\n    \"opensub_6008622\",\n    # En du elsker\n    \"opensub_5828376\",\n    \"opensub_5828378\",\n    # En chance til\n    \"opensub_6177523\",\n    # Lev st\u00e6rkt\n    \"opensub_6467655\",\n    # Nymphomaniac\n    \"opensub_5604391\",\n    \"opensub_5748340\",\n    \"opensub_5748494\",\n    \"opensub_5629516\",\n    # Kvinden i buret\n    \"opensub_5636248\",\n    \"opensub_5514603\",\n    \"opensub_5504932\",\n    # Den skaldede fris\u00f8r\n    \"opensub_5084880\",\n    \"opensub_5031826\",\n    # Jagten\n    \"opensub_6929419\",\n    \"opensub_4885548\",\n    # Melancholia\n    \"opensub_4421330\",\n    \"opensub_4406991\",\n    \"opensub_4418817\",\n    # Ambassad\u00f8ren\n    \"opensub_4557721\",\n    # Antichrist\n    \"opensub_5511502\",\n    \"opensub_3938655\",\n    \"opensub_3636940\",\n    \"opensub_3564521\",\n    \"opensub_3562215\",\n    # En kongelig aff\u00e6re\n    \"opensub_4725493\",\n    \"opensub_4725160\",\n    \"opensub_4725159\",\n    \"opensub_4916871\",\n    \"opensub_5186746\",\n    # Br\u00f8dre\n    \"opensub_233943\",\n    \"opensub_87475\",\n}\n</code></pre> <p>We have additionally removed duplicate entries from the original dataset.</p>"},{"location":"data/opensubtitles/opensubtitles.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/opensubtitles/opensubtitles.html#additional-information","title":"Additional Information","text":""},{"location":"data/opensubtitles/opensubtitles.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/peS2o_filtered/peS2o_filtered.html","title":"Dataset Card for PeS2o","text":"<p>A set of openly licensed scientific articles. </p> <p>This dataset is a version of the peS2o dataset restricted to openly licensed articles. PeS2o is derived from S2ORC, a corpus of openly licensed abstract and full-text papers that have been converted to a structured format using Grobid. Starting from Grobid\u2019s XML output, peS2o filters papers that are too short, have incorrect metadata, are in languages other than English, and contain OCR errors using a combination of heuristic- and model-based filtering steps. Please refer to the peS2o datasheet and code for more details on the peS2o processing pipeline. For the openly licensed articles in this collection, per-document license information is available in the license entry of the metadata field of each example.</p>"},{"location":"data/peS2o_filtered/peS2o_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 6.11M</li> <li>Number of tokens (Llama 3): 39.51B</li> <li>Average document length in tokens (min, max): 6.47K (109, 663.21K)</li> </ul>"},{"location":"data/peS2o_filtered/peS2o_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/peS2o_filtered/peS2o_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/peS2o_filtered/peS2o_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/peS2o_filtered/peS2o_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/peS2o_filtered/peS2o_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/peS2o_filtered/peS2o_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/plandata/plandata.html","title":"Dataset Card for Plan- og Landdistriktsstyrelsen - Plandata.dk","text":"<p>A comprehensive dataset consisting of municipal planning documents from across Denmark, including local development plans, municipal plans, planning strategies, and related document types.</p> <p>This dataset provides access to approximately 114,000 documents pertaining to physical and strategic planning within Danish municipalities. All documents were retrieved from https://kort.plandata.dk. The collection includes key planning instruments such as local development plans (lokalplaner), municipal master plans (kommuneplaner), municipal planning strategies (kommuneplanstrategier), and other related documents. Collectively, these define the regulatory framework for land use, urban development, infrastructure, nature conservation, and the allocation of business and residential areas.</p> <p>Authored and officially adopted by municipal authorities, these documents often possess significant legal weight, making them invaluable for legal, administrative, and urban planning analyses.</p> <p>Kommuneplaner (municipal plans) serve as comprehensive, long-term strategic blueprints for an entire municipality. Spanning multiple years, they establish overarching goals for land use, infrastructure, housing, economic development, and environmental protection, ensuring alignment with both national and regional policies while addressing specific local priorities.</p> <p>Lokalplaner (local plans) offer a more granular perspective, focusing on specific neighborhoods, districts, or individual development projects within a municipality. These plans typically detail zoning regulations, building standards, traffic management solutions, and precise land use specifications for smaller geographical areas.</p>"},{"location":"data/plandata/plandata.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 101.19K</li> <li>Number of tokens (Llama 3): 1.03B</li> <li>Average document length in tokens (min, max): 10.20K (6, 13.35M)</li> </ul>"},{"location":"data/plandata/plandata.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/plandata/plandata.html#additional-processing","title":"Additional Processing","text":"<p>The data have been pulled from https://kort.plandata.dk. </p> <p>Each document have been extracted from different file types (docx, pdf, etc.) but primarily PDF.</p> <p>For text extraction of PDF we have used the marker tool. </p>"},{"location":"data/plandata/plandata.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/plandata/plandata.html#additional-information","title":"Additional Information","text":""},{"location":"data/plandata/plandata.html#license-information","title":"License Information","text":"<p>The license for this is a written agreement with \"Plan- og Landdistriktsstyrelsen\" that we can use the data for training.</p> <p>Plan- og Landdistriktsstyrelsen driver Plandata.dk mens det er kommunerne som planl\u00e6gningsmyndighed, der udarbejder planerne.</p> <p>Planerne er offentligt tilg\u00e6ngelige, s\u00e5 dem henter I bare.</p>"},{"location":"data/plandata/plandata.html#citation-information","title":"Citation Information","text":"<p>No citation available.</p>"},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html","title":"Dataset Card for Pre-1929 Books","text":"<p>A set of books published in the US pre-1929.</p> <p>Books published in the US before 1929 passed into the public domain on January 1, 2024. We used the bibliographic catalog Hathifiles produced by HathiTrust to identify digitized books which were published in the US before 1929. The collection contains over 130,000 books digitized and processed by the Internet Archive on behalf of HathiTrust member libraries. The OCR plain text files were downloaded directly from the Internet Archive website.</p>"},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 124.55K</li> <li>Number of tokens (Llama 3): 10.56B</li> <li>Average document length in tokens (min, max): 84.82K (19, 5.78M)</li> </ul>"},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/pre_1929_books_filtered/pre_1929_books_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/pressbooks_filtered/pressbooks_filtered.html","title":"Dataset Card for PressBooks","text":"<p>A set of openly licensed books.</p> <p>PressBooks is a searchable catalog of over 8,000 open access books. To collect openly licensed content from PressBooks we construct a search query to retrieve URLs for all books written in English and listed as public domain or under CC BY or CC BY-SA licenses. For each matched book, we collect its contents directly from the publicly available web version provided by PressBooks. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 54.43K</li> <li>Number of tokens (Llama 3): 125.65M</li> <li>Average document length in tokens (min, max): 2.31K (19, 425.98K)</li> </ul>"},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/pressbooks_filtered/pressbooks_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html","title":"Dataset Card for Project Gutenberg","text":"<p>All books from project gutenberg marked as english and public domain.</p> <p>Project Gutenberg is an online collection of over 75,000 digitized books available as plain text. We use all books that are 1) English and 2) marked as in the Public Domain according to the provided metadata. Additionally, we include any books that are part of the PG19 dataset, which only includes books that are over 100 years old. Minimal preprocessing is applied to remove the Project Gutenberg header and footers, but many scanned books include preamble information about who digitized them.</p>"},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 55.45K</li> <li>Number of tokens (Llama 3): 4.91B</li> <li>Average document length in tokens (min, max): 88.51K (18, 8.68M)</li> </ul>"},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/project_gutenberg_filtered/project_gutenberg_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre> <pre><code>@article{raecompressive2019,\n  author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and\n            Hillier, Chloe and Lillicrap, Timothy P},\n  title = {Compressive Transformers for Long-Range Sequence Modelling},\n  journal = {arXiv preprint},\n  url = {https://arxiv.org/abs/1911.05507},\n  year = {2019},\n}\n</code></pre></p>"},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html","title":"Dataset Card for Public Domain Review","text":"<p>A set of articles describing works of art that is part of public domain.</p> <p>The Public Domain Review is an online journal dedicated to exploration of works of art and literature that have aged into the public domain. We collect all articles published in the Public Domain Review under a CC BY-SA license.</p>"},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.41K</li> <li>Number of tokens (Llama 3): 1.51M</li> <li>Average document length in tokens (min, max): 1.07K (35, 10.80K)</li> </ul>"},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/public_domain_review_filtered/public_domain_review_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/pubmed_filtered/pubmed_filtered.html","title":"Dataset Card for PubMed","text":"<p>A set of permissively licensed papers collected from tthe PubMed Central.</p> <p>PubMed Central is an open-access archive of biomedical and life sciences research papers maintained by the U.S. National Institutes of Health\u2019s National Library of Medicine. We collected papers from PubMed whose metadata indicated that the publishing journal had designated a CC BY, CC BY-SA, or CC0 license. PubMed stores the text content of each article as a single nXML file, which we convert to markdown using pandoc. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/pubmed_filtered/pubmed_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 3.79M</li> <li>Number of tokens (Llama 3): 35.32B</li> <li>Average document length in tokens (min, max): 9.33K (6, 1.13M)</li> </ul>"},{"location":"data/pubmed_filtered/pubmed_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/pubmed_filtered/pubmed_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/pubmed_filtered/pubmed_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/pubmed_filtered/pubmed_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/pubmed_filtered/pubmed_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/pubmed_filtered/pubmed_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html","title":"Dataset Card for Python Enhancement Proposals (PEP)","text":"<p>This set consists of almost all PEPs created.</p> <p>Python Enhancement Proposals, or PEPs, are design documents that generally provide a technical specification and rationale for new features of the Python programming language. There have been 661 PEPs published. The majority of PEPs are published in the Public Domain, but 5 were published under the \u201cOpen Publication License\u201d and omitted from this dataset. PEPs are long, highly-polished, and technical in nature and often include code examples paired with their prose. PEPs are authored in ReStructured Text; we used pandoc to convert them to plain text.</p>"},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 655</li> <li>Number of tokens (Llama 3): 2.54M</li> <li>Average document length in tokens (min, max): 3.87K (49, 19.78K)</li> </ul>"},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/python_enhancement_proposals_filtered/python_enhancement_proposals_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/regulations_filtered/regulations_filtered.html","title":"Dataset Card for Regulations.Gov","text":"<p>This dataset includes all plain-text regulatory documents published by a variety of U.S. federal agencies on Regulations.Gov</p> <p>Regulations.gov is an online platform operated by the U.S. General Services Administration that collates newly proposed rules and regulations from federal agencies along with comments and feedback from the general public. This dataset includes all plain-text regulatory documents published by a variety of U.S. federal agencies on this platform, acquired via the bulk download interface provided by Regulations.gov. These agencies include the Bureau of Industry and Security (BIS), Department of Transportation (DOT), Environmental Protection Agency (EPA), Federal Aviation Administration (FAA), Food and Drug Administration (FDA), Federal Emergency Management Agency (FEMA), Federal Energy Regulatory Commission (FERC), Federal Motor Carrier Safety Administration (FMCSA), Federal Railroad Administration (FRA), National Highway Traffic Safety Administration (NHTSA), Occupational Safety and Health Administration (OSHA), Pipeline and Hazardous Materials Safety Administration (PHMSA), Securities and Exchange Commission (SEC), and United States Coast Guard (USCG).</p>"},{"location":"data/regulations_filtered/regulations_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 192.44K</li> <li>Number of tokens (Llama 3): 1.28B</li> <li>Average document length in tokens (min, max): 6.65K (19, 51.77M)</li> </ul>"},{"location":"data/regulations_filtered/regulations_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/regulations_filtered/regulations_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/regulations_filtered/regulations_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/regulations_filtered/regulations_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/regulations_filtered/regulations_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/regulations_filtered/regulations_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/relig/relig.html","title":"Dataset Card for Religious texts","text":"<p>Danish religious text from the 1700-2022.</p> <p>This section contains a Danish translation of the Bible from the Massively Parallel Bible corpus (Christodouloupoulos and Steedman, 2015) without any pre-processing other than file format conversion.</p>"},{"location":"data/relig/relig.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 66</li> <li>Number of tokens (Llama 3): 1.24M</li> <li>Average document length in tokens (min, max): 18.85K (473, 66.42K)</li> </ul>"},{"location":"data/relig/relig.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"relig_SON\",\n  \"text\": \"Salomos H\u00f8jsang\\nKys mig, giv mig Kys af din mund thi din K\u00e6rlighed er bedre end Vin.\\nLifligt dufter [...]\",\n  \"source\": \"relig\",\n  \"added\": \"2020-09-14\",\n  \"created\": \"1700-01-01, 2022-01-01\",\n  \"token_count\": 4099\n}\n</code></pre>"},{"location":"data/relig/relig.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/relig/relig.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/relig/relig.html#additional-information","title":"Additional Information","text":""},{"location":"data/relig/relig.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/retsinformationdk/retsinformationdk.html","title":"Dataset Card for retsinformation.dk (Danish legal information)","text":"<p>retsinformation.dk (legal-information.dk) the official legal information system of Denmark. </p> <p>It serves as a central repository for Danish legislation, administrative regulations, and other legally binding documents. The platform ensures transparency and public access to laws and legal materials. The sites includes:</p> <ol> <li>Legislation: Danish laws, acts, and statutes passed by the Parliament (Folketinget).</li> <li>Administrative Regulations: Rules, guidelines, and executive orders issued by governmental authorities.</li> <li>Historical Versions: Archived versions of laws and regulations, useful for legal research or historical reference.</li> <li>Preparatory Works (Forarbejder): Documents explaining the background and intent behind legislative acts, such as proposals and committee reports.</li> <li>Case Law References: Links to decisions and interpretations that relate to specific legislation.</li> </ol>"},{"location":"data/retsinformationdk/retsinformationdk.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 100.52K</li> <li>Number of tokens (Llama 3): 818.25M</li> <li>Average document length in tokens (min, max): 8.14K (34, 9.59M)</li> </ul>"},{"location":"data/retsinformationdk/retsinformationdk.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"AA014851\",\n  \"text\": \"Indsamlingsn\u00e6vnets afg\u00f8relse i sag nr. 22-730-00015\\n\\nIndsamlingsn\u00e6vnet fandt det kritisabelt, at Gad[...]\",\n  \"source\": \"retsinformationdk\",\n  \"added\": \"2025-06-26\",\n  \"created\": \"2025-06-25, 2025-06-25\",\n  \"token_count\": 4062\n}\n</code></pre>"},{"location":"data/retsinformationdk/retsinformationdk.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/retsinformationdk/retsinformationdk.html#license-information","title":"License Information","text":"Danish Copyright Law <p> Danish Copyright law at https://www.retsinformation.dk/forms/r0710.aspx?id=164796 states    \u00a7 9. Love, administrative forskrifter, retsafg\u00f8relser og lignende offentlige aktstykker er ikke genstand for ophavsret.  Stk. 2. Bestemmelsen i stk. 1 g\u00e6lder ikke for v\u00e6rker, der fremtr\u00e6der som selvst\u00e6ndige bidrag i de i stk. 1 n\u00e6vnte aktstykker. S\u00e5danne v\u00e6rker m\u00e5 dog gengives i forbindelse med aktstykket. Retten til videre udnyttelse afh\u00e6nger af de i \u00f8vrigt g\u00e6ldende regler.  </p>"},{"location":"data/retsinformationdk/retsinformationdk.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/retsinformationdk/retsinformationdk.html#additional-information","title":"Additional Information","text":""},{"location":"data/retsinformationdk/retsinformationdk.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/retspraksis/retspraksis.html","title":"Dataset Card for retspraksis","text":"<p>Case law or judical practice in Denmark derived from Retspraksis.</p> <p>It encompasses the body of legal decisions made by Danish courts, which play a significant role in interpreting and applying the law.</p>"},{"location":"data/retspraksis/retspraksis.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 4.36K</li> <li>Number of tokens (Llama 3): 56.26M</li> <li>Average document length in tokens (min, max): 12.90K (298, 979.66K)</li> </ul>"},{"location":"data/retspraksis/retspraksis.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"retspraksis_517\",\n  \"text\": \"                             h\u00f8jesterets dom\\n                        afsagt torsdag den 6. december [...]\",\n  \"source\": \"retspraksis\",\n  \"added\": \"2020-09-24\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 10838\n}\n</code></pre>"},{"location":"data/retspraksis/retspraksis.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/retspraksis/retspraksis.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/retspraksis/retspraksis.html#additional-information","title":"Additional Information","text":""},{"location":"data/retspraksis/retspraksis.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/skat/skat.html","title":"Dataset Card for skat.dk","text":"<p>Skat is the Danish tax authority. This dataset contains content from its website skat.dk.</p>"},{"location":"data/skat/skat.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 14.71K</li> <li>Number of tokens (Llama 3): 122.11M</li> <li>Average document length in tokens (min, max): 8.30K (2, 175.22K)</li> </ul>"},{"location":"data/skat/skat.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"skat_SKM2010.712.SKAT\",\n  \"text\": \"Andelsboligforeningers levering af brugsrettigheder til andelshaverne mod betaling af boligafgift si[...]\",\n  \"source\": \"skat\",\n  \"added\": \"2020-10-01\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 1717\n}\n</code></pre>"},{"location":"data/skat/skat.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/skat/skat.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/skat/skat.html#additional-information","title":"Additional Information","text":""},{"location":"data/skat/skat.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/spont/spont.html","title":"Dataset Card for Spontaneous speech","text":"<p>Conversational samples collected as a part of research projects at Aarhus University.</p> <p>The conversational corpus included originates from interdisciplinary research conducted within the Interacting Minds Centre,  and the Puzzle of Danish project at Aarhus University. Transcribed Danish speech is generally a rare kind of data,  and spontaneous speech especially so; these manually transcribed conversations thus form a valuable resource. Spontaneous and pseudo-spontaneous conversations  come from various contexts, e.g., getting to know each other, solving a puzzle together, or making joint decisions. The participants have agreed on  releasing anonymized transcripts of their conversations. All conversations involve two speakers, sometimes conversing face-to-face, sometimes via a chat tool.  Speech is transcribed post-hoc by native speakers. Studies published relying on this data include  Fusaroli et al. (2012),  Dideriksen et al. (2019), and  Tyl\u00e9n et al. (2016).</p>"},{"location":"data/spont/spont.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 411</li> <li>Number of tokens (Llama 3): 1.56M</li> <li>Average document length in tokens (min, max): 3.79K (85, 14.03K)</li> </ul>"},{"location":"data/spont/spont.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"spont_PuzzleOfDanish132\",\n  \"text\": \"Taler 6: mm\\nTaler 7: er du klar?\\nTaler 6: ja\\nTaler 7: s\u00e5 er sp\u00f8rgsm\u00e5let om vi skal- om det er s\u00e5dan [...]\",\n  \"source\": \"spont\",\n  \"added\": \"2020-01-21\",\n  \"created\": \"2019-01-01, 2020-01-01\",\n  \"token_count\": 3902\n}\n</code></pre>"},{"location":"data/spont/spont.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/spont/spont.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/spont/spont.html#formatting","title":"Formatting","text":"<p>To represent speakers in the text files, prefix each turn with \u201cTALER 1:\u201d (substituting whatever ID is appropriate).  Note: there is no space before the colon; use one space after the colon. Speaker IDs should be consistent throughout all documents in a section. Speaker IDs need only be unique within a section, not universally.</p>"},{"location":"data/spont/spont.html#additional-information","title":"Additional Information","text":""},{"location":"data/spont/spont.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/stackexchange_filtered/stackexchange_filtered.html","title":"Dataset Card for Stack Exchange","text":"<p>StackExchange is a collection of Q&amp;A communities spanning a wide variety of topics.</p> <p>While StackExchange formerly provided structured dumps of all of their content, since July of 2024, StackExchange has stopped publishing XML dumps to the Internet Archive. Instead, each site can provide a logged-in user with a custom URL to download the dump for that site. This means that dumps for defunct sites like windowsphone.stackexchange.com are inaccessible. Additionally, in dumps produced by the new export tool, many questions that are available in past dumps (and accessible on the site) are not present. For this reason, we extract all questions and answers from community-uploaded dumps from December of 2024 from the Internet Archive and additionally extract missing questions and answers from the last official dumps in July of 2024 to account for the deficiencies listed above. We use a question, its comments, its answers, and the comments on each answer as a single document. Following the display order on StackExchange, answers are ordered by the number of votes they received, with the exception that the \u201caccepted answer\u201d always appears first. PyMarkdown was used to convert each comment into plain text. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 30.99M</li> <li>Number of tokens (Llama 3): 21.83B</li> <li>Average document length in tokens (min, max): 704.5059687867861 (11, 2.69M)</li> </ul>"},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/stackexchange_filtered/stackexchange_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html","title":"Dataset Card for Stack V2 Edu","text":"<p>Stack V2 Edu is a dataset containing files in various programming and markup languages from openly licensed projects.</p> <p>We filter the Stack V2 to only include code from openly licensed repositories, based on the license detection performed by the creators of Stack V2. When multiple licenses are detected in a single repository, we ensure that all of the licenses are on the Blue Oak Council certified license list. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 67.79M</li> <li>Number of tokens (Llama 3): 62.39B</li> <li>Average document length in tokens (min, max): 920.2978463866087 (1, 6.70M)</li> </ul>"},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/stackv2_edu_filtered/stackv2_edu_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/synne/synne.html","title":"Dataset Card for synnejysk Forening","text":"<p>Dataset collected from synnejysk forening's website, covering the Danish dialect s\u00f8nderjysk.</p>"},{"location":"data/synne/synne.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 177</li> <li>Number of tokens (Llama 3): 52.02K</li> <li>Average document length in tokens (min, max): 293.8813559322034 (128, 891)</li> </ul>"},{"location":"data/synne/synne.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"synne_forening_0140\",\n  \"text\": \"Mange\u00e6gskage Hent printvenligt dokument her \u2013 Klik her Som medlem af \u00c6 Synnejysk Forening er du med [...]\",\n  \"source\": \"synne\",\n  \"added\": \"2020-06-26\",\n  \"created\": \"2000-01-01, 2022-01-01\",\n  \"token_count\": 144\n}\n</code></pre>"},{"location":"data/synne/synne.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/synne/synne.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/synne/synne.html#additional-information","title":"Additional Information","text":""},{"location":"data/synne/synne.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/tv2r/tv2r.html","title":"Dataset Card for TV 2 Radio","text":""},{"location":"data/tv2r/tv2r.html#dataset-description","title":"Dataset Description","text":"<p>Contemporary Danish newswire articles published between 2010 and 2019. </p> <p>It contains articles of regional interest, written following editorial standards. This section\u2019s value is in both its temporal variation, covering a decade of events, and its spatial variation, covering many local events across most of Denmark (TV2 Bornholm is excluded). As a result of local event coverage, the section contains many locally relevant named entities, which might otherwise not be present in a dataset of national news.</p> <ul> <li>Number of samples: 49.13K</li> <li>Number of tokens (Llama 3): 21.67M</li> <li>Average document length in tokens (min, max): 441.055562339724 (16, 5.27K)</li> </ul>"},{"location":"data/tv2r/tv2r.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"tv2r_92548\",\n  \"text\": \"Storken er landet\\n02 april 2017 kl. 17.58\\nS\u00f8ndag aften er storken Annika landet i sin rede ved Smeda[...]\",\n  \"source\": \"tv2r\",\n  \"added\": \"2019-11-13\",\n  \"created\": \"2015-01-01, 2020-01-01\",\n  \"token_count\": 465\n}\n</code></pre>"},{"location":"data/tv2r/tv2r.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/tv2r/tv2r.html#license-information","title":"License Information","text":"CC-BY-SA 4.0 <p> The owner of this content is TV2 Regionerne, Denmark. Creative Commons Attribution 4.0 International </p>"},{"location":"data/tv2r/tv2r.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/tv2r/tv2r.html#additional-information","title":"Additional Information","text":""},{"location":"data/tv2r/tv2r.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html","title":"Dataset Card for Ubuntu IRC","text":"<p>Ubuntu-hosted Internet Relay Chat (IRC) is an online chat service.</p> <p>Logs of all discussions on the Ubuntu-hosted Internet Relay Chat (IRC) since 2004 have been archived and released into the Public Domain. We downloaded all chats from all channels up until March of 2025. We consider all messages for a given channel on a given day as a single document. We removed system messages as well as those from known bots.</p>"},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 216.20K</li> <li>Number of tokens (Llama 3): 1.76B</li> <li>Average document length in tokens (min, max): 8.14K (1, 1.37M)</li> </ul>"},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/ubuntu_irc_filtered/ubuntu_irc_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html","title":"Dataset Card for UK Hansard","text":"<p>Hansard represents the official record of parliamentary proceedings across the United Kingdom\u2019s legislative bodies.</p> <p>This dataset incorporates records from multiple sources, including debates and written answers from the UK Commons and Lords, devolved legislatures (Scottish Parliament, Senedd in both English and Welsh, Northern Ireland Assembly), London Mayor\u2019s Questions, and ministerial statements. Data was sourced from ParlParse, covering Commons debates from 1918 forward and Lords proceedings from the 1999 reform. Each document was processed to preserve complete parliamentary sessions as cohesive units, maintaining the natural flow of debate. All content is published under the Open Parliament License.</p>"},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 47.91K</li> <li>Number of tokens (Llama 3): 2.01B</li> <li>Average document length in tokens (min, max): 42.00K (1, 439.78K)</li> </ul>"},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#license-information","title":"License Information","text":"<p>This dataset is licensed under the Open Parliament License. </p> <p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/uk_hansard_filtered/uk_hansard_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/usgpo_filtered/usgpo_filtered.html","title":"Dataset Card for USGPO","text":"<p>The United States Government Publishing Office (USGPO) is a federal agency responsible for disseminating official documents authored by the U.S. government.</p> <p>This dataset includes all plain-text documents made available through the USGPO\u2019s GovInfo.gov developer API. This collection comprises over 2.7 million documents, spanning issues of the Federal Register, congressional hearing transcripts, budget reports, economic indicators, and other federal publicatio</p>"},{"location":"data/usgpo_filtered/usgpo_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 2.01M</li> <li>Number of tokens (Llama 3): 7.78B</li> <li>Average document length in tokens (min, max): 3.87K (18, 10.96M)</li> </ul>"},{"location":"data/usgpo_filtered/usgpo_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/usgpo_filtered/usgpo_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/usgpo_filtered/usgpo_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/usgpo_filtered/usgpo_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/usgpo_filtered/usgpo_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/usgpo_filtered/usgpo_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/uspto_filtered/uspto_filtered.html","title":"Dataset Card for USPTO","text":"<p>In the United States, patent documents are released into the public domain as government works.</p> <p>Patents follow a highly standardized format with distinct required sections for background, detailed description, and claims. We include patents from the US Patents and Trademark Office (USPTO) as provided by the Google Patents Public Data dataset, which includes millions of granted patents and published patent applications dating back to 1782. We processed these documents to extract clean text while preserving this structured format. Mathematical expressions and equations were converted into LaTeX.</p>"},{"location":"data/uspto_filtered/uspto_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 16.39M</li> <li>Number of tokens (Llama 3): 142.39B</li> <li>Average document length in tokens (min, max): 8.69K (24, 10.10M)</li> </ul>"},{"location":"data/uspto_filtered/uspto_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/uspto_filtered/uspto_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/uspto_filtered/uspto_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/uspto_filtered/uspto_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/uspto_filtered/uspto_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/uspto_filtered/uspto_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/wiki/wiki.html","title":"Dataset Card for Wikipedia","text":"<p>The Danish subsection of wikipedia.</p> <p>You can read more about wikipedia on their about page.</p>"},{"location":"data/wiki/wiki.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 308.42K</li> <li>Number of tokens (Llama 3): 172.43M</li> <li>Average document length in tokens (min, max): 559.0759571239593 (8, 106.84K)</li> </ul>"},{"location":"data/wiki/wiki.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"wiki_366127\",\n  \"text\": \"Vimoutiers er en kommune i departementet Orne i Basse-Normandie regionen i det nordvestlige Frankrig[...]\",\n  \"source\": \"wiki\",\n  \"added\": \"2021-03-28\",\n  \"created\": \"2019-01-01, 2021-01-01\",\n  \"token_count\": 126\n}\n</code></pre>"},{"location":"data/wiki/wiki.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/wiki/wiki.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/wiki/wiki.html#additional-information","title":"Additional Information","text":""},{"location":"data/wiki/wiki.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/wikibooks/wikibooks.html","title":"Dataset Card for Wikibooks","text":"<p>The Danish Subsection of Wikibooks.</p>"},{"location":"data/wikibooks/wikibooks.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 1.73K</li> <li>Number of tokens (Llama 3): 7.63M</li> <li>Average document length in tokens (min, max): 4.40K (8, 368.88K)</li> </ul>"},{"location":"data/wikibooks/wikibooks.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"wikibooks_1125\",\n  \"text\": \"Spilinfo.\\nSpillet er lavet af Blizzard Entertainment.\\nDet er efterf\u00f8lgeren til Diablo 1, og der er k[...]\",\n  \"source\": \"wikibooks\",\n  \"added\": \"2021-03-28\",\n  \"created\": \"2019-01-01, 2021-01-01\",\n  \"token_count\": 937\n}\n</code></pre>"},{"location":"data/wikibooks/wikibooks.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/wikibooks/wikibooks.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/wikibooks/wikibooks.html#additional-information","title":"Additional Information","text":""},{"location":"data/wikibooks/wikibooks.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/wikimedia_filtered/wikimedia_filtered.html","title":"Dataset Card for Wikimedia","text":"<p>Official Wikimedia wikis are released under a CC BY-SA license.</p> <p>We downloaded the official database dumps from March 2025 of the English-language wikis that are directly managed by the Wikimedia Foundation. These database dumps include the wikitext\u2014MediaWiki\u2019s custom markup language\u2014for each page as well as talk pages, where editors discuss changes made to a page. We only use the most recent version of each page. We converted wikitext to plain text using wtf_wikipedia after light adjustments in formatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitext math into LaTeX math using our custom code. Finally, any remaining HTML tags were removed via regexes. This collection includes data from the following Wikimedia wikis: Wikipedia, Wikinews, Wikibooks, Wikiquote, Wikisource, Wikiversity, Wikivoyage, and Wiktionary.</p>"},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 16.27M</li> <li>Number of tokens (Llama 3): 14.08B</li> <li>Average document length in tokens (min, max): 865.3751033306361 (5, 890.39K)</li> </ul>"},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/wikimedia_filtered/wikimedia_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/wikisource/wikisource.html","title":"Dataset Card for Wikisource","text":"<p>The Danish subsection of Wikisource.</p>"},{"location":"data/wikisource/wikisource.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 3.00K</li> <li>Number of tokens (Llama 3): 6.28M</li> <li>Average document length in tokens (min, max): 2.09K (17, 261.10K)</li> </ul>"},{"location":"data/wikisource/wikisource.html#dataset-structure","title":"Dataset Structure","text":"<p>An example from the dataset looks as follows.</p> <pre><code>{\n  \"id\": \"wikisource_4804\",\n  \"text\": \"&amp;lt;poem&amp;gt;\\nK\u00e6mpeh\u00f8jen.\\nJeg har st\u00e5et p\u00e5 mindets h\u00f8je,\\nf\u00f8lt dets vemodsdybe lyst\\nmed en t\u00e5re i mit [...]\",\n  \"source\": \"wikisource\",\n  \"added\": \"2021-03-28\",\n  \"created\": \"1700-01-01, 2022-01-01\",\n  \"token_count\": 1025\n}\n</code></pre>"},{"location":"data/wikisource/wikisource.html#data-fields","title":"Data Fields","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/wikisource/wikisource.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/wikisource/wikisource.html#additional-information","title":"Additional Information","text":""},{"location":"data/wikisource/wikisource.html#citation-information","title":"Citation Information","text":"<p>This dataset was initially published as part of the Danish gigaword. We recommend that you cite and reference it if you use this dataset:</p> <p>Derczynski, L., Ciosici, M. R., et al. (2021). The Danish Gigaword Corpus. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa 2021).</p> <pre><code>@inproceedings{dagw,\n title = {{The Danish Gigaword Corpus}},\n author = {Leon Derczynski and Manuel R. Ciosici and Rebekah Baglini and Morten H. Christiansen and Jacob Aarup Dalsgaard and Riccardo Fusaroli and Peter Juel Henrichsen and Rasmus Hvingelby and Andreas Kirkedal and Alex Speed Kjeldsen and Claus Ladefoged and Finn \u00c5rup Nielsen and Jens Madsen and Malte Lau Petersen and Jonathan Hvithamar Rystr\u00f8m and Daniel Varab},\n year = 2021,\n booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics},\n publisher = {NEALT}\n}\n</code></pre>"},{"location":"data/wikiteam_filtered/wikiteam_filtered.html","title":"Dataset Card for Wikiteam","text":"<p>There are many wikis on the internet that are not managed by the Wikimedia Foundation, but do use their MediaWiki software to power their wiki.</p> <p>There are many wikis on the internet that are not managed by the Wikimedia Foundation, but do use their MediaWiki software to power their wiki. Many of these wikis have been archived by Wikiteam, a collection of volunteers that create unofficial database dumps of wikis and upload them to the Internet Archive. We download all dumps made by Wikiteam when the metadata indicates the wiki was licensed under CC BY, CC BY-SA, or released into the public domain on the Internet Archive as of September of 2024. This results in downloading approximately 330,000 wikis. When multiple dumps of the same wiki exist, we use the most recent dump. We converted wikitext to plain text using wtf_wikipedia after light adjustments in formatting to avoid errors in section ordering caused by a bug. Before parsing, we converted wikitext math into LaTeX math using our custom code. Finally, any remaining HTML tags were removed via regexes. After preprocessing, we removed documents from wikis that appeared to contain large amounts of license laundering, e.g. those that were collections of song lyrics or transcripts. Per-document license information is available in the license entry of the metadata field of each example. Code for collecting, processing, and preparing this dataset is available in the common-pile GitHub repo.</p>"},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 10.23M</li> <li>Number of tokens (Llama 3): 2.94B</li> <li>Average document length in tokens (min, max): 287.8559942189961 (19, 432.48K)</li> </ul>"},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/wikiteam_filtered/wikiteam_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"data/youtube_filtered/youtube_filtered.html","title":"Dataset Card for Creative Commons Youtube","text":"<p>YouTube is a large-scale video-sharing platform where users have the option of uploading content under a CC BY license.</p> <p>To collect high-quality speech-based textual content and combat the rampant license laundering on YouTube, we manually curated a set of over 2,000 YouTube channels that consistently release original openly licensed content containing speech. The resulting collection spans a wide range of genres, including lectures, tutorials, reviews, video essays, speeches, and vlogs. From these channels, we retrieved over 1.1 million openly licensed videos comprising more than 470,000 hours of content. Finally, each video was transcribed to text using the Whisper speech recognition model. Code for collecting, processing, and preparing this dataset is available here.</p>"},{"location":"data/youtube_filtered/youtube_filtered.html#dataset-description","title":"Dataset Description","text":"<ul> <li>Number of samples: 988.25K</li> <li>Number of tokens (Llama 3): 4.07B</li> <li>Average document length in tokens (min, max): 4.12K (1, 193.44K)</li> </ul>"},{"location":"data/youtube_filtered/youtube_filtered.html#dataset-structure","title":"Dataset Structure","text":"<p>An entry in the dataset consists of the following fields:</p> <ul> <li><code>id</code> (<code>str</code>): An unique identifier for each document.</li> <li><code>text</code>(<code>str</code>): The content of the document.</li> <li><code>source</code> (<code>str</code>): The source of the document.</li> <li><code>added</code> (<code>str</code>): An date for when the document was added to this collection.</li> <li><code>created</code> (<code>str</code>): An date range for when the document was originally created.</li> <li><code>token_count</code> (<code>int</code>): The number of tokens in the sample computed using the Llama 8B tokenizer</li> </ul>"},{"location":"data/youtube_filtered/youtube_filtered.html#additional-processing","title":"Additional Processing","text":""},{"location":"data/youtube_filtered/youtube_filtered.html#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"data/youtube_filtered/youtube_filtered.html#additional-information","title":"Additional Information","text":""},{"location":"data/youtube_filtered/youtube_filtered.html#license-information","title":"License Information","text":"<p>While we aim to produce datasets with completely accurate licensing information, license laundering and inaccurate metadata can cause us to erroneously assign the incorrect license to some documents (for further discussion of this limitation, please see our paper). If you believe you have found an instance of incorrect licensing in this dataset, please start a discussion on this repository.</p>"},{"location":"data/youtube_filtered/youtube_filtered.html#citation-information","title":"Citation Information","text":"<p>If you use this dataset, please cite: <pre><code>@article{kandpal2025common,\n  title={{The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text}},\n  author={Nikhil Kandpal and Brian Lester and Colin Raffel and Sebastian Majstorovic and Stella Biderman and Baber Abbasi and Luca Soldaini and Enrico Shippole and A. Feder Cooper and Aviya Skowron and Shayne Longpre and Lintang Sutawika and Alon Albalak and Zhenlin Xu and Guilherme Penedo and Loubna Ben  and Elie Bakouch and John David  and Honglu Fan and Dashiell Stander and Guangyu Song and Aaron Gokaslan and John Kirchenbauer and Tom Goldstein and Brian R and Bhavya Kailkhura and Tyler Murray},\n  journal={arXiv preprint},\n  year={2025}\n}\n</code></pre></p>"},{"location":"taggers_stats/taggers_speed.html","title":"Statistics on Dolma and Scandi taggers","text":"<p>The dataset for testing is a subset of DAGW, which is only consist of documents from domain <code>Wiki &amp; Books</code> (430837 in total). However, during testing, 161915 documents that only contains white space(s) were found out due to the <code>ZeroDivisionErro</code> in tagger <code>pii_regex_v1</code> when there is no empty documents.</p> <p>Finally, the test on taggers performed on cleaned <code>Wiki &amp; Books</code> section of DAGW (268922 in total), each following tagger was tested only once using one proccess.</p> <p>Here is an example of how to use a tagger (Detailed documentation):</p> <p><pre><code>dolma tag \\\n    --documents \"/work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz\" \\\n    --experiment char_length_v1 \\\n    --taggers char_length_v1 \\\n    --processes 1\n</code></pre> Remark: <code>pii_presidio_v1</code> has a maximum text of length 1000000:  <pre><code>dolma.core.errors.DolmaFatalError: Failed to process /work/github/test_on_dagw_wiki/documents/dagw_only_wiki.json.gz due to ValueError: [E088] Text of length 1215638 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre></p> # Dolma Tagger Description Process Time (In total, Speed) 1 char_length_v1 Computes document length in characters 16s, 16.2kd/s 2 char_length_with_paragraphs_v1 Computes document and paragraph length in characters 49s, 5.40kd/s 3 cld2_en_doc_v2 Detects document language using cld2 56s, 4.76kd/s 4 olmo_pretokenizer_v1 Counts number of tokens using OLMo v1 pre-tokenizer 6m57s, 645d/s 5 olmo_pretokenizer_with_paragraphs_v1 Counts tokens in document and paragraphs using OLMo v1 pre-tokenizer 7m02s, 636d/s 6 whitespace_tokenizer_v1 Counts whitespace-separated tokens in document 1m00s, 4.47kd/s 7 whitespace_tokenizer_with_paragraphs_v1 Counts whitespace-separated tokens in document and paragraphs 1m39s, 2.70kd/s 8 random_number_v1 Assigns a random number to each document 17s, 15.6kd/s 9 ft_lang_id_en_doc_v2 Uses fastText to detect the language of the document 2m28s, 1.82kd/s 10 ft_lang_id_en_paragraph_v2 Uses fastText to detect the language of each paragraph 6m21s, 705d/s 11 ft_lang_id_en_paragraph_with_doc_score_v2 Uses fastText to detect the language of each paragraph and assigns a score based on the fraction of English paragraphs 6m16s, 715d/s 12 gopher_v1 Tags spans of documents matching\u00a0Deepmind's Gopher\u00a0removal rules 15m49s, 283d/s 13 c4_v1 Implements taggers used to generate the\u00a0C4\u00a0dataset 3m50s, 1.17kd/s 14 c4_v2 Faster implementation of the C4 taggers 2m08s, 2.10kd/s 15 pii_presidio_v1 Tags spans of documents that contain personally identifiable information (PII) using the\u00a0Presidio Analyzer\u00a0library way to slow: about 7s per document. However <code>analyzer_results</code> in pii.py defines the language as English if . See line 110 in here 16 pii_regex_v1 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions 2m55s, 1.53kd/s 17 pii_regex_v2 Faster implementation of\u00a0<code>pii_regex_v1</code> 2m51s, 1.57kd/s 18 pii_regex_with_counts_v2 Tags spans of documents that contain personally identifiable information (PII) using a set of regular expressions. It also counts the number of matches for each regular expression 2m43s, 1.65kd/s 19 pii_regex_with_counts_fast_v2 Faster implementation of\u00a0<code>pii_regex_with_counts_v2</code> 1m01s, 4.36kd/s 20 cld2_scandi_doc Language Detection using cld2 1m11s, 3.79kd/s 21 cld2_scandi_paragraph Language Detection on paragraph level using cld2 5m59s, 748d/s 22 ft_lang_id_scandi_doc FastText Language Detection 3m14s, 1.38kd/s 23 ft_lang_id_scandi_paragraph FastText Language Detection on paragraph level 14m06s, 318d/s 24 cld2_scandi_paragraph_with_doc_score Language Detection on paragraph level with a total score using cld2 8m04s, 556d/s 25 ft_lang_id_scandi_paragraph_with_doc_score FastText Language Detection on paragraph level with a total score 14m37s, 306d/s 26 jigsaw_hatespeech_document_v2 Tags documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 1m38s, 2.74kd/s 27 jigsaw_hatespeech_sentence_v2 Tags spans of documents as containing hate speech or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0hate speech dataset. 9m45s, 460d/s 28 jigsaw_nsfw_document_v1 Tags documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 6m40s, 671d/s 29 jigsaw_nsfw_sentence_v2 Tags spans of documents as containing NSFW content or not using a FastText classifier trained on the\u00a0Jigsaw\u00a0NSFW dataset. 9m02s, 496d/s"},{"location":"tutorials/finetune.html","title":"Efficiently Finetuning Language Models","text":"<p>This notebook will allow you to try out finetuning of the <code>munin-7b-alpha</code> model or, indeed, any other generative model out there.</p> <p>We'll be finetuning the model on a Danish translated instruction tuning dataset, using the QLoRA method.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121\n# %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\"\n</pre> # Uncomment to install packages (already done for you) # %pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton --index-url https://download.pytorch.org/whl/cu121 # %pip install \"unsloth[cu121_ampere_torch211] @ git+https://github.com/unslothai/unsloth.git\" In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport getpass\n\n# For loading the finetuning datasets\nfrom datasets import load_dataset\n\n# For loading and finetuning the models\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer, setup_chat_format\nfrom transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import getpass  # For loading the finetuning datasets from datasets import load_dataset  # For loading and finetuning the models from unsloth import FastLanguageModel from trl import SFTTrainer, setup_chat_format from transformers import TrainingArguments, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow finetuning gated models (like LLaMA-2) and to upload your finetuned models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None In\u00a0[\u00a0]: Copied! <pre>RANDOM_SEED = 42\n\nMODEL_CONFIGURATION = dict(\n    model_name=\"danish-foundation-models/munin-7b-alpha\",\n    max_seq_length=2048,  \n    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs\n    load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.\n    attn_implementation=\"flash_attention_2\"\n)\n\nPEFT_CONFIGURATION = dict(\n    r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\", \n        \"k_proj\", \n        \"v_proj\", \n        \"o_proj\", \n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,  # Supports any, but = 0 is optimized\n    bias = \"none\",  # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    use_rslora = False,  # Support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n    random_state = RANDOM_SEED,\n)\n\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n</pre> RANDOM_SEED = 42  MODEL_CONFIGURATION = dict(     model_name=\"danish-foundation-models/munin-7b-alpha\",     max_seq_length=2048,       dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ GPUs     load_in_4bit=True,  # Use 4bit quantisation to reduce memory usage. Quantises on the fly, so can take a while.     attn_implementation=\"flash_attention_2\" )  PEFT_CONFIGURATION = dict(     r = 16,  # Adapter rank, choose any number &gt; 0, but suggested 8, 16, 32, 64, 128     target_modules=[         \"q_proj\",          \"k_proj\",          \"v_proj\",          \"o_proj\",          \"gate_proj\",          \"up_proj\",          \"down_proj\",     ],     lora_alpha = 16,     lora_dropout = 0,  # Supports any, but = 0 is optimized     bias = \"none\",  # Supports any, but = \"none\" is optimized     use_gradient_checkpointing = True,     use_rslora = False,  # Support rank stabilized LoRA     loftq_config = None,  # And LoftQ     random_state = RANDOM_SEED, )  FINETUNING_CONFIGURATION = dict(     per_device_train_batch_size=8,     gradient_accumulation_steps=1,     warmup_steps=5,     num_train_epochs=1,     learning_rate=2e-4,     weight_decay=0.01,     lr_scheduler_type=\"linear\", ) In\u00a0[\u00a0]: Copied! <pre>model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN)\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\nmodel = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION)\n</pre> model, tokenizer = FastLanguageModel.from_pretrained(**MODEL_CONFIGURATION, token=HUGGING_FACE_TOKEN) model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer) model = FastLanguageModel.get_peft_model(model, **PEFT_CONFIGURATION) <p>Load the dataset from Hugging Face Hub:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</pre> dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\") print(f\"Number of samples in dataset: {len(dataset):,}\") <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> In\u00a0[\u00a0]: Copied! <pre>n_samples = 1000\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</pre> n_samples = 1000 dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples)) <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> In\u00a0[\u00a0]: Copied! <pre>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</pre> def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:     \"\"\"This converts the sample to the standardised ChatML format.      Args:         sample:             The data sample.      Returns:         The sample set up in the ChatML format.     \"\"\"     return {         \"messages\": [             {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},             {\"role\": \"user\", \"content\": sample[\"question\"]},             {\"role\": \"assistant\", \"content\": sample[\"response\"]}         ]     }  dataset = dataset.map(create_conversation, batched=False) In\u00a0[\u00a0]: Copied! <pre>trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],\n    dataset_num_proc=4,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</pre> trainer = SFTTrainer(     model=model,     tokenizer=tokenizer,     train_dataset=dataset,     max_seq_length=MODEL_CONFIGURATION[\"max_seq_length\"],     dataset_num_proc=4,     packing=True,  # Can make training 5x faster for short sequences.     args = TrainingArguments(         optim=\"adamw_8bit\",         fp16=not torch.cuda.is_bf16_supported(),         bf16=torch.cuda.is_bf16_supported(),         logging_steps=3,         seed=RANDOM_SEED,         output_dir=\"outputs\",         **FINETUNING_CONFIGURATION     ), ) In\u00a0[\u00a0]: Copied! <pre># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</pre> # Log some GPU stats before we start the finetuning gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(     f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"     f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\" ) In\u00a0[\u00a0]: Copied! <pre># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</pre> # This is where the actual finetuning is happening trainer_stats = trainer.train() In\u00a0[\u00a0]: Copied! <pre># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</pre> # Log some post-training GPU statistics used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory / max_memory * 100, 3) lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) print(     f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"     f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"     \"was used for LoRa.\" ) <p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,  #\u00a0Required by unsloth\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id,     use_cache=False,  #\u00a0Required by unsloth ) <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad synes du om Danish Foundation Models projektet? Skriv kortfattet.\"  # And change this too     ), ]  outputs = model.generate(     input_ids=tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),     streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN) <p>The popular inference framework vLLM can take advantage of having a model available in lower precision, enabling faster inference times.</p> <p>You can uncomment the following lines if you want to save the model in 16-bit or even 4-bit precision:</p> In\u00a0[\u00a0]: Copied! <pre># Merge to 16bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)\n\n# Merge to 4bit\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN)\n</pre> # Merge to 16bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_16bit\", token=HUGGING_FACE_TOKEN)  # Merge to 4bit # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\", token=HUGGING_FACE_TOKEN) <p>Alternatively, you can save only the adapter weights, which are very light, but which requires the base model to be able to use it:</p> In\u00a0[\u00a0]: Copied! <pre># Just LoRA adapters\n# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n# model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN)\n</pre> # Just LoRA adapters # model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",) # model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\", token=HUGGING_FACE_TOKEN) <p>You can also save the model in the popular <code>GGUF</code> or <code>llama.cpp</code> formats, by uncommenting any of the following:</p> In\u00a0[\u00a0]: Copied! <pre># Save to 8bit Q8_0\n# model.save_pretrained_gguf(\"model\", tokenizer)\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)\n\n# Save to 16bit GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)\n\n# Save to q4_k_m GGUF\n# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n# model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN)\n</pre> # Save to 8bit Q8_0 # model.save_pretrained_gguf(\"model\", tokenizer) # model.push_to_hub_gguf(\"hf/model\", tokenizer, token=HUGGING_FACE_TOKEN)  # Save to 16bit GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=HUGGING_FACE_TOKEN)  # Save to q4_k_m GGUF # model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\") # model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=HUGGING_FACE_TOKEN) <p>Now, use the <code>model-unsloth.gguf</code> file or <code>model-unsloth-Q4_K_M.gguf</code> file in <code>llama.cpp</code> or a UI based system like <code>GPT4All</code>. You can install GPT4All by going here.</p>"},{"location":"tutorials/finetune.html#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/finetune.html#efficiently-finetuning-language-models","title":"Efficiently Finetuning Language Models\u00b6","text":""},{"location":"tutorials/finetune.html#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/finetune.html#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/finetune.html#configure-the-model","title":"Configure the Model\u00b6","text":""},{"location":"tutorials/finetune.html#load-the-model","title":"Load the Model\u00b6","text":""},{"location":"tutorials/finetune.html#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":""},{"location":"tutorials/finetune.html#finetune","title":"Finetune!\u00b6","text":""},{"location":"tutorials/finetune.html#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/finetune.html#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/finetune.html#extra-export-model-to-other-frameworks","title":"Extra: Export Model to Other Frameworks\u00b6","text":""},{"location":"tutorials/finetune.html#saving-to-float16-for-vllm","title":"Saving to float16 for vLLM\u00b6","text":""},{"location":"tutorials/finetune.html#gguf-llamacpp-conversion","title":"GGUF / llama.cpp Conversion\u00b6","text":""},{"location":"tutorials/finetune_qlora.html","title":"Finetuning Language Models using QLoRA","text":"<p>We demonstrate how to finetune a <code>munin-7b-alpha</code> or another large language model (LLM) on a Danish translated instruction tuning dataset, with LoRA and tools from the PyTorch and Hugging Face ecosystem. This notebook can be run on on a typical consumer GPU (e.g. NVIDIA T4 16GB).</p> <p>This notebook takes some liberties to ensure simplicity and readability, while remaining reasonably efficient. However if you want a more efficient approach, see the tutorial on (efficiently) finetuning language models.</p> In\u00a0[1]: Copied! <pre>%pip install -q datasets bitsandbytes peft trl accelerate sentencepiece protobuf --upgrade\n\n# Description of the libraries:\n# - Datasets: A high-performant dataset library for easily sharing and accessing datasets from the huggingface Hub at huggingface.co/datasets\n# - bitsandbytes: A lightweight library for loading models using low-precession (this makes it faster and use less memory)\n# - Transformers: A high-level library for working with language LLMs\n# - PEFT: A library for parameter-efficient fine-tuning of LLMs\n# - TRL: A library for training LLMs using reinforcement learning\n# - Accelerate: A library for distributed and efficient training of LLMs\n# - Sentencepiece: A library for tokenizing text required by some models\n</pre> %pip install -q datasets bitsandbytes peft trl accelerate sentencepiece protobuf --upgrade  # Description of the libraries: # - Datasets: A high-performant dataset library for easily sharing and accessing datasets from the huggingface Hub at huggingface.co/datasets # - bitsandbytes: A lightweight library for loading models using low-precession (this makes it faster and use less memory) # - Transformers: A high-level library for working with language LLMs # - PEFT: A library for parameter-efficient fine-tuning of LLMs # - TRL: A library for training LLMs using reinforcement learning # - Accelerate: A library for distributed and efficient training of LLMs # - Sentencepiece: A library for tokenizing text required by some models In\u00a0[2]: Copied! <pre># print the version of the libraries for reproducibility\nimport datasets\nimport bitsandbytes\nimport transformers\nimport peft\nimport trl\nimport accelerate\nimport sentencepiece\n\nprint(f\"datasets: {datasets.__version__}\")\nprint(f\"bitsandbytes: {bitsandbytes.__version__}\")\nprint(f\"transformers: {transformers.__version__}\")\nprint(f\"peft: {peft.__version__}\")\nprint(f\"trl: {trl.__version__}\")\nprint(f\"accelerate: {accelerate.__version__}\")\nprint(f\"sentencepiece: {sentencepiece.__version__}\")\n</pre> # print the version of the libraries for reproducibility import datasets import bitsandbytes import transformers import peft import trl import accelerate import sentencepiece  print(f\"datasets: {datasets.__version__}\") print(f\"bitsandbytes: {bitsandbytes.__version__}\") print(f\"transformers: {transformers.__version__}\") print(f\"peft: {peft.__version__}\") print(f\"trl: {trl.__version__}\") print(f\"accelerate: {accelerate.__version__}\") print(f\"sentencepiece: {sentencepiece.__version__}\")  <pre>datasets: 2.19.1\nbitsandbytes: 0.43.1\ntransformers: 4.40.1\npeft: 0.10.0\ntrl: 0.8.6\naccelerate: 0.30.0\nsentencepiece: 0.2.0\n</pre> In\u00a0[3]: Copied! <pre>from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_name = \"mhenrichsen/danskgpt-tiny-chat\" # download a smaller model (due to memory constraint of Colab)\n# model_name=\"danish-foundation-models/munin-7b-alpha\" # if you have more memory you can use this\n\n\n# Load base model\n# - optionally load the model in 4-bit precision (recommended for large models to save memory)\nbnb_config = BitsAndBytesConfig(\n     load_in_4bit=True,\n     bnb_4bit_use_double_quant=True,\n     bnb_4bit_quant_type=\"nf4\",\n     bnb_4bit_compute_dtype=torch.bfloat16\n )\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config\n)\n</pre> from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig import torch  model_name = \"mhenrichsen/danskgpt-tiny-chat\" # download a smaller model (due to memory constraint of Colab) # model_name=\"danish-foundation-models/munin-7b-alpha\" # if you have more memory you can use this   # Load base model # - optionally load the model in 4-bit precision (recommended for large models to save memory) bnb_config = BitsAndBytesConfig(      load_in_4bit=True,      bnb_4bit_use_double_quant=True,      bnb_4bit_quant_type=\"nf4\",      bnb_4bit_compute_dtype=torch.bfloat16  ) model = AutoModelForCausalLM.from_pretrained(     model_name,     quantization_config=bnb_config ) <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n</pre> In\u00a0[4]: Copied! <pre>from transformers import TextStreamer, AutoTokenizer\n\nprompt = \"Meningen med livet er\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer([prompt], return_tensors=\"pt\")\nstreamer = TextStreamer(tokenizer)\noutputs = model.generate(**inputs, streamer=streamer, max_new_tokens=50)\n# The output is influence by quantization (if the model is not trained with quantization)\n# Try disabling it to see the difference.\n</pre> from transformers import TextStreamer, AutoTokenizer  prompt = \"Meningen med livet er\"  tokenizer = AutoTokenizer.from_pretrained(model_name) inputs = tokenizer([prompt], return_tensors=\"pt\") streamer = TextStreamer(tokenizer) outputs = model.generate(**inputs, streamer=streamer, max_new_tokens=50) # The output is influence by quantization (if the model is not trained with quantization) # Try disabling it to see the difference. <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>&lt;s&gt; Meningen med livet </pre> <pre>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n</pre> <pre>er at finde en balance mellem arbejde og fritid. Det er ikke n\u00f8dvendigt at have en stor m\u00e6ngde penge for at have det godt. Det er vigtigt at have\n</pre> In\u00a0[5]: Copied! <pre>from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n\n# Prepare quantized model for peft training\nmodel = prepare_model_for_kbit_training(model)\n\n# create lora confgi\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# Create PeftModel which inserts LoRA adaper modules into the model\nmodel = get_peft_model(model, lora_config)\n\n# to save the adapter weights (not the model weights)\n# model.save_pretrained(\"my_awesome_adapter\")\n</pre> from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training  # Prepare quantized model for peft training model = prepare_model_for_kbit_training(model)  # create lora confgi lora_config = LoraConfig(     r=8,     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],     bias=\"none\",     task_type=TaskType.CAUSAL_LM, )  # Create PeftModel which inserts LoRA adaper modules into the model model = get_peft_model(model, lora_config)  # to save the adapter weights (not the model weights) # model.save_pretrained(\"my_awesome_adapter\") <p>Load the dataset from Hugging Face Hub or use local data. In this example, we will use the <code>kobprof/skolegpt-instruct</code> dataset from the Hugging Face Hub. The dataset is a Danish instruction dataset that has been translated from English to Danish.</p> <p>Examining the data you can see that it would be easy to replace it with your own dataset.</p> In\u00a0[6]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\")\nprint(f\"Number of samples in dataset: {len(dataset):,}\")\n</pre> from datasets import load_dataset  dataset = load_dataset(\"kobprof/skolegpt-instruct\", split=\"train\") print(f\"Number of samples in dataset: {len(dataset):,}\") <pre>Number of samples in dataset: 21,580\n</pre> In\u00a0[7]: Copied! <pre># let us examine one sample\nsample = dataset[101]\nprint(sample)\n</pre> # let us examine one sample sample = dataset[101] print(sample) <pre>{'id': 't0.1101311', 'system_prompt': 'Du er en AI-assistent. Brugeren vil give dig en opgave. Dit m\u00e5l er at udf\u00f8re opgaven s\u00e5 trofast, som du kan. Mens du udf\u00f8rer opgaven, skal du t\u00e6nke trin for trin og begrunde dine trin.', 'question': 'Hvilket amerikansk rockband fra Frederick, Maryland, med oprindelse i Germantown, Maryland, havde en live-DVD med titlen Full Fathom Five: Video Field Recordings?', 'response': 'For at finde svaret p\u00e5 dette sp\u00f8rgsm\u00e5l vil jeg f\u00f8rst huske popul\u00e6re amerikanske rockbands og tjekke, om nogen af dem er fra Frederick, Maryland, eller har forbindelser til Germantown, Maryland. Hvis jeg ikke kan finde svaret gennem min vidensbase, vil jeg s\u00f8ge p\u00e5 internettet efter relevante oplysninger.\\n\\nDa jeg genkalder mig popul\u00e6re amerikanske rockbands, kan jeg ikke umiddelbart identificere nogen bands fra Frederick eller Germantown, Maryland.\\n\\nJeg vil nu s\u00f8ge p\u00e5 internettet efter oplysningerne.\\n\\n[S\u00f8ger...]\\n\\nEfter at have s\u00f8gt p\u00e5 internettet fandt jeg det amerikanske rockband Clutch, som er fra Frederick, Maryland, og stammer fra Germantown, Maryland. De har en live-DVD, der hedder Full Fathom Five: Video Field Recordings.\\n\\nA: Det amerikanske rockband, du leder efter, er Clutch.', 'source': 't0'}\n</pre> <p>We just take a random subset, 1000 samples should take around 7 minutes on this machine depending on settings.</p> In\u00a0[8]: Copied! <pre>n_samples = 1000\nRANDOM_SEED = 42\ndataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples))\n</pre> n_samples = 1000 RANDOM_SEED = 42 dataset = dataset.shuffle(seed=RANDOM_SEED).select(range(n_samples)) <p>Lastly, we set up the conversations in the dataset into the standard ChatML format.</p> In\u00a0[9]: Copied! <pre>def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:\n    \"\"\"This converts the sample to the standardised ChatML format.\n\n    Args:\n        sample:\n            The data sample.\n\n    Returns:\n        The sample set up in the ChatML format.\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n            {\"role\": \"user\", \"content\": sample[\"question\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n        ]\n    }\n\ndataset = dataset.map(create_conversation, batched=False)\n</pre> def create_conversation(sample: dict) -&gt; dict[str, list[dict[str, str]]]:     \"\"\"This converts the sample to the standardised ChatML format.      Args:         sample:             The data sample.      Returns:         The sample set up in the ChatML format.     \"\"\"     return {         \"messages\": [             {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},             {\"role\": \"user\", \"content\": sample[\"question\"]},             {\"role\": \"assistant\", \"content\": sample[\"response\"]}         ]     }  dataset = dataset.map(create_conversation, batched=False) In\u00a0[13]: Copied! <pre>from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Setting up the Trainer\nFINETUNING_CONFIGURATION = dict(\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    warmup_steps=5,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=1024, # The maximum sequence length the model can handle\n    packing=True,  # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        optim=\"adamw_8bit\",\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=3,\n        seed=RANDOM_SEED,\n        output_dir=\"outputs\",\n        **FINETUNING_CONFIGURATION\n    ),\n)\n</pre> from trl import SFTTrainer from transformers import TrainingArguments  # Setting up the Trainer FINETUNING_CONFIGURATION = dict(     per_device_train_batch_size=8,     gradient_accumulation_steps=1,     warmup_steps=5,     num_train_epochs=1,     learning_rate=2e-4,     weight_decay=0.01,     lr_scheduler_type=\"linear\", )  trainer = SFTTrainer(     model=model,     tokenizer=tokenizer,     train_dataset=dataset,     max_seq_length=1024, # The maximum sequence length the model can handle     packing=True,  # Can make training 5x faster for short sequences.     args = TrainingArguments(         optim=\"adamw_8bit\",         fp16=not torch.cuda.is_bf16_supported(),         bf16=torch.cuda.is_bf16_supported(),         logging_steps=3,         seed=RANDOM_SEED,         output_dir=\"outputs\",         **FINETUNING_CONFIGURATION     ), ) In\u00a0[14]: Copied! <pre># Log some GPU stats before we start the finetuning\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(\n    f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"\n    f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\"\n)\n</pre> # Log some GPU stats before we start the finetuning gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(     f\"You're using the {gpu_stats.name} GPU, which has {max_memory:.2f} GB of memory \"     f\"in total, of which {start_gpu_memory:.2f}GB has been reserved already.\" ) <pre>You're using the Tesla T4 GPU, which has 14.75 GB of memory in total, of which 6.81GB has been reserved already.\n</pre> In\u00a0[15]: Copied! <pre># This is where the actual finetuning is happening\ntrainer_stats = trainer.train()\n</pre> # This is where the actual finetuning is happening trainer_stats = trainer.train() <pre>/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n</pre>        [76/76 07:59, Epoch 1/1]      Step Training Loss 3 1.759400 6 1.845600 9 1.809500 12 1.581200 15 1.613000 18 1.587200 21 1.521900 24 1.490400 27 1.484400 30 1.517800 33 1.434900 36 1.434500 39 1.479500 42 1.619800 45 1.453000 48 1.324800 51 1.379600 54 1.454800 57 1.385700 60 1.366800 63 1.279800 66 1.319700 69 1.400300 72 1.390300 75 1.426700 <p> </p> In\u00a0[16]: Copied! <pre># Log some post-training GPU statistics\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(\n    f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"\n    f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"\n    \"was used for LoRa.\"\n)\n</pre> # Log some post-training GPU statistics used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory / max_memory * 100, 3) lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) print(     f\"We ended up using {used_memory:.2f} GB GPU memory ({used_percentage:.2f}%), \"     f\"of which {used_memory_for_lora:.2f} GB ({lora_percentage:.2f}%) \"     \"was used for LoRa.\" ) <pre>We ended up using 9.05 GB GPU memory (61.38%), of which 2.25 GB (15.23%) was used for LoRa.\n</pre> In\u00a0[18]: Copied! <pre>from transformers import GenerationConfig\n\nGENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256,\n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True,\n    temperature=0.2,\n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    use_cache=False,\n)\n</pre> from transformers import GenerationConfig  GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,      # Controlling how the model chooses the next token to generate     do_sample=True,     temperature=0.2,     repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id,     use_cache=False, ) <p>Let's use <code>TextStreamer</code> for continuous inference - so you can see the generation token by token, instead of waiting the whole time!</p> In\u00a0[20]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"N\u00e6vn nogle positive og negative sider ved large language models.\"  # And change this too\n    ),\n]\n\noutputs = model.generate(\n    input_ids=tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),\n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"N\u00e6vn nogle positive og negative sider ved large language models.\"  # And change this too     ), ]  outputs = model.generate(     input_ids=tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\"),     streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <pre>Large Language Models (LLM) er en type af maskinl\u00e6ringsteknologier, der bruges til at generere eller forudsige tekster p\u00e5 et stort antal sprog. De har flere fordele:\n\n1. Genereret tekst: LLM-modeller kan generere tekst baseret p\u00e5 inputdata fra store m\u00e6ngder af data. Dette g\u00f8r det muligt at generere meget omfattende og detaljerede tekster med h\u00f8j grad af n\u00f8jagtighed.\n\n2. Forbedret tekstbehandling: LLM-modeller kan behandle store m\u00e6ngder af tekst i realtid, hvilket betyder, at de ikke skal vente p\u00e5, at inputdata bliver indsamlet f\u00f8rst. Dette reducerer tiden, det tager at generere en tekst, og giver dem mulighed for at fokusere mere p\u00e5 den specifikke opgave.\n\n3. Brugervenlig: LLM-modeller er designet til at v\u00e6re nemme at bruge for mennesker,\n</pre> <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/qlora_model\", token=HUGGING_FACE_TOKEN)"},{"location":"tutorials/finetune_qlora.html#draft-false-date-2024-05-08","title":"draft: false date: 2024-05-08\u00b6","text":""},{"location":"tutorials/finetune_qlora.html#finetuning-language-models-using-qlora","title":"Finetuning Language Models using QLoRA\u00b6","text":""},{"location":"tutorials/finetune_qlora.html#open-in-colab","title":"Open In Colab\u00b6","text":"<p>You can open this notebook in Google Colab by clicking the button below:</p> <p></p>"},{"location":"tutorials/finetune_qlora.html#introduction","title":"Introduction\u00b6","text":"<p>Large Language Models (LLMs) have shown impressive capabilities in a wide variety of applications. Developers often seek to tailor these LLMs for specific use-cases and applications to fine-tune them for better performance or other reasons including but not limited to:</p> <ul> <li>Reducing Hallucinations</li> <li>Better handling of retrieved information</li> <li>Learn New Information (When data size is large)</li> <li>Cost Optimization</li> <li>Privacy</li> </ul> <p> </p> Figure: An simple illustration of model fine-tuning. <p>However, LLMs are large by design and require a large number of GPUs to be fine-tuned. A common approach to fine-tuning LLMs is to use a technique called Parameter Efficient Fine-Tuning (PEFT). PEFT methods aim to drastically reduce the number of trainable parameters of a model while keeping the same performance as full fine-tuning. The following sections will introduce the LoRA method, but it is perfectly fine to skip this section.</p> An example of the memory requirements for fine-tuning a large language model (click to unfold)  <p>Let\u2019s focus on a specific example by trying to fine-tune a Llama model on a free-tier Google Colab instance (1x NVIDIA T4 16GB). Llama-2 7B has 7 billion parameters, with a total of 28GB in case the model is loaded in full-precision. Given our GPU memory constraint (16GB), the model cannot even be loaded, much less trained on our GPU. This memory requirement can be divided by two with negligible performance degradation. You can read more about running models in half-precision and mixed precision for training here.</p> <p>In the case of full fine-tuning with Adam optimizer using a half-precision model and mixed-precision mode, we need to allocate per parameter:</p> <ul> <li>2 bytes for the weight</li> <li>2 bytes for the gradient</li> <li>4 + 8 bytes for the Adam optimizer states</li> </ul> <p>With a total of 16 bytes per trainable parameter, this makes a total of 112GB (excluding the intermediate hidden states). Given that the largest GPU available today can have up to 80GB GPU VRAM, it makes fine-tuning challenging and less accessible to everyone. To bridge this gap, Parameter Efficient Fine-Tuning (PEFT) methods are largely adopted today by the community.</p>"},{"location":"tutorials/finetune_qlora.html#low-rank-adaption-for-large-language-models-lora-parameter-efficient-fine-tuning","title":"Low-rank Adaption for Large Language Models (LoRA) Parameter Efficient Fine-Tuning\u00b6","text":"<p>Parameter Efficient Fine-Tuning (PEFT) methods, such as LoRA, aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning. Multiple PEFT methods to get an overview we recommend the article \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\", however in this notebook we will focus on the LoRA method.</p> <p>The LoRA method by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model(that we will denote by base model).</p> <p>To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn\u2019t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.</p> <p>This approach has several advantages:</p> <ul> <li>LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.</li> <li>The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.</li> <li>LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.</li> <li>The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.</li> <li>LoRA does not add any inference latency when adapter weights are merged with the base model</li> </ul> <p>In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix.</p> <p> </p> Figure: Animated diagram that show how LoRA works in practice."},{"location":"tutorials/finetune_qlora.html#install-dependencies","title":"Install Dependencies\u00b6","text":"<p>Before we start, we need to install the following dependencies:</p>"},{"location":"tutorials/finetune_qlora.html#loading-and-testing-model","title":"Loading and testing Model\u00b6","text":"<p>This sections loads the model and tests it on a simple example. For this example, we will use the <code>munin-7b-alpha</code> model created by the Danish Foundation Models team.</p>"},{"location":"tutorials/finetune_qlora.html#add-in-the-lora-adapters","title":"Add in the LoRA Adapters\u00b6","text":"<p>This section adds in the LoRA adapters to the model. The LoRA adapters are added to the attention blocks of the model. The adapters are initialized with random values and are trained during the fine-tuning process. The original weights of the model are kept frozen and are not updated during the fine-tuning process. The adapters are merged with the original weights during inference to produce the final results.</p>"},{"location":"tutorials/finetune_qlora.html#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":""},{"location":"tutorials/finetune_qlora.html#finetuning-the-model","title":"Finetuning the Model\u00b6","text":"<p>We will use the <code>trl</code> library to finetune the model. <code>trl</code> is a library which provides a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. In this notebook, we will only use the SFT step.</p>"},{"location":"tutorials/finetune_qlora.html#trying-out-the-new-model","title":"Trying out the new Model\u00b6","text":"<p>Time to try out the new finetuned model. First we need to set up how to generate text with it.</p> <p>You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p>"},{"location":"tutorials/finetune_qlora.html#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/finetune_qlora.html#references","title":"References\u00b6","text":"<p>This notebook takes inspiration, snippets, figures, and quotes from the following sources:</p> <ul> <li>Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem</li> <li>Our previous tutorial on (efficiently) finetuning language models</li> <li>Enhancing LLM inferencing with RAG and fine-tuned LLMs - Generative AI Workshop, AI-ML Systems Conference - 2023, Bengaluru</li> </ul>"},{"location":"tutorials/merge.html","title":"Merging Language Models","text":"<p>Model merging is a relatively new method that allows one to combine the weights of different language models into a single model.</p> <p>In this notebook you'll get to try this out, as well as try to interact with the merged model to see the results!</p> <p>The mergekit README is good to have open for this notebook. It has descriptions and examples for the different merge methods it supports.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment to install packages (already done for you)\n# !git clone https://github.com/cg123/mergekit.git\n# %cd mergekit\n# %pip install -e .\n# %cd ..\n</pre> # Uncomment to install packages (already done for you) # !git clone https://github.com/cg123/mergekit.git # %cd mergekit # %pip install -e . # %cd .. In\u00a0[\u00a0]: Copied! <pre># General packages\nimport torch\nimport shutil\nfrom pathlib import Path\n\n# For merging the models\nfrom mergekit.config import MergeConfiguration\nfrom mergekit.merge import MergeOptions, run_merge\n\n# For loading the models and running them after the merge\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n</pre> # General packages import torch import shutil from pathlib import Path  # For merging the models from mergekit.config import MergeConfiguration from mergekit.merge import MergeOptions, run_merge  # For loading the models and running them after the merge import transformers from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig <p>To allow merging gated models (like LLaMA-2) and to upload your merged models, you can put your Hugging Face token in the cell below.</p> <p>You can generate a token at https://hf.co/settings/tokens.</p> <p>If you don't want to supply a token then simply leave it blank!</p> In\u00a0[\u00a0]: Copied! <pre>import getpass\nHUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \")\nif not HUGGING_FACE_TOKEN:\n    print(\"Not using a Hugging Face token.\")\n    HUGGING_FACE_TOKEN = None\n</pre> import getpass HUGGING_FACE_TOKEN = getpass.getpass(\"Hugging Face Token: \") if not HUGGING_FACE_TOKEN:     print(\"Not using a Hugging Face token.\")     HUGGING_FACE_TOKEN = None <p>This is where we set up which models we would like to merge, and which merging method to use.</p> <p>This configuration was the configuration used to create the Munin-NeuralBeagle model, but you can change it to whatever you like!</p> In\u00a0[\u00a0]: Copied! <pre>merge_config = dict(\n    models=[\n        dict(\n            model=\"danish-foundation-models/munin-7b-alpha\",\n        ),\n        dict(\n            model=\"mlabonne/NeuralBeagle14-7B\",\n            parameters=dict(\n                density=0.53,\n                weight=0.6,\n            ),\n        ),\n    ],\n    merge_method=\"dare_ties\",\n    base_model=\"danish-foundation-models/munin-7b-alpha\",\n    parameters=dict(\n        int8_mask=True,\n    ),\n    dtype=\"bfloat16\",\n)\n</pre> merge_config = dict(     models=[         dict(             model=\"danish-foundation-models/munin-7b-alpha\",         ),         dict(             model=\"mlabonne/NeuralBeagle14-7B\",             parameters=dict(                 density=0.53,                 weight=0.6,             ),         ),     ],     merge_method=\"dare_ties\",     base_model=\"danish-foundation-models/munin-7b-alpha\",     parameters=dict(         int8_mask=True,     ),     dtype=\"bfloat16\", ) In\u00a0[\u00a0]: Copied! <pre>LAZY_UNPICKLE = False  # Experimental low-memory model loader\nLOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap\nOUT_PATH = \"./merged\"\n</pre> LAZY_UNPICKLE = False  # Experimental low-memory model loader LOW_CPU_MEMORY = True  # Enable if you have more VRAM than RAM+swap OUT_PATH = \"./merged\" In\u00a0[\u00a0]: Copied! <pre>run_merge(\n    MergeConfiguration.model_validate(merge_config),\n    out_path=OUT_PATH,\n    options=MergeOptions(\n        lora_merge_cache=\"/tmp\",\n        cuda=torch.cuda.is_available(),\n        copy_tokenizer=True,\n        lazy_unpickle=LAZY_UNPICKLE,\n        low_cpu_memory=LOW_CPU_MEMORY,\n    )\n)\n</pre> run_merge(     MergeConfiguration.model_validate(merge_config),     out_path=OUT_PATH,     options=MergeOptions(         lora_merge_cache=\"/tmp\",         cuda=torch.cuda.is_available(),         copy_tokenizer=True,         lazy_unpickle=LAZY_UNPICKLE,         low_cpu_memory=LOW_CPU_MEMORY,     ) ) <p>Time to try out the new merged model. Let's start by loading it from disk.</p> In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(OUT_PATH)\n\n# Choosing a chat template for a merged model can be difficult. The one defined in \n# NeuralBeagle seems broken. Additionally, it does not have special tokens that some \n# of the merged models might have been trained with\ntokenizer.chat_template = \"\"\"\n{% if not add_generation_prompt is defined %}\n    {% set add_generation_prompt = false %}\n{% endif %}\n{% for message in messages %}\n    {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}}\n{% endfor %}\n{% if add_generation_prompt %}\n    {{ '&lt;|im_start|&gt;assistant\\n' }}\n{% endif %}\n\"\"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n</pre> model = AutoModelForCausalLM.from_pretrained(OUT_PATH, load_in_4bit=True) tokenizer = AutoTokenizer.from_pretrained(OUT_PATH)  # Choosing a chat template for a merged model can be difficult. The one defined in  # NeuralBeagle seems broken. Additionally, it does not have special tokens that some  # of the merged models might have been trained with tokenizer.chat_template = \"\"\" {% if not add_generation_prompt is defined %}     {% set add_generation_prompt = false %} {% endif %} {% for message in messages %}     {{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' + '\\n'}} {% endfor %} {% if add_generation_prompt %}     {{ '&lt;|im_start|&gt;assistant\\n' }} {% endif %} \"\"\"  pipeline = transformers.pipeline(     \"text-generation\",     model=model,     tokenizer=tokenizer,     device_map=\"auto\", ) <p>Next, we need to set up how to generate text with it. You can leave the following config as-is, or you can experiment. Here is a list of all the different arguments.</p> In\u00a0[\u00a0]: Copied! <pre>GENERATION_CONFIG = GenerationConfig(\n    # What should be outputted\n    max_new_tokens=256, \n\n    # Controlling how the model chooses the next token to generate\n    do_sample=True, \n    temperature=0.2, \n    repetition_penalty=1.2,\n    top_k=50,\n    top_p=0.95,\n\n    #\u00a0Miscellaneous required settings\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n</pre> GENERATION_CONFIG = GenerationConfig(     # What should be outputted     max_new_tokens=256,       # Controlling how the model chooses the next token to generate     do_sample=True,      temperature=0.2,      repetition_penalty=1.2,     top_k=50,     top_p=0.95,      #\u00a0Miscellaneous required settings     eos_token_id=tokenizer.eos_token_id,     pad_token_id=tokenizer.eos_token_id ) In\u00a0[\u00a0]: Copied! <pre>messages = [\n    dict(\n        role=\"system\",\n        content=\"\"  # Change this to anything you want\n    ),\n    dict(\n        role=\"user\",\n        content=\"Hvad er en stor sprogmodel?\"  # And change this too\n    ),\n]\n\noutputs = pipeline(\n    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True), \n    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n    generation_config=GENERATION_CONFIG,\n)\n</pre> messages = [     dict(         role=\"system\",         content=\"\"  # Change this to anything you want     ),     dict(         role=\"user\",         content=\"Hvad er en stor sprogmodel?\"  # And change this too     ), ]  outputs = pipeline(     tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),      streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),     generation_config=GENERATION_CONFIG, ) <p>You can share your new model to the Hugging Face Hub - this requires that you've included your Hugging Face token at the top of this notebook.</p> In\u00a0[\u00a0]: Copied! <pre># model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN)\n</pre> # model.push_to_hub(\"your_name/merged_model\", token=HUGGING_FACE_TOKEN) <p>This deletes the merged model, as well as clearing the Hugging Face cache.</p> <p>WARNING: You will have to redownload any used models if you do this!</p> In\u00a0[\u00a0]: Copied! <pre># shutil.rmtree(OUT_PATH, ignore_errors=True)\n# shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)\n</pre> # shutil.rmtree(OUT_PATH, ignore_errors=True) # shutil.rmtree('/home/ubuntu/.cache', ignore_errors=True)"},{"location":"tutorials/merge.html#draft-false-date-2024-02-02","title":"draft: false date: 2024-02-02\u00b6","text":""},{"location":"tutorials/merge.html#merging-language-models","title":"Merging Language Models\u00b6","text":""},{"location":"tutorials/merge.html#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"tutorials/merge.html#get-hugging-face-token","title":"Get Hugging Face Token\u00b6","text":""},{"location":"tutorials/merge.html#configure-the-merge","title":"Configure the Merge\u00b6","text":""},{"location":"tutorials/merge.html#merge","title":"Merge!\u00b6","text":""},{"location":"tutorials/merge.html#try-it-out","title":"Try it Out\u00b6","text":""},{"location":"tutorials/merge.html#share-the-model","title":"Share the Model\u00b6","text":""},{"location":"tutorials/merge.html#clean-up","title":"Clean Up\u00b6","text":""},{"location":"blog/archive/2024.html","title":"2024","text":""}]}